{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to funcstream Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-funcstream","text":"","title":"Welcome to funcstream"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"about/","text":"Functional programming with Haskell This is an exploration of a beautiful programming language Haskell and functional programming. I am learning from several books and documenting the learning here. There are Haskell related links and I will attempt to write short reviews on each, and then there are observation that are following books I currently read, often using real life analogies and examples. I find that observing code not just through numbers but through seemingly unrelated analogies enables me in turn to get a better grasp at numbers. When I try to explain some Haskell code to people who do not know programming I notice they intuitively do get programming, it's just the thing with numbers is that they can induce heavy feelings of guilt or remind them on how bad were they in math which is only the fault of the teachers and their reluctance and to bring clarity that is also rich with a variety of conceptual examples that are not just stiff images but life like analogies that motivate and remind the human about processes they are familiar with that are in their essence programming, and not just that but at their core, functional programming. There will be errors and before your vents go off please notice that I am thinking in what you could call a free jazz improvisational method, or the so called Miles Davis there are no wrong notes method in which after you make a mistake you do not correct the mistake but find a new turn from which the wrong note, or sound will sound correct. Such exploration may be tedious in the beginning, and yet this spaced error repetition eventually builds a different kind of knowledge, one that goes from errors to correctness, from exploring a variety of mistakes to a much richer correct understanding of the issue at hand. It also reduces the memory footprint since by pruning the errors and developing from them, our understanding becomes capable of a much wider web of connections. If one orients on just being correct it is possible to induce higher stress levels since one is always striving for correctness","title":"About"},{"location":"about/#functional-programming-with-haskell","text":"This is an exploration of a beautiful programming language Haskell and functional programming. I am learning from several books and documenting the learning here. There are Haskell related links and I will attempt to write short reviews on each, and then there are observation that are following books I currently read, often using real life analogies and examples. I find that observing code not just through numbers but through seemingly unrelated analogies enables me in turn to get a better grasp at numbers. When I try to explain some Haskell code to people who do not know programming I notice they intuitively do get programming, it's just the thing with numbers is that they can induce heavy feelings of guilt or remind them on how bad were they in math which is only the fault of the teachers and their reluctance and to bring clarity that is also rich with a variety of conceptual examples that are not just stiff images but life like analogies that motivate and remind the human about processes they are familiar with that are in their essence programming, and not just that but at their core, functional programming. There will be errors and before your vents go off please notice that I am thinking in what you could call a free jazz improvisational method, or the so called Miles Davis there are no wrong notes method in which after you make a mistake you do not correct the mistake but find a new turn from which the wrong note, or sound will sound correct. Such exploration may be tedious in the beginning, and yet this spaced error repetition eventually builds a different kind of knowledge, one that goes from errors to correctness, from exploring a variety of mistakes to a much richer correct understanding of the issue at hand. It also reduces the memory footprint since by pruning the errors and developing from them, our understanding becomes capable of a much wider web of connections. If one orients on just being correct it is possible to induce higher stress levels since one is always striving for correctness","title":"Functional programming with Haskell"},{"location":"blogs/","text":"Blogs Steve Yegge - Execution in The Kingdom of Nouns Hello, world! Today we're going to hear the story of Evil King Java and his quest for worldwide verb stamp-outage. Caution: This story does not have a happy ending. It is neither a story for the faint of heart nor for the critical of mouth. If you're easily offended, or prone to being a disagreeable knave in blog comments, please stop reading now. Letter to a Young Haskell Enthusiast The following letter is not about what \"old hands\" know and newcomers do not. Instead, it is about lessons that we all need to learn more than once, and remind ourselves of. It is about tendencies that are common, and understandable, and come with the flush of excitement of learning any new thing that we understand is important, and about the difficulty, always, in trying to decide how best to convey that excitement and sense of importance to others, in a way that they will listen. It is written more specifically, but only because I have found that if we don't talk specifics as well as generalities, the generalities make no sense. This holds for algebraic structures, and it holds for other, vaguer concepts no less. It is a letter full of things I want to remember, as well as of advice I want to share. I expect I will want to remind myself of it when I encounter somebody who is wrong on the internet, which, I understand, may occur on rare occasion. Monday Morning Haskell At Monday Morning Haskell, we have tutorials for all levels of programmers! If you're new to Haskell, take a look at our Beginners series. If you have some experience with the language already, we've got some more Advanced material so you can get started on real world projects! Either way, come back every Monday morning for some new material on the Blog! Why are partial functions (as in head , tail ) bad? The problem with partial functions is that they're liars. Consider head: its type is [a] -> a, which means \"give me a list of as and I'll give you an a\". So I give it [] - does it give me an a? No, it doesn't, it throws an exception instead. And when functions start lying about the things they return, you can no longer reason about them. Applied Haskell Syllabus Applied Haskell is a commercial training program focusing on teaching intermediate Haskell. The goal is to help someone move from knowing Haskell basics to being able to write commercial software, with enough knowledge to pick up any new skills needed on demand. Haskell Learn - FPComplete An introduction to recursion schemes Because nested structures appear in almost every problem domain and programming environment, from databases to 3D graphics to filesystems, the act of iterating through these structures is common, so common that most programmers barely notice when they\u2019re doing it. As such, generalizing the act of recursive traversals provides immediate real-world benefits: our new generalized traversal can replace a host of type-specific traversal functions. In addition, by decoupling how a function recurses over data from what the function actually does, we reduce cognitive overhead and can focus entirely on the core behavior of our recursive functions. The Road to Proficient Haskell Getting started with Haskell You Could Have Invented Monads In fact, I hope to get you to invent them now if you haven't already. It's then a small step to notice that all of these solutions are in fact the same solution in disguise. And after reading this, you might be in a better position to understand other documents on monads because you'll recognise everything you see as something you've already invented. How To Do Basic Error Handling Logging Basic Haskell: An Examination of a Todo List Standardized ladder of functional programming Sum Types How Laziness works What I'm about to describe is completely useless to learn how to write Haskell, but if you're like me and like poking at things under the hood, by all means join in. Space Leak Zoo There are a few different types of space leak here, but they are quite different and a visitor would do well not to confuse them (the methods for handling them if encountered in the wild vary, and using the wrong technique could exacerbate the situation). Money in the type system where it belongs Notwithstanding the value civilization gives to a particular currency, the amount of said currency one owns can't spontaneously increase nor decrease in number. As programmers, we do play a crucial role in ensuring amounts of currency are never made up nor lost. In this article we will explore how we can leverage types, functional programming, and in particular the safe-money Haskell library to ensure that our software deals with monetary values and world currencies as carefully as civilization requires. Mostly, we will be exploring the type system and learning how to reason through types. Embedding Linear Lambda Calculus, Quickly and Easily Suppose you want to create an embedded DSL based on the linear lambda calculus. Why might you want to do this? Well, you might want to control access to some resource, or perhaps you've heard that linear types can change the world and now you'd like to compile your EDSL to some target language and optimize things using mutable data structures. Imperative Haskell I was working through Tim Roughgarden\u2019s Algorithms 1 (which has now been replaced by two smaller courses) and attempting to do all the exercises in Haskell when I bumped up against an uncomfortable truth. An Opiniated Guide to Haskell in 2018 In the meantime, in the interest of both sharing with others the small amount of wisdom I\u2019ve gained and preserving it for my future self, I\u2019ve decided to write a long, rather dry overview of a few select parts of the Haskell workflow I developed and the ecosystem I settled into. Functor-Oriented Programming My style of Haskell programming has been evolving over the 15 years that I have been working with it. It is turning into something that I would like to call \u201cfunctor oriented programming\u201d. The traditional use of typed functional programming focuses on data types. One defines data types to model the data structures that your program operates on, and one writes functions to transform between these structures. One of the primary goals in this traditional methodology is to create data structures that exclude erroneous states to the extent that is reasonably possible. As long as one ensures that pattern matching is complete, then the type system catches many errors that would otherwise lead to these erroneous states, which have been crafted to be unrepresentable. Functor oriented programming is a refinement of this traditional focus on data types. The \"What are Monads\" Fallacy The Haskell community has a monad problem. No, that's not quite right. Let me rephrase that.Haskell beginners have a monad problem, and the Haskell community is partly to blame. Opaleye's sugar on top People often talk of how solutions fall naturally into place when we program in Haskell and embrace its type system. This article walks us through that process, serving as a gentle introduction to some practical uses of advanced features of the Haskell type system within the context of Opaleye and SQL. I invite you to continue reading even if you are not particularly interested in Opaleye nor SQL, as the approach explained here can be used in other contexts too. Finite-state Machines, Part 1: Modeling with Haskell Dat Types Stateful programs often become complex beasts as they grow. Program state incohesively spread across a bunch of variables, spuriously guarded by even more variables, is what I refer to as implicit state. When working with such code, we have to reconstruct a model mentally, identifying possible states and transitions between them, to modify the program with confidence. Even if a test suite can help, the process is tedious and error-prone, and I insist we should have our tools do the heavy lifting instead. Basic Type Level Programming in Haskell Dependently typed programming is becoming all the rage these days. Advocates are talking about all the neat stuff you can do by putting more and more information into the type system. It\u2019s true! Type level programming gives you interesting new tools for designing software. You can guarantee safety properties, and in some cases, even gain performance optimizations through the use of these types. www.mathmeth.com resources Here is a collection of materials which we find particularly instructive and useful. Some materials are available online for free, while others have to be purchased. Builtins I've recently written a document elaborating on the design of FFI for a functional language implemented in Haskell. Type safety of FFI is ensured via type-level programming (GADTs, type families, etc).","title":"Blogs"},{"location":"blogs/#blogs","text":"Steve Yegge - Execution in The Kingdom of Nouns Hello, world! Today we're going to hear the story of Evil King Java and his quest for worldwide verb stamp-outage. Caution: This story does not have a happy ending. It is neither a story for the faint of heart nor for the critical of mouth. If you're easily offended, or prone to being a disagreeable knave in blog comments, please stop reading now. Letter to a Young Haskell Enthusiast The following letter is not about what \"old hands\" know and newcomers do not. Instead, it is about lessons that we all need to learn more than once, and remind ourselves of. It is about tendencies that are common, and understandable, and come with the flush of excitement of learning any new thing that we understand is important, and about the difficulty, always, in trying to decide how best to convey that excitement and sense of importance to others, in a way that they will listen. It is written more specifically, but only because I have found that if we don't talk specifics as well as generalities, the generalities make no sense. This holds for algebraic structures, and it holds for other, vaguer concepts no less. It is a letter full of things I want to remember, as well as of advice I want to share. I expect I will want to remind myself of it when I encounter somebody who is wrong on the internet, which, I understand, may occur on rare occasion. Monday Morning Haskell At Monday Morning Haskell, we have tutorials for all levels of programmers! If you're new to Haskell, take a look at our Beginners series. If you have some experience with the language already, we've got some more Advanced material so you can get started on real world projects! Either way, come back every Monday morning for some new material on the Blog! Why are partial functions (as in head , tail ) bad? The problem with partial functions is that they're liars. Consider head: its type is [a] -> a, which means \"give me a list of as and I'll give you an a\". So I give it [] - does it give me an a? No, it doesn't, it throws an exception instead. And when functions start lying about the things they return, you can no longer reason about them. Applied Haskell Syllabus Applied Haskell is a commercial training program focusing on teaching intermediate Haskell. The goal is to help someone move from knowing Haskell basics to being able to write commercial software, with enough knowledge to pick up any new skills needed on demand. Haskell Learn - FPComplete An introduction to recursion schemes Because nested structures appear in almost every problem domain and programming environment, from databases to 3D graphics to filesystems, the act of iterating through these structures is common, so common that most programmers barely notice when they\u2019re doing it. As such, generalizing the act of recursive traversals provides immediate real-world benefits: our new generalized traversal can replace a host of type-specific traversal functions. In addition, by decoupling how a function recurses over data from what the function actually does, we reduce cognitive overhead and can focus entirely on the core behavior of our recursive functions. The Road to Proficient Haskell Getting started with Haskell You Could Have Invented Monads In fact, I hope to get you to invent them now if you haven't already. It's then a small step to notice that all of these solutions are in fact the same solution in disguise. And after reading this, you might be in a better position to understand other documents on monads because you'll recognise everything you see as something you've already invented. How To Do Basic Error Handling Logging Basic Haskell: An Examination of a Todo List Standardized ladder of functional programming Sum Types How Laziness works What I'm about to describe is completely useless to learn how to write Haskell, but if you're like me and like poking at things under the hood, by all means join in. Space Leak Zoo There are a few different types of space leak here, but they are quite different and a visitor would do well not to confuse them (the methods for handling them if encountered in the wild vary, and using the wrong technique could exacerbate the situation). Money in the type system where it belongs Notwithstanding the value civilization gives to a particular currency, the amount of said currency one owns can't spontaneously increase nor decrease in number. As programmers, we do play a crucial role in ensuring amounts of currency are never made up nor lost. In this article we will explore how we can leverage types, functional programming, and in particular the safe-money Haskell library to ensure that our software deals with monetary values and world currencies as carefully as civilization requires. Mostly, we will be exploring the type system and learning how to reason through types. Embedding Linear Lambda Calculus, Quickly and Easily Suppose you want to create an embedded DSL based on the linear lambda calculus. Why might you want to do this? Well, you might want to control access to some resource, or perhaps you've heard that linear types can change the world and now you'd like to compile your EDSL to some target language and optimize things using mutable data structures. Imperative Haskell I was working through Tim Roughgarden\u2019s Algorithms 1 (which has now been replaced by two smaller courses) and attempting to do all the exercises in Haskell when I bumped up against an uncomfortable truth. An Opiniated Guide to Haskell in 2018 In the meantime, in the interest of both sharing with others the small amount of wisdom I\u2019ve gained and preserving it for my future self, I\u2019ve decided to write a long, rather dry overview of a few select parts of the Haskell workflow I developed and the ecosystem I settled into. Functor-Oriented Programming My style of Haskell programming has been evolving over the 15 years that I have been working with it. It is turning into something that I would like to call \u201cfunctor oriented programming\u201d. The traditional use of typed functional programming focuses on data types. One defines data types to model the data structures that your program operates on, and one writes functions to transform between these structures. One of the primary goals in this traditional methodology is to create data structures that exclude erroneous states to the extent that is reasonably possible. As long as one ensures that pattern matching is complete, then the type system catches many errors that would otherwise lead to these erroneous states, which have been crafted to be unrepresentable. Functor oriented programming is a refinement of this traditional focus on data types. The \"What are Monads\" Fallacy The Haskell community has a monad problem. No, that's not quite right. Let me rephrase that.Haskell beginners have a monad problem, and the Haskell community is partly to blame. Opaleye's sugar on top People often talk of how solutions fall naturally into place when we program in Haskell and embrace its type system. This article walks us through that process, serving as a gentle introduction to some practical uses of advanced features of the Haskell type system within the context of Opaleye and SQL. I invite you to continue reading even if you are not particularly interested in Opaleye nor SQL, as the approach explained here can be used in other contexts too. Finite-state Machines, Part 1: Modeling with Haskell Dat Types Stateful programs often become complex beasts as they grow. Program state incohesively spread across a bunch of variables, spuriously guarded by even more variables, is what I refer to as implicit state. When working with such code, we have to reconstruct a model mentally, identifying possible states and transitions between them, to modify the program with confidence. Even if a test suite can help, the process is tedious and error-prone, and I insist we should have our tools do the heavy lifting instead. Basic Type Level Programming in Haskell Dependently typed programming is becoming all the rage these days. Advocates are talking about all the neat stuff you can do by putting more and more information into the type system. It\u2019s true! Type level programming gives you interesting new tools for designing software. You can guarantee safety properties, and in some cases, even gain performance optimizations through the use of these types. www.mathmeth.com resources Here is a collection of materials which we find particularly instructive and useful. Some materials are available online for free, while others have to be purchased. Builtins I've recently written a document elaborating on the design of FFI for a functional language implemented in Haskell. Type safety of FFI is ensured via type-level programming (GADTs, type families, etc).","title":"Blogs"},{"location":"books/","text":"Books A type of programming Learn you a Haskell for Greater Good Wise Man's Haskell Richard Bird: Thinking Functionally with Haskell Graham Hutton: Programming in Haskell Simon Thompson: Haskell - The Craft of Functional Programming Allen/Moronuki: Haskell From First Principles Will Kurt: Get Programming with Haskell Hudak/Quick: The Haskell School of Music David S. Touretzky: Common Lisp: A Gentle Introduction to Symbolic Computation Alejandro Serrano Mena: Practical Haskell - A Real World Guide to Programming Phil Freeman: Purescript by example Discrete Mathematics using a computer Parallel and concurrent programming in Haskell","title":"Books"},{"location":"books/#books","text":"A type of programming Learn you a Haskell for Greater Good Wise Man's Haskell Richard Bird: Thinking Functionally with Haskell Graham Hutton: Programming in Haskell Simon Thompson: Haskell - The Craft of Functional Programming Allen/Moronuki: Haskell From First Principles Will Kurt: Get Programming with Haskell Hudak/Quick: The Haskell School of Music David S. Touretzky: Common Lisp: A Gentle Introduction to Symbolic Computation Alejandro Serrano Mena: Practical Haskell - A Real World Guide to Programming Phil Freeman: Purescript by example Discrete Mathematics using a computer Parallel and concurrent programming in Haskell","title":"Books"},{"location":"categories/","text":"Category Theory 1.2 What is a category Abstraction Composition Identity Homotopy type theory. What is equal? What it identity? Composition and Identity is what defines category theory. We are now ready for first definition! A category is a bunch of objects. What can be bigger than a set? Sets are defined by membership. Elements of sets can also be sets. They have a set hammer and everything is a set nail.. What is a good example of a set that is a member of itself? A bunch of dogs is not a dog. Barbers paradox Who shaves the barber? A Category consists of objects, and arrows. These arrows are also called morphisms. So a morphism is an arrow that goes from a to b a ---(f)---> b . What is an object? It has no properties. What is an arrow, morphism? It is just a name for an arrow. We percieve the universe through these notions described by hunter gatherers, with spatial relationships. Categorists put things in space, higher or lower level abstractions.. Hunter gatherers understand movement too. Realize what language you are using and how it constrains you. So what kind of things can happen? You can have zero or more arrows for each pair of objects.Some objects are not connected, some are, some are connected with infinite number of arrows. So if you have an idea that category is a graph except you have to be open minded what is a graph. You can have arrow going from a to b , from b to a and arrows going from a to a and from b to b .. It's OK, just give them different names. Composition is a very simple property that if you have an arrow going from a to b and an arrow from b to c you have an arrow going from a to c . A --- f ---> B --- g ---> C g . f -------------------> So this is called composition. The category is defined by saying what the objects are and arrow and then defining the composition, like multiplication table for arrows. Different composition tables will give you different categories. Identity for every object, there is this arrow that we call identity, one per object. Why am I calling it identity? Because of composition. If I compose f with identity b I get f again. So: (id a ---> a) a --- f ---> b (id b ---> b) idb after f = f So if you think of it as a graph it has to have some properties, it has to have an identity arrow on each node. So that is left and right identity. The third law is the law of associativity. f ----> g ----> h g after f and then I compose this with h . So I have: h . (g . f) == (h . g) . f That is extremely important to make is manageable for us humans. Is it possible to have no associativity? There are mathematicians that work on making associativity weak.. If objects form a set it's called a small category, if not it's called a large category. In programming, objects are types and arrows are functions. A function is an arrow or a morphism between two types. In Haskell is a little more complicated because of laziness, so the trick is that every type contains the bottom value, because categories don't really take into account time . Time is hard to describe in mathematics but in programming is important. When does the calculation terminates? If the function never ends what is its type? So in Haskell it return an Int type, the bottom type, which means it never terminates. You might ask what are types? Sets of values? There is a simplistic model, they are just sets of values so we can model programming as in a category of sets, sets of values and functions are just functions between sets. And that's a good model too. So a mathematical function is between sets. NOTE: Interesting when drawing arrows between sets the arrows as functions are not morphism. They seem to just map objects but do not morph them. It is like a different view. Seeing morphisms blindly like seeing on which objects they work but without seeing the work, the type of work itself. When I put my categories glasses I see no structures like in sets. We are studying the sets and we find out there are many functions going from set to itself, we find the identity function and this is the identity morphism in my big multiplication table.. I'm abstracting, forgeting what's inside the objects, what the functions do and I end up with a category sets. I have these arrow I forget where they came from.. but I have the multiplication table which fulfills my laws. I can forget where it came from. I don't care about the structure of my functions or objects, I forget what these are.. What can I say about these objects if I just look at the morphism? Oh this set is empty How do you know it? Well, it turnes out the empty set has this property that can be expressed just in terms of morphisms.. it's not easy but it's possible.. You can identify a lot of properties about sets just by looking at the multiplication table without looking at the sets.. If you just look at the sets you are like doing assembly language.. instead you use a higher language of categories looking at the relations.. You look at the interface not the objects. Think data hidding and abstraction . This is the end of road for abstraction. The most abstract language you can think of. We can stop now. (Clap) (.) :: (b -> c) -> (a -> b) -> a -> c f . g = \\x -> f (g x) 2.1 Functions, epimorphisms recap: Category of types and category of functions, identity and composability Operational vs denotational semantics, sometimes I use semantics of sets and sometimes semantics of types. A mathematical functions are total functions while in imperative programming we use partial functions which explode for some conditions. How can you tall if a functions is pure? A function is pure if you can memoize it, if you can turn it into a lookup table. Like functions on characters are easy to tabulate, while functions of strings or integers are not easy to tabulate, though that is a problem of resources. What is the lowest, or the highest level of abstraction? The simplest building blocks with which we can build complex stuff? We want to get to the bottom so that we can recompose stuff. So now we have this category of types and functions How can we use functions as category of morphisms on sets? So functions are defined as special kind of relations. So we have two sets, and we look at elements. A relation is a subset of pairs of elements. So.. it's just pairing. This element is in a relation with this element. What is a set of pairs? A Cartesian Product. The set of all pairs forms a cartesian product. Now we take a subset of these pairs and any subset is a relation, by definition. There are no other requirements. So in this sense, a relation has no directionality while functions have these arrows. Functions have some kind of directionality. What kind of condition do we have to impose on a relation to be a function. So many elements from set 1 can be in relation with one element with set 2 but one element cannot be mapped with bunch of things. It's still OK for many to be mapped to one value. And all elements have to be mapped into something in the other set. However not all elements of the second set have to be mapped to the first set. Domain ----- f -----> Codomain The mapped part(subset) is called the image of the function This directionality is very important, this intuition of functions. Ask yourself, is the function invertible? Usually it's not, there isn't always a function that goes other way arount, an inverse function. f :: a -> b The function is invertible if there is a function that goes from b to a g :: b -> a g . f = id f . g = id A function that is invertible, symmetric, is called isomorphism . f (id a) <- a < ========= > b -> (id b) g One reason for a function not to be isomorphic is to collapse. All even numbers map to True , all odd numbers map to False . That's one reason not to be invertible. Other reason is that it's whole image does not fill the whole codomain. So if you had to invert this whole image what to do with elements beyond the image in the second set? So instead inverting you could also say the counterimage, fiber.. you could build a set of fibers so on a set of fibers this thing is invertible.. fibration is interesting in categories too! So there are these two reasons for invertibility. So a function takes place in time. A function that is not invertible is something that increases entropy . You cannot unboil an egg. These two phenomena correspond to very interesting thought process, abstraction. Like I am throwing some information and I am left with one piece of information. Like abstracting numbers to even or odd. Like embedding a shadow on a wall of a cave.. If a function does not collapse things than it's called injection. An injective function does not collapse things. No shrinking, no abstraction, it just injects.. x1 -> f x2 y1 -> f y2 If the function covers the whole codomain, if the image covers the whole codomain it's called surjective. If its surjective and injective it is called isomorphism. For all ys there exists an x that y = f x . So I have defined something in terms of elements How can I talk about category theory now if I cannot look at the elements? I have to express this stuff only in terms in morphisms. It is a very holistic approach. /If my microscopes don't work maybe my telescopes work?/ Note: In category theory we don't like latin we like greek. When something is surjective is called epic , when something is injective is called monic . (when you consider set theory) Epimorphism and Monomorphism Let us say that we have guys that are in terra incognita, outside of the image in set b so I make a set c and map a function g to c . So g after f will actually not probe this terra incognita even though g maps everything, inside composition it will actually only act on this inside a composition. Ok if I have two of these functions g1 and g2 mapping the same point, but if they are outside of f function halo then the composition is the same. So the converse of this is if g1 after f is equal to g2 after f then g1 = g2 , so the function is surjective . Now I have expressed this purely in categorical terms. This is an epimorphism . If I have g1 . f = g2 . f then I can cancel f meaning g1 = g2 . 2.2 Monomorphisms, simple types Recap: f :: a -> b f is an epimorphism from a to b if for every other object c and for every pair of morphisms that goes from b to c , if the composition g1 after f is the same as g2 after f follows then g1 = g2 , then this is an epimorphism. In another words if have a composition g1.f equals to g2.f we can cancel the f on the right. [[https://en.wikipedia.org/wiki/Epimorphism][Epimorphism]] Note: wikipedia In category theory, an epimorphism (also called epic morphism or, an epi ) is a morphism f : X -> Y that is right-cancellative in the sense that, for all objects Z and all morphisms g1, g2: Y -> Z , g1 . f = g2 . f => g1 = g2 Epimorphisms are categorical analogues of surjective functions (and in the category of sets the concept corresponds exactly to the surjective functions), but it may not exactly coincide in all contexts A function f from a set X to a set Y is surjective (also known as onto , or a surjection ), if for every element y in the codomain Y of f , there is at least one element x in the domain X of f such that f(x) = y . It is not required that x be unique; the function f may map one or more elements of X to the same element of Y . A function would not be surjective if the image does not fill the whole codomain. Identity function idx on X is surjective. The function f : Z -> {0,1} defined by f(n) = n mod 2 , (that is, even integers are mapped to 0 and odd integers to 1) is surjective. So let's start with something that's not a monomorphism. A non-injective function will just map two different elements on one set to the sem element of set B , like x1 and x2 are mapped to the same y . --g1---> x1 \\ z ---f---> y --g2---> x2 / If you compose g1 with f and g2 with f you get the same result. They only differ in the way they map z but you get the same result. f after g1 will be equal to f after g2 . This is similar to epimorphism, we use precomposition rather than postcomposition. For every object C and every pair of g1 and g2 , this time they go from c -> a if f after g1 = f after g2 leads to g1 equal g2 always then we say it is a monomorphism . Notice f is not monomorphism by itself. I have to use the whole universe to define this product, a universal property. I hope you get some better feel what functions do. Let's talk about sets a little bit. Sets are models for types. Let's think of simplest possible types/sets. Empty set! Doesn't empty set correspond to a type in programming? You find it in Haskell. An empty set corresponds to a type (forget for a moment for functions that do not terminate) Void . There is no way to construct a Void type. Can we define functions that take Void as argument? f :: Void -> Int ? Mathematically speaking yes. I have a function of type Void, I challenge you :) id void :: Void -> Void This is a good function, you can never call it though, it exists in vacuum when you cannot provide an argument. In Haskell it has a name: absurd :: Void -> Int In logic Void corresponds to false, because you cannot construct falsity from something, you cannot prove something is false, it has no proof. Proof is a function, in this case since you cannot create void there is no proof of falsity, but on the other hand if you assume false, you can derive anything. So it's a polymorphic function: absurd :: Void -> a Next is a singleton set, in Haskell is called a Unit () it has only one element. () :: () corresponds to True . Unit :: a -> () . What about a function that takes a Unit and returns an Int ~() -> Int ? This function must be constant, has to return the same integer. There are many functions like this. one :: () -> Int , boolean... What about a type that has two elements? It's just boolean. Boolean has True and False , whatever you call it. Boolean is not an atomic construction in sets or categories, it can actually be defined as a sum of two units. We can talk about functions from bool to bool. A function that returns a boolean is called a predicate. 3.1 Examples of categories, orders, monoids Last time we talked about sets, sets as sets of elements. That's not a very categorical view, I was trying to reformulate some of the properties of sets in terms of morphisms or functions. It makes sense to reformulate them between sets, then we can ask how does this generalize to an arbitrary category.. but we don't know many categories? Let's broaden our horizons! Let's start with simplest possible category! Very few objects. Zero! But a category is defined with objects and arrows, so there are no objects then there are no arrows . Are the conditions then fulfilled. Well the answer is yes, if there aren't any then it's automatically satisfied, so is there an identity arrow? We can say anything about it then since there are no objects. It sort of sounds like a joke. What's the use of this category? It's useless, however the value is in context. Just like zero by itself is useless. What is the context. The context is a category of all categories . In that context, it's an initial object . The next category has one object. There has to be an identity arrow id . That will be a terminal object Next is two objects. Two objects with two identity arrows, two arrows from a to b and b to a and so on... In general we can always start with a graph . But not every graph is a category. It turns out if we start with a graph we can keep adding additional arrows to get a category. The first thing we need are identity arrows, for every node in the graph. Then we come up with composition. For every pair or composable arrows f and g we have a third arrow g(f) , g after f . We need to satisfy associativity . These compositions produce the same arrows, then some can be identified. This kind of construction is called free construction since we are not imposing any constraints other than constraints of category theory. Order categories - in orders arrows are not functions, arrows represent relations. An interesting relation is less than equal =< . =< a ------> b So this arrow doesn't have any meaning other than a is less than of equal to b . It's a relation. Or we can say a comes before b in some order. There are different types of orded. There is preorder , partial order, total order. A preorder satisfies just the minimum of conditions, it has to be composable, so if a is less than b and b is less than c we want a to be less than c . =< =< a ----> b -----> c ------------> =< We recognize this as composition from category theory. Is it associative ? It is! Why? It's because two objects are either in a relation or not. If there are no relation than there is no arrows, it's a binary choice. Now in total arrow you can say between any two objects there is an arrow. But in preorder that is not true. 2nd condition (identity)! So is it true that for every object this object is less than or equal to itself? That's called reflexivity . Here we have only one arrow going from a to b and another arrow from b to a , but we cannot have multiple arrows. A category like this is called a thin category . The set of arrows between any two objects has a name also, it is called a hom-set :: C (a,b)~ or ~C(a,a) A thin category is one in which a every hom-set is either an empty set or a singleton set. That's the definition in terms of hom-sets . We can now impose additional conditions and the next thing we get is partial order . We don't like preorder, we don't like loops. So partial order has no loops because if there is an arrow from a to b then you cannot have an arrow from b to a . If you look at a graph it corresponds to a DAG , directed acyclic graph and further if you say OK, a total order is an order in which there is an arrow between any two objects. And now with this preorder category I can show you epi and mono. Something whats both epi and mono does not have to be invertible. In sets it corresponded to injective and surjective and if it was both it was reversible, called bijection but that is not true in every category. You can have an epi and mono that is not inversible. mono epi z === h1,h2 ===> a --- f ---> b === g1,g2 ===> c Every arrow in preorder is a monomorphism and every arrow in preorder is a epimorphism, but it's not invertible, especially in partial order than it's definetly not invertible because there are no arrow going back. So that't a counter example. You can think of the most general category as being like a preorder that is thick . It gives you a different intuition. Here when you have order you think of it as a relation and this means it is true or false. So it's a black and white world. Now in a thick category you might say, if I have a number of arrows, and each of these arrows sort of represent a proof of this relation. Here is one proof called g , here is another called f .. so you might think of a category like a proof relevant preorder . A thin category defines a relation and a thick category defines a proof relevant definition. That's a different way of looking. It's not only enough to show that something is related to something, Homotopy type theory studies relations in that sense. In one object category we can have many arrows. We can have many more loops. -- Monoid / <===> m -- <===> id \\ <===> So any category with a single object and many arrows is called a Monoid, sort of a pre-group. Monoid is usually defined as a set of elements with some operation defined on them, let us say multiplication. So it's a binary operator. And this binary operator, we can impose certain conditions. We want one of these elements to be the unit ( identity ), sort of like multiplication by 1 , you always get the same result. And the other condition that can be imposed is associativity . ~(a * b) * c = a * (b * c)~ ~e * a = a * e~ String concatenation, that's an interesting monoid, does it have a unit? Yes, an empty string, you append or preappend an empty string don't change anything. It's associative. It is a good example because it's not symmetric. Multiplication and addition is symmetric, you can change the order, with strings you can't, you append two strings, the result will be different if you append them in the opposite order, so this is a very nice example of a monoid. And lists, appending lists forms a monoid. In Haskell, strings are lists of characters and they form a monoid. So thi is one view of a monoid from set theory. Let's call this Monoid M. There is only one hom-set from M(m,m) since there is only one object. This hom-set is a set, right? This category defines a set, and guess what, there is a third element that corresponds to the composition to these two arrows. Well, let's say this is our multiplication. So the third element is the product of two elements. If you pick any two arrows in hom-set, the end of one is the beginning of the next, so there is a third one, f , g , g after f , so g after f is also an element. And then id is here also. All these arrows are members of this set. Example, arrows would correspond to adding a number in some category. So the binary operator in a monoid has to be defined for all elements of the set, has to be a total function of two arguments. So a category of types corresponds to a strongly typed system. You cannot compose any two functions. The result of one function has to have a type that is the same as the argument of the next function. That's strong typing . Not any two functions are composable. The types have to match. A monoid is a very special category in which every two functions are composable, that corresponds to your languages, that have weak typing, any two functions are composable. 3.2 Kleisl category Let's define a relation on sets, this is a relation of inclusion , what it means to be a subset of another set. It is a relation, the question is what kind of relation is this? Is this an order, preorder? What should we check? Identity in terms of order is reflexivity . If a is a subset of b and b is a subset of c we have composition. Is it associative? Yes. So it is definetly a preorder, is it a partial order? We have no loops. If a =< b and b =< a then a = b so it is a partial order, is it a total order? No. Is it possible to have like a diamond relation? They form a dag . a / \\ b /= c \\ / d I want to introduce a category close to us programmers, not based on types and function, we get to it by solving a real programming problem. The problem is this: We have a library of functions. One day the manager says there is a new requirements, that every function has to have a audit trail, every function has to create a little log that says the name or something, has to be appended to a log. Now go and rewrite our library so that every function leaves a trail. The simplest imperative solution would be: have a global log. A simple solution introducing many dependencies. But logs don't compose, deadlocks... pair <bool, string> negate (bool x, string log) { return makePair (!x, log + \"Not!\"); } The subtle problem is this use of plus, why does a function called negate knows about appending strings? This one function is more local but still it has this element knowing stuff it does not belong. So this is a good solution but not quite. par <bool,string> negate (bool x) { return make_pair (!x, \"not!\"); } Who does appending of this logs? Somebody has to concatenate these logs. So the answer is.. what do we do with these functions? We compose them. What if we modify how we compose functions? Let us define a new way of composing functions. Appending strings is in essence composing of functions. [[https://blog.softwaremill.com/kleisli-category-from-theory-to-cats-fbd140bf396e][Kleisli-category-rom-theory-to-cats]] Note: I didn't get this at all :( but will keep watching the lectures :). I also do not understand the examples in scala above. What I do understand that the composition between a and b resultet in an embellished function where a defines a b and a string, so it is not just a to b but a to b which results in pairing the result with another string. So this makes a monad, a way of composing special functions. 4.1 Terminal and Initial Objects Recap of Kleisl categories, important to understand Monads. It seems challenging because you have to hold two categories in your head. So you start with one category in which you have objects and arrows. Now based on this category you are trying to build another category, the Kleisli category, and you are building it this way - you're saying the objects in this category are exactly the same as in the starting category , however the arrows in this category are not the same arrows as here so if I have an arrow from a to b is not the same arrow as the one in category C from a to b . Actually I have something that for every object here in C gives me some other object. Now, we talked about a particular case in which for every type, if I had a type a , I assign to it a new type that's a pair of a and String . A pair of a and String is a different type than a , but it's a mapping of types, so for every type a I have this type. Let me call this f a , but maybe not, it's not a function. So let me call it m . Now, m is a mapping that mapps objects to objects or types to types. Later we will learn that this kind of mapping is called a functor. For type b I will have a type m b (the pair of b and String ) so if there is an arrow from a -> m b this will be my arrow from a -> b in my Kleisli category, so this is equal to this. *C* *Kleisli* a ----> ma a <====> id |\\ (a, String) | | \\ ------------------> | | \\ | b m b b |\\ (b,String) | | \\ | | \\ | c m c c (c,String) So it's like im implementing a new category(Kleisli) in terms of this(C) category, I'm implementing the arrow in this category as an arrow in this category, this is an arrow (Kleisli), this is how it's implemented (C). How do I know it's a category? So what's an arrow from b to c (In Kleisli)? It's not an arrow from b to c (In C). It's implemented as an arrow as b to (c, String) , or in general some mc , right? So in this (C) categry they do not compose because the end of this one, (mb) is not the same as the beginning of this one(b). How do I compose these (Kleisli) guys? In principle I don't know. Now I showed you that in this case when is (b, String) and (c, String), let me call this function first a -> mb and I'll get this pair b, String and I will split this pair into b and String and then I will pass this b ( b,String ) here (points to b a -> b ) and I will get this c, String , I will get c and String , right? And then I can combine these things, I can concatenate these two strings, and return a pair (c, s1 ++ s2) so I have now a way of composing these arrows. Now in general for any kind of mapping it's not true, I was just lucky. There was a composition, I could define a clever way of composing these things! If I find the way of cleverly composing the implementation here (C) then I can say this is how I compose these arrows in this Kleisli category, and for this to be a category I have to have an identity. How is identity implemented? It has to go from a to this m a or in another words (a, String) and it has to be a Unit with respect to my new special kind of composition. I have to pick a string thats an empty string so that the concatenation with an empty string will give back the original string. Once I do that than I can say this is a category and if this is a Kleisli category, then this mapping from a to a string or in general from a -> ma is called a Monad! So this is one of many definitions of a monad. This is a very categorical construction. And now for something completely different. So we talked about sets and there is this category set and there is also set theory, and there are these two views, that very useful. One view is sets are things that have elements and we can define things like function, mapping elements to elements, so a lot of these things can be defined in terms of elements. And then we have this category set and in this category we suddenly got amnesia and we are forbidden to talk about elements, only about arrows. We started from arrows, and we know functions between sets so every time we have a function between two sets there will be an arrow in category of sets. And we know how to compose functions. What is an empty set? How do I define an empty set if I don't know anything about elements, a singleton? A cartesian product (set of pairs)? So all this stuff have to be completely rediscovered. Just in terms of arrows and compositions. There is this method of defining things. It's called Universal construction We use this in category theory to pick a particular kind of pattern. Since we cannot go inside of the object we define the properties of the object in terms of the relation to other objects. So we have to define everything in terms of arrows coming and going to this object. We have to think about the whole universe, and we talked about it with epi and mono. So it's like googling. Think of a simple pattern, OK google in this category, show me all hits, so everything that matches this pattern, and usually you have lots of hits. The next thing you do is you have to rank these hits. If you have two hits, see which one is better. The best match defines the object that you are looking for. We will try to define a singleton set. How does this set relate to other set? Think arrows! There is one property of singleton set that's interesting. It has an arrow coming from every other set. There is an arrow from any other set to singleton set. In programming we call it Unit type , an empty tuple () so from any type or any set there is a function to Unit and this function is called Unit, it's a polymorphic function, it just ignores it's argument and creates a unit and returns it a --- Unit ---> () , or Void --- Unit void --> () . Does it really single out singleton object? Is there any other type that have the same property? Well unfortunately yes because set is a category that is extremely rich in arrows! Only, if you have an non empty set and empty set there is no function there! You can only say they all map to the same element, and I'm fucked! So for instance, OK, Bool, the type bool of two element set, is there an arrow from every other set to it? You bet, right? In fact there are two arrows from any other set. One is called True and False . They just ignore the argument and return true or false. The singleton type or unit type there is always a unique arrow from any other object, so this way we can define using pure arrows we can define what we mean by singleton set, without talking about elements. Let's forget about sets! What would we call this object? We will call it terminal object , for all arows, all arrows will converge on this object. Not every category has a terminal object. We can try, we'll say a terminal object in a category is an object that has a unique arrow coming from any other object. Understand, these are two separate conditions. For all objects/type a there exist an f that goes from a -> () . So this is one condition. And for every two functions from a -> () they have to be equal. That's how you define uniqueness. (for all) a (there exist) f :: a -> () (for all) f :: a -> (), g :a -> () => f = g An empty set can be defined by outgoing arrows (singleton set by incoming arrows) Void --- absurd ---> a So I have just reversed the definition I used for terminal object. By the same token I want this arrow to be unique. This object will be called initial object , the opposite from terminal , it has a unique outgoing arrow to every other object. This corresponds to empty set, or in programming to Void . The property of the terminal object, no matter what path you take to the terminal object you can always shrink it to one arrow and it's always the same arrow, this is where uniqueness comes. With boolean for example there would be two ways of shrinking, some path would become true paths some false, see there are more ways of shrinking these paths. When this object is terminal there is only one true path, leading you to the terminal object. Ok, the next question we might ask, how many of these objects are there? How many empty sets are there? Just one, seems natural to think that, what about terminal object, how many singleton sets are there? Tougher question.. is it the same, the set that contains one apple is it the same as the set containing one orange? I don't know.. from perspective category theory, what does it mean for two objects to be equal? I don't know, there is no equality of objects. There is an equality of arrows, if they have the same ends and beginnings, right? So we can compare arrows for equality but cannot compare objects for equality, instead we can ask if they are isomorphic . Isomorphism is this fact that you have two arrows, one being inverse of the other. Terminal object is unique up to an isomorphism. And even stronger condition is that there is an unique isomorphism between them. Like if you have two element sets, (true and false) and (black and white), true is black, false is white, these are two morphisms, both invertible. Suppose we have two terminal objects a and b , so there will be an arrow from b to a , and it is a unique arrow because a is terminal object, but b is a terminal object so there is unique arrow coming from a to b . What's the composition? It's a loopy thing. <------g---- (id a) a b (id b) ------f-----> -- Unique isomorphism: g . f = id a f . g = id b How does the pattern and ranking relate to this? So my pattern is an object, a simple pattern, now show me all example of this pattern in your category, what will you show me? You will show me all your objects, because I didn't specify anything about it, that's a very imprecise query, it gives you huge recall, but we have the ranking. So if I have two matches I will say a is better than b if there is a unique arrow from b to a. OK, maybe there is no unique arrows.. ok fine well then you don't compare these objects. I didn't say its a total order, its a partial order. What is like the best fit? One that is better than all else, so terminal object is better than any other object. The difference between initial and terminal object is just in the ranking. 4.2 Products There was a question about terminal objects. There is nothing I said about outgoing arrows from the terminal object. I talked about incoming arrows to the terminal object, they have to be unique from every object. It doesn't mean there are no outgoing arrows and in fact there are usually outgoing arrows from the terminal object and these are the arrows that helps us define generalized elements in other objects, every arrow from the terminal object to another object is a definition of a generalized element in this other object. This is what happens in set, when you map a singleton set into some other set, thats equivalent of picking one element and say, this element of singleton set is mapped to this particular element of the other set, so its picking another one, there are many morphisms, each of them picks a different element. Now let's talk... We have now two examples of universal construction, the terminal object and initial object. I talked about reversing the arrows. It turns out this has a much deeper meaning. Every construction in category theory has its opposite construction that is done by reversing arrows. If you define the terminal object you get for free the definition of the initial object. You can allways create a new category which is identital to another category but with arrows reversed. C (a -- f --> b) C op (b -- fop --> a) f . g (g . f) op (g . f) op = f op . g op Cartesian product (the set of pairs), for instance a plane is a cartesian product of two axis, and cartesian product corresponds to a point. For every cartesian project there are these functions, called projections. In Haskell we call then fst and snd . Of a x b there are these two arrows called first and second. First maps to a second maps to b . That's a pattern. I'll call a x b a C , maybe I'll call these two arrows p(fst) and q(snd . There could be many such things, it could be anything. Now one of that is my cartesian product, but which one? Universal construction to the rescue! I have to be able to rank them, that some cartesian product is better than another one. So let us say we have c and c' . We say c is better than c' if there is a morphism, let's call it m . C' / | \\ p'/ | \\ q' / m \\ / p | q \\ a <---- C ----> b p . m = p' q . m = q' Later: a = Int b = Bool p = fst q = snd Is this enough to pick? No it's still not enough. So to summarize, c is better than c' if there is a unique morphism m from c' to c such that, this is true (p.m=p', q.m=q') . How do we read this? If this were multiplication then you would say p' factorizes into p times m and q' factorizes into q times m . So they have a common factor m . So I can like extract a common factor. So this morphism is special, it factorizes these two projections. It takes the worst out of these two projections, condenses them. Why the worst? If you look at different candidates, like the Goldilocks principle , some candidates are too small some too big, they don't fit. Morphism can loose information, it can squeeze, may not cover, so like all this non injectivity is concentrated in this m . This is a bad guy, it does all this non injective non surjective stuff, they are concentrated in there. Like p is this nice clean projection but if you add this uglines you get this p' projection. So the real product of a and b is a pair (a,b) . That's the type. fst (a,_) = a snd (_,b) = b a = Int b = Bool (Int, Bool) Int candidate p :: Int -> Int p = identity q :: Int -> Bool q = True I have to show there is an mapping m . m :: Int -> (Int, Bool) m x = (x, True) -- non-injective badness Correction: I mean non-surjective, It misses pairs of the form (x, False) Let's try a different candidate. We want a richer candidate. Let's have a triple. (Int, Int, Bool) Now I can define a projection. p' :: (Int, Int, Bool) -> Int p' (x,_,_) = x q' :: (Int, Int, Bool) -> Bool q' (_,_,b) = b But this guy (p') is too big, like 3D cube and my product is just a square, so I'm shrinking stuff. What is the m in this case. m would have to be a mapping from the bad candidate which is (Int,Int,Bool) -> (Int,Bool) . m (x,y,b) = (x,b) so it's non-injective . How do we define a product? So a categorical product of two objects a and b is a third object c with two projections p and q . C p : c -> a q : c -> b For any other C' that has some p' from c' to a , and q' from c' to b , for any other pretender there is a unique morphism m that goes from C' to C which factorizes the two projections. p' = p . m and q'= q . m . You remember the picture, the commuting diagram. Two paths through a diagram give you a same result. C is called the product of a and b 5.1 Coproducts, sum types Today I show you the dual of the product, the same thing but in the opposite category, I will take the product and reverse the arrows and show you whats produced and the thing constructed is called coproduct. Co- is usually called when you reverse something, Monad, Comonad.. So a product is this object with two morphism p and q into /a/ and /b/. So it's a product of a and b and a product C , but there are lots of things that have two projections, so product is the best, the ultimate! But what about this C' prime, it also has projections p' and q' why this is not a product? There is a unique mapping ~m~ that makes these two triangles commute. This C is the best candidate if for any other candidate we can do this direct mapping, we can reduce it to this one C and these two projections (p,q) which in sets were just (a,b) so now if we /reverse/ the arrows, we will try to draw this upside down, we want the diagram flow from top to bottom. +BEGIN_SRC a b \\ / \\ / \\ i \\ / j / \\ \\\\ // / i' C j' \\ | / | | C' +END_SRC So a coproduct will match this pattern, It's an object with two arrows coming to it from a and b, and these two arrows are called injections. We just reverse the arrows. So instead of mapping the fake candidate into the real thing using a unique morphism we are mapping the real thing using a unique morphism into the fake candidate. +BEGIN_SRC i' = m . i j' = m . j +END_SRC In programming cartesian product gives us a tuple, a pair of two things. ~(a,b)~ and this is well known thing. However a coproduct? What is it in set theory? So the fact that we have this injection means we are embedding the set a into C and be are embeddin a set b into set C, I'm thinking function, because we want to figure out what this gives us. This is like the best fit for this pattern so we want this to inject the whole set without collapsing, so what happens is that like the best fit would be a set which contains both a and b so it's like s /union/ of these two sets. Injected /faithfully/, means that the whole set a is mapped into this and the whole set b is mapped into this and there is no bloating, no unmapped things, just a and just b and nothing else and this defines absolute best possible fit and this is true of every universal construction, picking the ideal thing. Two objects, two injections, a coproduct. Here we can say that a product is something that every other candidate can be shrunk into this product that we can recognize, map this candidate into the product while here, in coproduct we can recognize a and b inside of C and this unique morphism tells us which parts of C' belong where, what we can find in C'. This does not completely define this thing. In set theory you have this union of two sets. What happens when they overlap? What's a union of set with itself? Just the same set, but you can tag these elements, it's actually duplicated, because it has a tag. I came from the left one, I came from the right one, so this is called a /discriminated union/. It turnse out that it is a discriminated union . There is a mapping from discriminated union to union. a left and a right. So we have twice as many elements, which can be mapped to a single a, a /non-injective/ mapping and it's a unique mapping of this discriminated union into a regular union. The other way around we couldn't. There is no way of mapping of mapping an a from union to a discriminated union because a function cannot split. What is it then in terms of types? In terms of types discriminated union is called a /tagged union/ or a /variant/. It means you have a /data type/ like which if you take a union of Integer and Boolean, this is something that either contains an Int or a Bool, not both of them, a pair is something that contains both, you need both to construct it, in order to construct a /discriminated union/ you either give me a integer and I give you this union, and I tag it, I'm an integer if you look inside me you find an integer, or if you give me a boolen we will have a tag that says it's a bool. The simplest example of this is an /enumeration/. It is an union of things, it can be either this or that. A sum type is not built in and the canonical example is called Either +BEGIN_SRC haskell data Either a b = Left a | Right b +END_SRC You read it either left a or right b. It means you can construct an element of this type, either by calling this constructor and it contains an a or by calling this constructor and it contains b, so these two constructors correspond to this ~i~ and ~j~, one of them injects a and other one injects b whereas in a pair I had two, lets say /distructors/, they destroyed the pair by picking it a part (fst, snd) se here I have two /projections/ here I have two /injections/. Because this is a dual picture, how do we extract stuff? Somebody gave me something ~x :: Either Int Bool~ I cannot just say give me an integer from this x, I can't do that because maybe it is a boolean, I do not know how is constructed. I have to take into account both possibilities, to write code that will work in either case, and that's called /pattern matching/, code that will match left pattern and right pattern. +BEGIN_SRC haskell f :: Either Int Bool -> Bool f (Left i) = i > 0 f (Right j) = b -- they will only match when f is called with left element -- and will make this i equal to the integer sitting -- inside +END_SRC Now we pretty much have the foundation of the type system. In every programming language product types are all over the place. +BEGIN_SRC haskell data Point = P Float Float -- record syntax = { x :: Float , y :: Float } +END_SRC Most of programming is done with products. Standard union in C++ is not tagged. Why is is called a product and a sum? Well sort of like a union, maybe.. so this is sort of like multiplication, like a /plus/ (take a break) and we come back to /algebraic data types/. 5.2 Algebraic Data Types So we have products and we have sums, just like in algebra. Product, sort of like multiplication, what does it mean, it means we have a monoid, at least, right? So a monoid would be something that has multiplication, associative and that has a unit. But now we are talking about types. Is there something like an alebgra of types? Is the product in algebra of types actually behaving like multiplication? Let's check a few things in haskall. The product of numbers, it's not true of every monoid, but a product of numbers is symmetric, let's see if a product of two types is symetric. ~(a, b)~ is it the same as ~(b, a)~? No it's not. If you have a function that takes a pair of Int and Bool it will not accept a pair of Bool and Int. So these two types are not the same, however they contain exactly the same information, they encode it slightly differently, which means that actually they are /isomorphic/. And this isomorphism is called /swap/. +BEGIN_SRC haskell swap :: (a, b) -> (b, a) swap p = (snd p, fst p) +END_SRC It is symmetric up to isomorphism. The monoid product is associative. What does that mean? ~((a, b) c)~ is it the same as ~(a, (b, c))~. This won't typecheck. But again they contain the same information. +BEGIN_SRC haskell assoc ((a, b) c) = (a (b, c)) +END_SRC swap is isomorphic because if you swap two times you get the same thing. Does it have a unit of multiplication? What would be the type if you pair it with any other type, you will just get back the same type? Well it has to be a type that has only one element. So the type that has one element is called a unit. ~(a, ()) /= a~ +BEGIN_SRC haskell munit (x, ()) = x munit_inv x = (x, ()) munit = fst munit_inv x = (x, ()) +END_SRC This follows from sum being associative up to isomorphism. +BEGIN_SRC haskell Either a b ~ Either b a data Triple a b c = Left a | Right c | Middle b +END_SRC What's the unit of sum? /Void/. +BEGIN_SRC haskell Either a Void ~ a a + 0 = a +END_SRC So we have two monoids but that's not all! We would like to combine these two monoids into a bigger thing. What would be that? From algebra we know that we can multiply to 0, we have this ~a * 0 = 0~ so a pair of (a, Void) ~ Void So I can never construct a pair of a and Void which is the same as Void. So a times zero is zero. There is /distributive law/. ~a * (b + c) = a * b + a * c~ +BEGIN_SRC haskell (a, Either b c) ~ Either (a, b) (a, c) +END_SRC What is this structure called when you have multiplication and addition in the same thing. It is called /a ring/. Except a ring has an inverse of addition. And here we don't have inverses. We don't know how to /subtract/ something. What's the inverse of integer of a type, it's nothing.. A ring that has no inverse is called a /Rig/ or /Semiring/ What is the correspondence of 2 = 1 + 1? So 1 is a unit type, we can call left unit true and right ne false, so 2 is a bool. What else? 1 + a is our friend ~Maybe~ ~data Maybe a = Nothing | Just a~ (So Nothing is equivalent to (left) unit ~()~, and right Just a is ~a~. But there is more one interesting trick :) Let's solve equations! So the equation i want to solve is this +BEGIN_SRC haskell l(a) = 1 + a * l(a) l(a) - a * l(a) = 1 l(a)(1 - a) = 1 l(a) = 1 / 1 - a +END_SRC +BEGIN_SRC haskell data List a = Nil | Cons a (List a) +END_SRC Now, unfortunately I cannot do division and subtraction. Does anybody recognize this? It is a sum of geometric sequence. For n = 0 to infinity a to the power of n. +BEGIN_SRC ~ = E a^n = 1 + a + (a * a) + (a * a * a) + ... n=0 ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ [] [a] [a,a] [a,a,a] +END_SRC What is not a trick or is a little trick that can be explained later is that this equation can be solved by substitution. This expantion ends in a /fixed point/. This can be formalized in a fixed point combinator. So this is how you get a list. +BEGIN_SRC L(a) = 1 + a * (1 + a * L(a)) = 1 + a + a * a (1 + a * L(a)) +END_SRC This is why these are called /algebraic data types/. We will get to the point when we do exponentials. Then our algebra will be really really interesting! 6.1 Functors So today I want to talk about functors. So all previous lectures were just introduction to functors. Mathematicians will say /natural transformations/ are important and you need functors for it. Why are they important? There are things in category theory that are formalized that we thought were not. What is really universtal construction about? It about being able to define what is means to be a perfect embodiment of a idea, to be an ideal. It's like we say how do we define a product. Well we have all these possibilities, how do we pick one? Well we just say let's pick the best one. And we have these two types of universal construction, one for product one for coproduct and they sort of define slightly different ways what is more ideal than other thing. And the property of this perfect thing there is a morphism coming from any other candidate (product) which means anything else you throw at me can be /distilled/, any two objects with projections and I can distill it into this perfect thing called a product, finding a unique morphism. With a coproduct it means here is an ideal coproduct and any other candidate has an image in it because there is a morphism from this perfect ideal thing down to any other candidate that's not perfect, that kind of embedds it, finds this element of perfection. So functors now. Mathematically speaking it's a simple idea, Functor is just a mapping from one category to another. When we talked about products and coproduct I used a loose language, looking for a pattern. So we have this pattern which constists of single object (terminal, initial) or an object and two morphisms (product, coproduct) and now we are trying to find this pattern, match this pattern in our big category. What does it mean? So pattern recognition, category theory tells us how to formalize /pattern recognition/. Well if you want to recognize a pattern inside a category, you have to define what you mean by a pattern, a pattern must be some kind of structure and you sort of have to map this pattern into this category but you have to map it in such a way that you recognize this as a pattern, meaning you have to preserve the structure. What is structure? Well category is a definition of structure, its pure structure, dots and arrows. So if you want to say I want to recognize a certain structure it means you want to define your pattern as a category. So if you want to map one category to another category you first map the objects (small category example) and since objects form sets in a small category its just mapping of sets, and thats a function. But there is something funny about functions that i have not mentioned, that functions are sort of primitive, trivial. What we are really interested in are mappings that preserve structure and it so happens that function are mappings between sets and sets have no structure They have just a bunch of objects. It's really hard to implement something that totally disorganized on top of hardware thats organized so people implement trees. But in order to implement a tree you need to be able to compare elements.. what is important is to find mappings that preserve structure . We are not really used to thinking about what does it mean to preserve structure. /A discrete category corresponds to a single set/ Any category that is no discrete by definition has structure. So if we want to preserve structure our mapping has to have arrows. So first we preserve mapping between objects. +BEGIN_SRC C D a -------------- F ------------------> Fa |\\ | \\ | \\ | \\ f g.f | Ff \\ | \\ | \\ F(g.f) | | \\ b------------------------------------> Fb \\ \\ \\ \\ Fg \\Fb \\ \\ c Fc C(a,b) ----------------------------> D(Fa, Fb) F (g.f) = Fg . Ff +END_SRC /Hom-set/ C(a, b) /Hom-set/ D(Fa, Fb) A functor is /huge/ potentially number of separate functions. One function per every hom-set. And I haven't even talked about preserving structure, which is defined by composition. I have to map composition I want to preserve structure. A /functor/ is this kind of mapping of objects and morphisms that preserves composition and identity. This is sort of obvious way to define a functor. And this also formalizes this idea what does it mean to preserve structure. So functor is something that preserves structure. What else can be say about this functor. Whenever objects are connected in source category they will be connected in target category. Doesn't mean every morphism will have a corresponding morphism, it doesnt have to be surjective of injective, but we can never destroy connections. If you done calculus it is sort of like /continuous transformation/. Essentially thinking about this continuity. Otherwise you can shrink things, you can collapse things, and in particular you can define functors that don't break things. If the mapping of /hom-sets/ is /injective/ than we call such mapping /faithful/. So a /faithful functor/ is injective on all hom-sets So a functor is /full/ when /surjective/ on hom-sets. So it can collapse objects and still be /injective/ on morphisms. Or it can map the whole category into a tiny category and still be surjective on hom-sets. The most beautiful is the /fully faithful functor/, the one that preserves, an isomorphism on hom-sets. Surjectivly, injectivly. What's a functor whose source is just a one object category (will have to have an ~id~)? So if we map it inside another category, well it has to be mapped into an identity as well, functor must map identity to identity. So that's equivalent to just picking just an object in this category. Just like we had with functions, from terminal object to another object. The other way around is a bit interesting. So, a functor that just mapps every object in this category into one single category, so it collapses every single, the whole category into a single object, a black hole, and all morphism collapse into one identity morphism. This is an important functor. This is called /constant functor/. So this one is called ~delta C~ from c to c. Very important. OK, now what does all this have to do with programming? Most common functors in programming will just deal with single category because that is the category of types and functions. But I never said that C and D have to be different categories. In principle it can be the same category. Objects in this category can be mapped into the same objects in that category. That's called an /endo-functor/. (going inside) In Haskell these endofunctors are just called functors. So what is a functor translating this (image above)? A functor has to be a mapping of types, a total mapping, every object has to have an image, which means it's a /type constructor/ but that's just one part of a functor. The type constructor is a mapping on types, function that works on objects, but it also has to map morphisms which means it has to map functions, so let's just grab some examples and see how we can define, starting from a type constructor. +BEGIN_SRC haskell data Maybe a +END_SRC So a is a parameter, so for every type a we are defining a new type a. So we are mapping types to types. +BEGIN_SRC haskell a ----- Maybe -----> Maybe a data Maybe a = Nothing | Just a +END_SRC Is this a functor? We need to define functions between hom-sets. If we have a and some b, b will go to some Maybe b, we have to define a function that goes from Maybe a to Maybe b, so if this is f this is the mapping of f using the functor. This mapping of functions is called fmap of f +BEGIN_SRC Maybe a -----------------> Maybe a | | | | f | fmap f | | | | b -----------------> Maybe b +END_SRC +BEGIN_SRC haskell fmap :: (a -> b) -> (Maybe a -> Maybe b) fmap f Nothing = Nothing fmap f (Just x) = Just (f x) +END_SRC fmap for Maybe must be of this signature. What can we put instead of ~fmap f Nothing = Nothing~? .. mental block.. We could say.. It's nothing unless the type a is Integer, Just zero? Why not? There is somthing called /ad-hoc/ polymorphism but we don't want to use it here. We are kind of straying from mathematics when programming with Haskell. We are actually imposing stronger condition, parametric polymorphism, we are making it restrictive, leading to /theorems for free/. This is something that says because in Haskell can actually only implement functions of some limited kind, a certain type of polymorphism, that imposes the conditions of what we can do. * 6.2 Functor II So we have defined a functor. Maybe we have maybe we haven't :) How do we know this preserves composition and identity. Well we cannot express this in Haskell, in type system we cannot encode these conditions. Unlike in other languages there is a way of using haskell on a whiteboard to prove things about the language. We would like to prove this functors preserves identity. ~fmap id = id~ Now this id works on a different object than this id. ~fmap id a = id Maybe a~ and we want it to preserve composition: ~fmap (g . f) = fmap g . fmap f~ What does it mean functions are equal? They have equal values on equal arguments. Whats so special about Haskell is that every definition in Haskell is an equality. And it means what is says. These two things are equal. It is an equation. Left side is same as the right side. In programming this is sometimes called /inlining/, but if you have pure functions you can do /inlining/ and you can do the other way around, /refactoring/, turning an expression into a function call. When ~fmap~ acts on ~id~ it produces a function from ~Maybe a~ to ~Maybe a~, right? So I have two cases to check, this ~Maybe~ could be a ~Nothing~ or it could be a ~Just~ ~fmap id Nothing = Nothing = id Nothing~ /see what did here? I did refactoring, replacing Nothing with id Nothing, so this checks/ ~fmap id (Just x) = Just (id x) = Just x~ ~id (Just x) = Just x~ Let's talk how to define a functor in general in Haskell. /Lifting/ +BEGIN_SRC Maybe a--------> Maybe b ^ fmap f ^ | | | | | | a ------------> b +END_SRC fmap is a higher order polymorphic function. It's not like you write one formula for fmap for all functors. So now you are seeing a different kind of polymorphism, in which depending on what your parameter is, in this case the functor, you get a different implementation of a function, fmap in this case. So this is an example of /ad hoc/ polymorphism. It's just we use a slightly different tool for /ad hoc/ polymorphism which is called a /typeclass/. A Typeclass is, you define a whole family, or a class of types that share some common interface. So in Haskell this is called a class. +BEGIN_SRC haskell class Eq a where (==) :: a -> a -> Bool +END_SRC So every type that supports this operator, that takes two a's and produces a boolean, but ~(==)~ is one name that will serve us for many different types, and its implementation will be different for every type, you implement equality different for integers, different for strings. So that's /ad-hoc/ polymorphism. Functors are actually /type constructors/. So Maybe is a functor, because it takes a type and produces a type. So if we want to define a functor we have to define it as a class. +BEGIN_SRC haskell class Functor f where fmap :: (a -> b) -> (f a -> fb) +END_SRC /So f here is actually a type constructor! a is a type, b is a type, then f must be something that acts on a type to produce another type./ The most intuitive example of a functor is list. +BEGIN_SRC haskell data List a = Nil | Cons a (List a) instance Functor List where fmap _ Nil = Nil -- h type of a, t type of list of a fmap f (Cons h t) = Cons (f h) (fmap f t) +END_SRC fmap = map /map is just a particular implementation of fmap for lists/ But lists came earlier before functors, so they already had this map defined, but it's really fmap. +BEGIN_SRC haskell type Reader r a = r -> a +END_SRC The arrow itself ~(->)~ arrow takes two types ~r~ and ~a~ and produces a type of function from ~r~ to ~a~. Now, so far we've been talking about these type constructors that just take one type as an argument, and here we something that takes two types. But we can always just fix one type and say, we only care about the second type. We fix the arrow and we say let's just /vary/ ~a~. +BEGIN_SRC Reader r a -----------> (r -> a) +END_SRC First one is fixed to bool for example and second one varies. This is called partial application, currying. +BEGIN_SRC haskell g f f g (r --> a --> b) ^ ^ ^ ^ fmap :: (a -> b) -> (r -> a) -> (r -> b) -- functor acting on a functor acting on b fmap f g = f . g = (.) f g -- I cross f g on both sides fmap = f . g = (.) fmap = (.) +END_SRC What is the general intuition behind this? I still haven't showed you identity or const functor. There is one intuition that works for endofunctors, some say it's bad some say it's good. The intuition is that a functor when it's acting on some type encapsulates, hides, the values of this type, so an element of the type Functor of a has elements of a in it, and something that has something inside is usually called a /container/. List is a container, it contains a's, a list of integers, a /tree/ which is a functor too is a container of objects, a vector is a container of elements and it's a Functor. But then there are these Functors that are problematic, like Maybe, it may contain an a or maybe not. A container can be empty? It kinda works and this idea that something is a container, what does it mean to apply a function to the contents of the container, just open this container, look at this stuff and apply the function. So this is what we did with Maybe, if it contains Nothing do nothing well if it contains an a just do this. But then we have this Reader guy, how is this a container? Look at a function that a Boolean and returns some other type? How many possible values does this function have. Two, true and false, so can I say it is a container of two values? I can memoize this function, replace it with a table lookup, which contains these two values. What about a function of integer, it is just an infinite sequence of ints, maybe I cannot memoize the whole thing, maybe I can partially and so on. So this distinction between a function and a data type, if you think about it is, /weak/. A list is a container, ok in Haskell I have a list from one to infinity ~[1..]~, obviously I cannot store it in memory, how is this implemented? As a function. All data types in Haskell, are /thunks/, they are function that can be evaluated to return a value, data are really functions, functions are really data. And we will talk about what function types are in category theory and you will see that it is actually an exponential, which is a data type. The only thing about Functor that's important is that you can apply a function to what it contains, there is no way at least the Functor does not provide you a way to retrieve this value, that's not a part of definition of a Functor. I want to leave you with this idea, that functors, /endofunctors/ are containers. 7.1 Functoriality, bifunctors Remember, a functor is like lots of functions put together, there is one major function that mapps objects and that's an actual function only if the category is /small/, objects form a set and functor is just a function on objects, but it also has to /preserve structure/ and that is the most important part, we learned what it means to preserve structure, so functor maps not the only objects but also mapps connections between objects which are morphisms. So for every connection between objects we have this set of arrows between them, which we call a /hom-set/, and as these two ends of a hom-set are mapped from one category to another we define also a mapping of these morphisms between hom-sets, and since hom-sets are sets in a /locally small category/, that's also a function so for every hom-set there is a function that maps it to the corresponding hom-set in the second category. That means preserving connection between objects. Since functors are built from functions, we know that functions compose. Category in which functors are morphisms and categories are objects is called /Cat/. Let's combine two endofunctors we know about, ~Maybe~ and ~List~. There is this function called ~tail~ and tail takes a list of some a's and returns a list of a's. ~tail :: [a] -> [a]~ It's defined so that it just throws away the head of the list, so it's well defined only for lists that are not empty. What if the list is empty? The program dies. The only reason people use it is because it's more optimal, because otherwise you always have to check is it empty? Otherwise it is the /achilles heal/ of ~Prelude~. But if we want to be sure, then let's define something called safe tail, that takes into account the possibility that list can be empty. +BEGIN_SRC haskell safeTail :: [a] -> Maybe [a] safeTail [] = Nothing safeTail (x:xs) = Just xs mis :: Maybe [Int] sq :: Int -> Int fmap (fmap sq) mis (fmap . fmap) sq mis +END_SRC Most type constructors that you woul normally used are sort of automatically functorial, what it means is that you have a type constructor and defining fmap for it kinda falls out automatically, it's like most data structures are regular in this sense, what it means, that algebraic data structures are automatically functorial. How do you define them? You form them using things like products or sums and you apply it to a unit or a type variable. If you create an algebraic data type using these operations then you automatically have a functor, so we have to start by asking ourselves is a product a functor? So a product of two types ~(a,b)~, this can be rewriten as a type constructor acting on a,b ~(,) a b~ so we could ask is this a functor in b? If we fix a. It kinda is. It's a type constructor. We can construct a type by pairing it with some fixed type. If a function goes from ~a -> b~ we can lift it ~(e,a)~ and ~(e,b)~ with ~fmap f~ where this fmap takes ~fmap f (e,x) = (e, fx)~. We can define something that is a product of two categories then a functor from a product of two categories would be equivalent to a functor of two arguments, one from one one from another category. We know in category theory products are these beasts and now I'm saying I want to do something bigger, a product of two categories! It turns out that a product of two categories is easier to define than a product from two objects in a category. OK, let me take two categories C and D and we take objects from C and D and we form a pair ~(c,d)~ that we call ~CxD~ (/c cross d/) in which objects are pairs of objects so really the objects are cartesian product of the sets of objects. What else do I need? Morphisms. And again, this is easy to do this pairing, I can do cartesian product of this hom-set and this hom-set. So I have a new category, called a /product category/. +BEGIN_SRC (f', g') . (f, g) = (f'. f, g'. g) (id a, id b) = id (a,b) +END_SRC Now when I have a product category is just a category, now I can define a functor that goes from this category. ~C x D -> E~ It means for every pair of objects I pick and object in E. And on morphisms, a morphism in C cross D is a pair of morphisms paired with a morphism in E. This functor is called a /bifunctor/. A bifunctor is a functor from a product category. In haskell we would have to lift a morphism from a product category. This would be a product with a same category with itself. ~C x C -> C~ but notice that we are actually talking about a functor that's not set from set, or ~hask~ to ~hask~, it's a functor from some other category to hask, a product of two hasks, so we are already getting outside of hask and getting into /hask \"squared\"/ What does it mean in terms of function we know? We have a mapping from two types into a type. That looks sort of like this ~(,) a b~. What about morphism? It is a pair of functions, we are lifting two functions /at the same time/. So if we want to define a bifunctor in haskell and just like functor it will be defined as a class. It will have to have this way of lifting two functions at the same time. This higher order function corresponding to fmap will be called ~bimap~ +BEGIN_SRC haskell class Bifunctor f where bimap :: (a -> a') -> (b -> b') -> (f a b -> f a' b') +END_SRC OK, product is /bifunctorial/. What about /sum/? mmm... Do we have to go through this construction and come up with a sum of two categories? What happens is that, we can use the same bifunctor idea for a sum so ~Either a b~ - thats the sum type, the canonical sum type of two types. Either a b is actually a bifunctor! So we can define the action of two actions of either a b. So Either a b takes a pair of types from a product category! It is a function of two arguments, takes two types and produces a third type of ~Either a b~. What we want to have in general is that if we have a product in a category, then this product /is/ actually a bifunctor and its a bifunctor of this type ~C x C -> C~. Here we have two examples in /hask/ one is the product in hask one is in hask they both are bifunctors like this. So a coproduct is also a bifunctor. +BEGIN_SRC a x b / . \\ ------+ p / / . \\ \\q == ------> / / . \\ \\ ......+ / / . \\ \\ + f.p / . \\g.q + ......+ = f x g a / + \\ b | / a'x b' \\ | | / / \\ \\ | f| / / \\ \\ |g | /p' q' \\ | +/ \\ + a' b' (a, b) -> a x b (C x C) -> C +END_SRC 7.2 Monoidal Categories, Functoriality of ADTs In a monoidal category we would like to define what does it mean to multiply two objects, ha? (waves hands) :) So a product, categorical product is sort of way like multiplying object, we already have one part of a monoid, we have this binary operation, on object, right? What was the unit for product in Haskell. It was the unit type :D which is a singleton set (in set theory). How do we define a singleton set? Terminal object. Is this terminal object maybe good candidate for a unit in our newly formed monoid structure? It would mean if you construct a product in which you have some object ~a~ and you have this terminal object ~()~, so ~a x ()~ with ~a~ and ~()~ projections. So I want to prove that a is actualy product of a and terminal object and if you multiply a by terminal object you get back a. What are the projections here? ~id a~ and this is ~unit~, it's a good candidate, is it the best candidate? Let's try some other, a ~b~, a /candidate/ is really a triple, its an object plus two projections. It has to have a projection that goes to a, lets call it ~p~, and and one that goes to unit, call it ~unit b~ so in order to prove this is the best guy I have to show there is a unique morphism from ~b -> a~ +BEGIN_SRC b / | \\ / | \\ p / |p \\ unit b / + \\ / ida / \\ \\ / / \\unit a \\ + / \\ + a + + () +END_SRC A product is defined up to unique isomorphism. So a categorical product, then you have this new structure, monoidal structure on objects, I can do the same thing with a coproduct, the unit would be the initial object, so that would also be a monoidal category with coproduct and initial object, in general maybe there are other things like this, but what we really need is a product which is a /bifunctor/, we need this binary operation on object, and we need this unit for the bifunctor and we get the monoidal category. What is a good name that could be a coproduct or a bifunctor, a good name is a /tensor product/. (writen as circle with a cross inside). I started all this discussion because I said ADTs are functorial. So product and coproduct are functorial. What else we use to construct data types? We can construct a datatype that does not depend on a data type. We have this way of constructing a trivial functor from a constant object, called a /Const Functor/, that takes two data types. Constant functor maps every object in one category into a single object in the second category, like a black hole, called ~delta c~ so in Haskell: +BEGIN_SRC haskell data Const c a = Const c instance Functor (Const c) where -- fmap :: (a -> b) -> Const c a -> Const c b fmap f (Const c) = Const c data Identity a = Identity a fmap f (Identity a) = Identity (f a) data Maybe a = Nothing | Just a -- Either () (Identity a) -- ^ -- ^ Const () a +END_SRC There is an extesion in Haskell, ~{-# LANGUAGE DeriveFunctor #-}~, then you can just say ~data Maybe = .... deriving Functor~ and the compiler will derive you the correct ~fmap~. There is one more type constructor that takes two arguments? Right! Function, the arrow. ~(->) a b = a -> b~ So arrow is a type constructor, it takes two types a and b and produces a third type which is a type of functions from a to b. Now strictly speaking I have not yet talked about function types, next lecture.. +BEGIN_SRC haskell newtype Reader c a = Reader (c -> a) +END_SRC Is this a bifunctor? Well, let's just check if we fix the second, because here we are fixing c, this argument type, what if we instead fix the return type and vary the argument type, can we create fmap for it? +BEGIN_SRC haskell data Op c a = Op (a -> c) fmap :: (a -> b) -> Op c a -> Op c b -- ^ a -> c b -> c -- ^ -- wrong arrow +END_SRC a -> c, a -> b, makes b -> c not good, we need b -> c to make it work. This kind of functor that works on the hask on inverted arrows is called /contravariant/ +BEGIN_SRC haskell class Contravariant f where contramap :: (b -> a) -> (f a -> f b) +END_SRC A contravariant functor, its not like a container, its sort of like a negative container, not only it does not contain it just actually requires a's for its action, needs \"fuel\" of type a. If you say instead of a's I will be providing you b's then you need to show how to convert b's to a's so that the functor accepts the a's. /\"It contains the empty matter of type a, so you have to have a warp converter\" :D/ The arrow is a covariant functor in the second argument in the return type and its a contravariant functor in the first argument. Now if you combine these two things, another interesting thing, arrow itself as a functor thingy ~C^op x C -> C~, you take a pair of morphism but the first one is flipped, so a thing of this kind of type is called a /profunctor/. Why is it called profunctor? I don't know.. +BEGIN_SRC haskell data class Profunctor p where dimap :: (a' -> a) -> (b -> b') -> p a b -> p a' b' -- f g (a->b) (a'->b') -- h -- result: g . h . f {- a' ---f---> a ---h---> b | / result | / | / g | / b' / +END_SRC An arrow is indeed the simplest profunctor. * 8.1 Function objects, exponentials Functions are separate from types. So far we've worked with this model in which types are objects in our category and morphisms are functions, types are objects soo.. functions are not on the same footing as types, right? If you are working in the category of set, which is approximation of what we do in programming, you can think of, you have object a and object b, functions between form a /hom-set/ so if types are sets, then functions are sets too. So this is sort of like sefl-referential thing about this particulary category of sets, so hom-set which is a set of morphisms between objects is also a set. But that's not generally true. In an arbitrary category we don't have this object, what we have is the /hom-set/ being an /external thing/. A hom-set being actually member of set. What we would like to have is an /internal hom-set/, an object in a category that corresponds to the sets of morphisms between two objects, a and b, somehow represents this set of morphisms. And it's possible to define it. So how would we go about this universal construction for a function object. First we define a pattern, then we define a ranking for matches and then we find the best match. So this /pattern/ for a function object must involve two types, argument type and return type, object a and b and the third part would be the third object, the candidate function object, let's call it ~z~. We have to have some kind of connection. What would define for us the action of a function on a argument that produces a result? We have to find a morphism between some of these objects that would represent this idea. But it is a relation between three objects. So we would like to put an arrow between these two objects and the third object, we can't do it in a category. How to do it in sets? Let's pick an element of this set that would be an argument and form a pair, function and an argument, this pair can be mapped to result, which is an element of b. So pairing in sets corresponds to taking a cartesian product of these two guys, ok so we can generalize it to a category called product and say ok, so we have a product +BEGIN_SRC z' z' x a |--| |-------| | | | |\\ | | | | \\ |--| | | \\ | ------- \\ h | | \\ | h x id| \\ g' z + + \\ |--| |-------| \\ | | | z x a | \\ a->b | |..| | \\ \\ | | | | \\ \\ ---- ---.---- \\g(eval)\\ . \\ \\ |--------| \\ \\ | | +--------| ---------- | | a ---------- b +END_SRC +BEGIN_QUOTE So thats the pattern we are looking at. But notice, that in order to define this pattern, we have to have a product in our category. And thats a very important thing. In order to define a function object in a category, you have to first define a product. If a category doesn't have a product, then we cannot perform this construction, and you will see later that it actually makes more sense when you think about function object as algebraicly as exponential, because an exponential is like iterated product, right? So if you don't have a product, how can you have an exponential? So thats the idea behind this. +END_QUOTE In the end we call this /eval/, that morphism is called evaluating a function. So thats the first thing to do. The next thing is ranking. So suppose that we have another candidate, ~z'~ z with g is better than z' with g' only if there is a unique morphism h from z' to z such that this diagram commutes ~g' = g (eval) . (h x id)~ And finally the third part of universal construction is picking the winner, the function object. ~a~ changes name to ~a -> b~ and ~g~ to ~eval~ meaning that for any other candidate ~z~ that has this function ~g~ from ~z x a~ to ~b~, there is a unique morphism ~h~ that maps z to ~a -> b~ such that this triangle commutes. /We can think of g as a function of two arguments, f(x,y,z)/ But now we are seeing something, that a function of two arguments, is equivalent to a single function that takes an argument and returns a /function type/, (hand points to z and then a -> b). There is one to one correspondence between g and h, so I have equivalence of /two ways of thinking/, one way of thinking I have a function of two arguments as a function that takes a product, and the other one its a function of one argument but it produces a function. And that's called currying. ~h :: z -> (a -> b)~ because in Haskell this function object is really represented by an arrow. And ~g :: (z, a) -> b~ +BEGIN_SRC haskell curry :: ((a, b) -> c) -> (a -> (b -> c)) curry f = \\a -> (\\b -> f(a, b)) uncurry :: (a -> (b -> c) -> ((a, b) -> c) uncurry f = (a,b) -> (f a) b +END_SRC In category theory people often don't call this a function object, they call it an exponential. So a function from a to b would be called b to the power of a, b^a, argument goes to the top, result goes to the bottom. If we have a function that goes from ~Bool -> Int~, so its really a pair of integers, one for false, one for true. So all possible functions from Bool to Int well there are just all possible pairs of Ints, so this is really a cartesian product of ints. So you can write it as ~Int x Int~ or ~Int^2~ (/Int squared/) so also Int to the power of Bool like 1 - () 2 - Bool ... The number of functions from a to b is really b to the power of a, if you look at all possible combinations, so by counting the number of possible functions you get the counting argument, and also this shows you the connection between product and exponential. An iterated product gives you an exponential. What we do want for programming in which we have exponentials, or function types. We definently want products. There are special kinds of categories called CCC /cartesian closed categories/ that are useful in programming. Cartesian category is one that has products for every pair of objects, /closed/ means it has exponentials as well, for every pair of objects a and b it has an exponential a to the power of b, and it also has a terminal object. Terminal object is like a /zeroth/ power of an object. Its like the first power is the object itself, second power of the object is ~a x a~, then we can have these exponentials, but the zero power is terminal object. We actually want a little bit more in programming, we want /coproducts/ and /initial object/ so something that has not only cartesian products but also coproducts is called BCCC /bicartesian closed category/ and in a BCCC we can do our beautiful algebra of types. So far we've seen the algebra using products, coproducts, initial and terminal objects, first we saw that products form a monoid, coproducts form another monoid but we can combine them and they give you this /semiring/ and now we are adding exponentials. With exponentials we can do more algebra! For instance, whats a to the power of zero ~a ^ 0~? One. ~1~. But is it true for types? What is zero? Thats our initial object, it is ~void~ ~a ^ 0 = 1~ ~Void -> a ~ ()~ ~absurd~ So its a function from void to a, the right hand side is /One/, one is a terminal object, that the unit type. Are these two types equivalent. A unit type is a type that has one element. So if this is a singleton as well then we are done. So first of all, is there a function from void to a and how many are there? Well we seen this function it is called absurd. It takes a void and produces an a, so there is a function likee this, absurd. What about ~1 ^ a = 1~ This is a function that takes an argument of type a and produces a unit ~a -> () ~ ()~ so there is only one function like this, it maps all elements of a into this unit element, this is collapsing, a const turning everything into a single values. ~a ^ 1 = a~ First power of an object is the object itself, but it also has this meaning ~() -> a ~ a~ unit to a is isomorphic to type a and remember what this is, its a function that takes one element from a. There is one to one correspondence, I call it generalized element (mathematicians call it a global element) 8.2 Type alegbra, Curry-Howard-Lambek isomorphism. ~a^b + c = a^b x a^c~ ~Either b c -> a~ ~(b -> a, c -> a)~ ~(a^b)^c = a^b x c~ ~c -> (b -> a) ~ (b, c) -> a~ Currying! ~(a x b)^c = a^c x b^c~ ~c -> (a, b) ~ (c -> a, c -> b)~ Sort of like you have to learn just one thing, and then everything else kinda falls out. So probably like the best thing is to start, instead of going to highschool, just start with category theory and then everything else will just follow from this. The other thing that, exactly the same structures for types and categories appear in logic. And thats the basis of famous Curry Howard isomorphism, or sometimes called propositions as types. So this isomorphism between type theory and programming in general and logic on the other side starts with identifying what it means to, what is a proposition. In logic is a statement that can be true or false so these propositions correspond to types in programming. Just as a proposition can be true or false, type can be inhabited or not, so the truth of a proposition means that type that corresponds to it has elements, members, its inhabited and most of the types we deal with are inhabited, so they are kinda like true propositions but there are types that are not inhibited, and they correspond to false propositions, and we know one such type, thats void. So if you want to prove a proposition you just have to prove that a type has an element. In logic there are these two basic values, true and false. So the corresponding things in type theory would be void type which corresponds to false, and unit type which corresponds to true. Unit type is always inhabited with one element. |---------------------------------+----------+---------+-----------------+---------------------+--------| | Curry Howard Lambek isomorphism | | | | | | |---------------------------------+----------+---------+-----------------+---------------------+--------| | /Logic/ | true | false | and | a or b | a => b | |---------------------------------+----------+---------+-----------------+---------------------+--------| | /Types/ | () | Void | (a,b) | Either a b | a -> b | |---------------------------------+----------+---------+-----------------+---------------------+--------| | /Category/ | terminal | initial | a x b (product) | a sum b (coproduct) | b^a | |---------------------------------+----------+---------+-----------------+---------------------+--------| (a => b), a) -> b a => b and a -> b /modus ponens/ 9.1 Natural Transformations Triad of things that are foundations of category theory: 1. Category 2. Functors 3. Natural Transformations. Category is about structure, what it means. Functors are these mappings between categories that preserve structure, intuition is that they take a category and embedd it inside another category, sometimes called modelling. What if we have two different functors? How are two images related? So we would like to be able to compare images given by functors. So natural transformations are defined as mappings between functors. And these mappings have to preserve structure. Let's start with two categories ~C~ and ~D~, lets concentrate on a single object in category C, object ~a~. One functor maps this object into ~F a~ in category D, second functor ~G~ maps the same object to some object ~G a~. A /natural transformation/ would be picking a morphism between these two objects (In D, one morphism from this /hom-set/, ~Fa -> Ga~. In this way I am creating a whole family of morphism, these are /components/ of natural transformation. So the related morphism in D would be called ~alpha a~, between ~Fa -> Ga~ Now we map object b the same way ~Fb -> Gb~ and we have ~alpha b~ Having a natural transformation between two functors means they are somehow related. See naturality square +BEGIN_QUOTE personal notu: This seem like an abstract jump to a higher level. Feeling kinda lost now but am continuing to watch the lecture. Need time to process and contemplate. Same actions are used but all together somehow making a much more complex /image/. As Bartosz says natural transformation gives us a higher level language of commuting diagrams. Notice natural transformation is a higher level language in category theory while just describing commuting diagrams is like /assembly language/. And then instead of just talking about commuting diagrams you begin to notice relations between functors. In this sense products and coproduts are just a case of limits and colimits and later there will be adjunctions etc.. +END_QUOTE In programming, natural transformation would be a family of functions between endofunctors, parametirized by a type, so a natural transformation would be a polymorphic function. +BEGIN_SRC haskell alpha :: forall a . Fa -> Ga +END_SRC The subtle difference being, in this haskell example, in this form, we are assuming parametric polymorphism meaning if we want to define this function we must use one single formula for all a's. We cannot say do this thing for integers and do this thing for booleans. One single formula for all. And this is /much stronger/ than a categorical definition. +BEGIN_SRC haskell alpha . fmap f = fmap f . alpha +END_SRC Let's pick a list functor and a maybe functor. We talked about safe tail, lets talk about safe head. +BEGIN_SRC haskell safeHead :: [a] -> Maybe a safeHead [] = Nothing safeHead (x:xs) = Just x +END_SRC This is a function that works for every ~a~, a total function, parametric polymorphic so it is automatically /natural/. But lets prove it, lets do equational reasoning on it. Show it on both empty list and non-empty list. +BEGIN_SRC haskell safeHead . fmap f [] safeHead [] Nothing . Nothing Nothing safeHead . fmap f (x:xs) f x : fmap f xs Just (f x) \\ \\ safeHead (x:xs) \\ Just x - - -Just (f x) +END_SRC If you look at it it is actually an /optimization/. Applying an fmap on a list is expensive, so being able to safeHead first and then fmap is cheaper, of course not in Haskell, because Haskell is lazy. Basically we use a lot of natural transformations in programming so ~a -> [a]~, thats actually a naturally transformation because ~a~ is just an identity functor acting on ~a~. There are also these function that take polymorphic object and return a number, like ~length~ of a list. Takes arbitrary type of list and returns a length. Thats also a functor, the ~const~ functor which ignores its argument. Natural transformation from a list to a const functor. If you have a function from one ADT to another ADT its a natural transformation because algebraic data types are functors. Not all are because we have these /contravariant/ functors, so if we have a polymorphic function which turns weird stuff it would not be a natural transformation. At some point you might learn about generalized natural transformations which operates on mixed convariants. +BEGIN_SRC haskell return :: a -> m a +END_SRC is actually a function from identity functor to ~m~, a natural transformation and when we talk about monads this thing will be defined as a natural transformation, so a Kleisli category really has to take into account that these transformations are natural. 9.2 Bicategories OK, lets talk about category theory but just like in a really really wide area, I want to give you the view of category theory as far as I was able to look into it. Because we have these basic part of category theory, categories, functors and natural transformations. Natural transformations are mapping of functors, now every time we have mappings of things we ask ourselves do these mappings compose? That should be the first thing on our mind. What does it mean to compose natural transformations? \\alpha F -> G \\beta G -> H \\beta . \\alpha Is this a natural transformation? We have to check, we have to have another object b a -> b -> (-> F G H) -> (Fa -> Ga -> Ha) -> (Ff Gf Hf) -> Fb -> Gb -> Hb Notation used is [C, D] or D^C, that suggests something :) Note: Feeling lost now, but following along. Its all abstracted while using the same words like before, product, category, functor, it seems simple yet still not reachable, like being there but still blind not able to remember the last movement, thus feeling lost in a known environment. ... 10.1 Monads A monad is really such a simple concept. Why do people have problems with monads. It's the wrong approach of explaining what a monad is. Suppose you explain what function is, it like you go to a tropical island, there are some friendly natives and they want to learn functional programming from you and they ask you what is a function? You try to explain and give them an example, like you can have a function that takes a list of fisherman and orders them by the number of fish they caught, or another function takes person and gives you their age, or another function that takes grain and gives you alcohol, right? And they say wow this is some amazing stuff, it does all these things, and more? Function is some powerful.. then you realize maybe thats not the best approach of explaining what a function is. Maybe we try this, function is like an animal, it has a mouth and it eats input and produces output on the other side, thats a better analogy but you get in big trouble when you try to explain function composition (laugh :D) Why do we use function? So that we can structure our program, so that we can decompose our program into smaller pieces and recompose it, and the power of function is really in the dot. Thats what the power is. Dot is the composition operator in Haskell, combines the output of one into input of the other. Functions are about composition, and so is the Monad. People start by giving examples of monads, there is state monad, there is exception monad, you know, thats, these are completely different things, what do exceptions have to do with state, with input output? Well its just like with functions, functions can be used to implement so many things, but really functions are about composition and so is the monad. Monad is all about composing stuff. It replaces this dot with the /Kleisli/ arrow. ~>=>~ The so called fish operator. Ok dot ~.~ is used for this simple functions, where output matches the input, the most trivial way of composing stuff. The fish operator is used to compose these functions whose output type is /embellished/, it really the output type of a function type would ~b~, but now we are embellishing it with some stuff, with logging, by adding a string to it, the logging Kleisli arrow, but then in order to compose these things, we have like unpack the return type before we can send it to the next so actually inside the /dot/ not much is happening, just one function is called and the result is passed to another function. Inside /fish/ much more is happening because there is unpacking and there is the passing of the argument to the next function, and also, maybe some decision is taken, like in the case of exceptions we will see that. Once we have this additional step of combining functions we can make decisions, maybe we don't want to call the next function at all? A lot of stuff can happen inside the fish. And just like we have the identity function here, identity with respect to the dot, here we have this kleisli arrow that represents identity that return the embellished result of the same type and we call it ~return~ in haskell. And its called return because at some point you want to be able to program like imperative programmer. It's not like imperative programming is bad. It could be good as long as its control, and the monad lets you do in this kinda imperative style, sometimes is easier to understand your code even though it is immediately translated into this style of composing function, so this is just for our convenience. Using the kleisli arrow is equivalent of using the dot. We don't see many programs even in Haskell that are written using dots, this is called point free style where you just compose function after function and never mention arguments to these function, they are hidden, they are not given any names, they just go straight from output from one function to the input of another function. Point free style is popular with some people but it is considered hard to read. So this definition of a monad with fish operator is not the main definition that is used in other languages. ** Fish anatomy +BEGIN_SRC haskell ~(>=>)~ :: (a -> m b) -> (b -> m c) -> (a -> m c) f >=> g = \\a -> let mb = f a in mb >>= g |--> (>>=) :: m b -> (b -> m c) -> m c | | | class Monad m where | (>>=) :: m a -> (a -> mb) -> m b | return :: a -> m a | ------------------------ | class Functor m => Monad m where | join :: m(ma) -> ma | return :: a -> ma | | -- but mathematicians go deeper into the fish, how to implement | -- bind? | |--> ma >>= f = join ( fmap f ma) / (a -> ma) m a / m (mb) / / / join :: m(ma) -> m a +END_SRC Remember this is all polymorphic in a, b and c. A, b and c are completely arbitrary types, we have to be able to define the fish operator for /any/ type a,b,c. Once you say this is any type it means you cannot do anything type specific, which means you cannot do anything :D except! you have a function here that takes an ~a~, thats like you only chance to do something to a, well apply ~f~ to ~a~. So the sytax is ~let mb = f a~ like defining a local variable in other languages, in Haskell it just means giving a name, binding a name to some value. Why are we using these monads? Monads are used to provide composition for kleisli arrows, but why kleisli arrows? The magic is not in what a monad is, its just composition, the magis is why are these kleisli arrows so useful, what kind of problems they solve and why? The idea was that functional programming, everything is a pure function, and as programmers we know pure functions cannot express everything, there are so many computation like basic things, input output is not pure, a function that getChar if it returns a different character, it returns a different character every time you call it, so its not a pure functions. Then there are functions that throw exceptions. It turns out and this is a miracle that everything that can be computed using non-pure function, they can all be converted as long as you replace pure functions with functions that return an embellished result. This result is encapsulated in some weird way, this is the interesting part. Where does the monad come in, when we say I have this gigantic function that starts with argument a and produces this embellished result, and do I have to write it all inline? No, I want to chop it into pieces, into hundred side effects and then compose it. This is where monad comes in play, when you want to compose these functions you use the monad. The monad lets you split you computations into smaller computations and glue them together. One example are functions that are not defined for everything, so called /partial functions/. Square root is defined for positive integers for example, what you do? The program can blow up or it can throw an exception, but a function is not pure if it throws an exception. So we can use ~Maybe~. +BEGIN_SRC haskell a -> b a -> Maybe b join :: Maybe (Maybe a) -> Maybe a join (Just (Just a)) = Just a join _ = Nothing -- this doesn't explain the cleverness of maybe so let me -- define bind Nothing >>= f = Nothing Just a >>= f = f a return a = Just a -- Notice we are shortcircuiting, like exceptions, this -- function f serves like continuation. If the first part -- of the computation fails, abandon the computation -- If we want to pass information about the arrow -- then instead of Maybe we use the Either type, either string -- or value +END_SRC Second example is /State/ +BEGIN_SRC haskell -- you have a computation from a to b that accesses some -- external state S, reading of modifying it but it can be turned -- into pure function as long as we pass the state -- explicitly a -> b (a, s) -> (b, s) -- using currying. This is a kleisli arrow! a -> (s -> (b, s)) newtype State s a = State (s -> (a, s)) +END_SRC +BEGIN_SRC haskell -- categorical terms m -> T join -> \\mu return -> \\eta \\eta :: Id -> T \\mu :: m (m a) -> m a T . T --.--> T Id ----------------> + | + C \\eta | C | ----------------> T T T C -----> C ----> C \\ T / \\ ------------> / \\ | / \\ |\\mu / \\ | / \\ / T +END_SRC For better diagrams google /horizontal composition of two natural transformations/. These are now two dimensional diagrams! 10.2 Monoid in the category of endofunctors ** Monad Graham Hutton tutorial Note: At this point Graham Hutton's [[https://youtu.be/t1e8gqXLbsU][Monad tutorial]] seems intuitive so I will follow along and write the code leading up to the monad itself. Monads are a concept invented in mathematics in 1960s, rediscovered in computer science in the 90s and what it gives you is a new way of thinking about programming with effects. This is one of the most important new ideas in programming languages in last 25 years. The example we are going to look at is the idea of writing a function that evaluates simple experessions. What we are going to start with is by defining a simple data type for the kind of expressions that we are going to be evaluating. +BEGIN_SRC haskell -- data type Expr has two new constructors -- Val builds expressions from Integers -- Div builds expressions from two sub-expressions -- 1 Val 1 -- 6 / 2 Div (Val 6) (Val 2) -- 6 / (3 / 1) Div (Val 6) (Div (Val 3) (Val 1)) data Expr = Val Int | Div Expr Expr -- first version of evaluator eval :: Expr -> Int eval (Val n) = n eval (Div x y) = eval x / eval y safediv :: Int -> Int -> Maybe Int safediv n m = if m == 0 then Nothing else Just (n / m) -- new evaluator eval :: Expr -> Maybe Int eval (Val n) = Just n eval (Div x y) = case eval x of Nothing -> Nothing Just n -> case eval y of Nothing -> Nothing Just m -> safediv n m -- too much management of failure, too much noise -- the idea here is we are going to observe a common pattern -- there are two case analysis. -- when you see the same thing multiple times you -- abstract them out and have them as a definition m case __(maybe value)_____ of Nothing -> Nothing f Just x -> __(function to process result)_____ x m >>= f = case m of Nothing -> Nothing Just x -> f x eval :: Expr -> Maybe Int eval (Val n) = return n eval (Div x y) = eval x >>= (\\n -> eval y >>= (\\m -> safediv n m)) eval :: Expr -> Maybe Int eval (Val n) = return n eval (Div x y) = do n <- eval x m <- eval y safediv n m -- the maybe monad return :: a -> Maybe a = :: Maybe a -> (a -> Maybe b) -> Maybe b +END_SRC Same idea works for other effects as well Supports pure programming with effects Use of effects is explicit in types Functions that work for any polymorphism","title":"Categories"},{"location":"categories/#category-theory","text":"","title":"Category Theory"},{"location":"categories/#12-what-is-a-category","text":"Abstraction Composition Identity Homotopy type theory. What is equal? What it identity? Composition and Identity is what defines category theory. We are now ready for first definition! A category is a bunch of objects. What can be bigger than a set? Sets are defined by membership. Elements of sets can also be sets. They have a set hammer and everything is a set nail.. What is a good example of a set that is a member of itself? A bunch of dogs is not a dog. Barbers paradox Who shaves the barber? A Category consists of objects, and arrows. These arrows are also called morphisms. So a morphism is an arrow that goes from a to b a ---(f)---> b . What is an object? It has no properties. What is an arrow, morphism? It is just a name for an arrow. We percieve the universe through these notions described by hunter gatherers, with spatial relationships. Categorists put things in space, higher or lower level abstractions.. Hunter gatherers understand movement too. Realize what language you are using and how it constrains you. So what kind of things can happen? You can have zero or more arrows for each pair of objects.Some objects are not connected, some are, some are connected with infinite number of arrows. So if you have an idea that category is a graph except you have to be open minded what is a graph. You can have arrow going from a to b , from b to a and arrows going from a to a and from b to b .. It's OK, just give them different names. Composition is a very simple property that if you have an arrow going from a to b and an arrow from b to c you have an arrow going from a to c . A --- f ---> B --- g ---> C g . f -------------------> So this is called composition. The category is defined by saying what the objects are and arrow and then defining the composition, like multiplication table for arrows. Different composition tables will give you different categories. Identity for every object, there is this arrow that we call identity, one per object. Why am I calling it identity? Because of composition. If I compose f with identity b I get f again. So: (id a ---> a) a --- f ---> b (id b ---> b) idb after f = f So if you think of it as a graph it has to have some properties, it has to have an identity arrow on each node. So that is left and right identity. The third law is the law of associativity. f ----> g ----> h g after f and then I compose this with h . So I have: h . (g . f) == (h . g) . f That is extremely important to make is manageable for us humans. Is it possible to have no associativity? There are mathematicians that work on making associativity weak.. If objects form a set it's called a small category, if not it's called a large category. In programming, objects are types and arrows are functions. A function is an arrow or a morphism between two types. In Haskell is a little more complicated because of laziness, so the trick is that every type contains the bottom value, because categories don't really take into account time . Time is hard to describe in mathematics but in programming is important. When does the calculation terminates? If the function never ends what is its type? So in Haskell it return an Int type, the bottom type, which means it never terminates. You might ask what are types? Sets of values? There is a simplistic model, they are just sets of values so we can model programming as in a category of sets, sets of values and functions are just functions between sets. And that's a good model too. So a mathematical function is between sets. NOTE: Interesting when drawing arrows between sets the arrows as functions are not morphism. They seem to just map objects but do not morph them. It is like a different view. Seeing morphisms blindly like seeing on which objects they work but without seeing the work, the type of work itself. When I put my categories glasses I see no structures like in sets. We are studying the sets and we find out there are many functions going from set to itself, we find the identity function and this is the identity morphism in my big multiplication table.. I'm abstracting, forgeting what's inside the objects, what the functions do and I end up with a category sets. I have these arrow I forget where they came from.. but I have the multiplication table which fulfills my laws. I can forget where it came from. I don't care about the structure of my functions or objects, I forget what these are.. What can I say about these objects if I just look at the morphism? Oh this set is empty How do you know it? Well, it turnes out the empty set has this property that can be expressed just in terms of morphisms.. it's not easy but it's possible.. You can identify a lot of properties about sets just by looking at the multiplication table without looking at the sets.. If you just look at the sets you are like doing assembly language.. instead you use a higher language of categories looking at the relations.. You look at the interface not the objects. Think data hidding and abstraction . This is the end of road for abstraction. The most abstract language you can think of. We can stop now. (Clap) (.) :: (b -> c) -> (a -> b) -> a -> c f . g = \\x -> f (g x)","title":"1.2 What is a category"},{"location":"categories/#21-functions-epimorphisms","text":"recap: Category of types and category of functions, identity and composability Operational vs denotational semantics, sometimes I use semantics of sets and sometimes semantics of types. A mathematical functions are total functions while in imperative programming we use partial functions which explode for some conditions. How can you tall if a functions is pure? A function is pure if you can memoize it, if you can turn it into a lookup table. Like functions on characters are easy to tabulate, while functions of strings or integers are not easy to tabulate, though that is a problem of resources. What is the lowest, or the highest level of abstraction? The simplest building blocks with which we can build complex stuff? We want to get to the bottom so that we can recompose stuff. So now we have this category of types and functions How can we use functions as category of morphisms on sets? So functions are defined as special kind of relations. So we have two sets, and we look at elements. A relation is a subset of pairs of elements. So.. it's just pairing. This element is in a relation with this element. What is a set of pairs? A Cartesian Product. The set of all pairs forms a cartesian product. Now we take a subset of these pairs and any subset is a relation, by definition. There are no other requirements. So in this sense, a relation has no directionality while functions have these arrows. Functions have some kind of directionality. What kind of condition do we have to impose on a relation to be a function. So many elements from set 1 can be in relation with one element with set 2 but one element cannot be mapped with bunch of things. It's still OK for many to be mapped to one value. And all elements have to be mapped into something in the other set. However not all elements of the second set have to be mapped to the first set. Domain ----- f -----> Codomain The mapped part(subset) is called the image of the function This directionality is very important, this intuition of functions. Ask yourself, is the function invertible? Usually it's not, there isn't always a function that goes other way arount, an inverse function. f :: a -> b The function is invertible if there is a function that goes from b to a g :: b -> a g . f = id f . g = id A function that is invertible, symmetric, is called isomorphism . f (id a) <- a < ========= > b -> (id b) g One reason for a function not to be isomorphic is to collapse. All even numbers map to True , all odd numbers map to False . That's one reason not to be invertible. Other reason is that it's whole image does not fill the whole codomain. So if you had to invert this whole image what to do with elements beyond the image in the second set? So instead inverting you could also say the counterimage, fiber.. you could build a set of fibers so on a set of fibers this thing is invertible.. fibration is interesting in categories too! So there are these two reasons for invertibility. So a function takes place in time. A function that is not invertible is something that increases entropy . You cannot unboil an egg. These two phenomena correspond to very interesting thought process, abstraction. Like I am throwing some information and I am left with one piece of information. Like abstracting numbers to even or odd. Like embedding a shadow on a wall of a cave.. If a function does not collapse things than it's called injection. An injective function does not collapse things. No shrinking, no abstraction, it just injects.. x1 -> f x2 y1 -> f y2 If the function covers the whole codomain, if the image covers the whole codomain it's called surjective. If its surjective and injective it is called isomorphism. For all ys there exists an x that y = f x . So I have defined something in terms of elements How can I talk about category theory now if I cannot look at the elements? I have to express this stuff only in terms in morphisms. It is a very holistic approach. /If my microscopes don't work maybe my telescopes work?/ Note: In category theory we don't like latin we like greek. When something is surjective is called epic , when something is injective is called monic . (when you consider set theory) Epimorphism and Monomorphism Let us say that we have guys that are in terra incognita, outside of the image in set b so I make a set c and map a function g to c . So g after f will actually not probe this terra incognita even though g maps everything, inside composition it will actually only act on this inside a composition. Ok if I have two of these functions g1 and g2 mapping the same point, but if they are outside of f function halo then the composition is the same. So the converse of this is if g1 after f is equal to g2 after f then g1 = g2 , so the function is surjective . Now I have expressed this purely in categorical terms. This is an epimorphism . If I have g1 . f = g2 . f then I can cancel f meaning g1 = g2 .","title":"2.1 Functions, epimorphisms"},{"location":"categories/#22-monomorphisms-simple-types","text":"Recap: f :: a -> b f is an epimorphism from a to b if for every other object c and for every pair of morphisms that goes from b to c , if the composition g1 after f is the same as g2 after f follows then g1 = g2 , then this is an epimorphism. In another words if have a composition g1.f equals to g2.f we can cancel the f on the right. [[https://en.wikipedia.org/wiki/Epimorphism][Epimorphism]] Note: wikipedia In category theory, an epimorphism (also called epic morphism or, an epi ) is a morphism f : X -> Y that is right-cancellative in the sense that, for all objects Z and all morphisms g1, g2: Y -> Z , g1 . f = g2 . f => g1 = g2 Epimorphisms are categorical analogues of surjective functions (and in the category of sets the concept corresponds exactly to the surjective functions), but it may not exactly coincide in all contexts A function f from a set X to a set Y is surjective (also known as onto , or a surjection ), if for every element y in the codomain Y of f , there is at least one element x in the domain X of f such that f(x) = y . It is not required that x be unique; the function f may map one or more elements of X to the same element of Y . A function would not be surjective if the image does not fill the whole codomain. Identity function idx on X is surjective. The function f : Z -> {0,1} defined by f(n) = n mod 2 , (that is, even integers are mapped to 0 and odd integers to 1) is surjective. So let's start with something that's not a monomorphism. A non-injective function will just map two different elements on one set to the sem element of set B , like x1 and x2 are mapped to the same y . --g1---> x1 \\ z ---f---> y --g2---> x2 / If you compose g1 with f and g2 with f you get the same result. They only differ in the way they map z but you get the same result. f after g1 will be equal to f after g2 . This is similar to epimorphism, we use precomposition rather than postcomposition. For every object C and every pair of g1 and g2 , this time they go from c -> a if f after g1 = f after g2 leads to g1 equal g2 always then we say it is a monomorphism . Notice f is not monomorphism by itself. I have to use the whole universe to define this product, a universal property. I hope you get some better feel what functions do. Let's talk about sets a little bit. Sets are models for types. Let's think of simplest possible types/sets. Empty set! Doesn't empty set correspond to a type in programming? You find it in Haskell. An empty set corresponds to a type (forget for a moment for functions that do not terminate) Void . There is no way to construct a Void type. Can we define functions that take Void as argument? f :: Void -> Int ? Mathematically speaking yes. I have a function of type Void, I challenge you :) id void :: Void -> Void This is a good function, you can never call it though, it exists in vacuum when you cannot provide an argument. In Haskell it has a name: absurd :: Void -> Int In logic Void corresponds to false, because you cannot construct falsity from something, you cannot prove something is false, it has no proof. Proof is a function, in this case since you cannot create void there is no proof of falsity, but on the other hand if you assume false, you can derive anything. So it's a polymorphic function: absurd :: Void -> a Next is a singleton set, in Haskell is called a Unit () it has only one element. () :: () corresponds to True . Unit :: a -> () . What about a function that takes a Unit and returns an Int ~() -> Int ? This function must be constant, has to return the same integer. There are many functions like this. one :: () -> Int , boolean... What about a type that has two elements? It's just boolean. Boolean has True and False , whatever you call it. Boolean is not an atomic construction in sets or categories, it can actually be defined as a sum of two units. We can talk about functions from bool to bool. A function that returns a boolean is called a predicate.","title":"2.2 Monomorphisms, simple types"},{"location":"categories/#31-examples-of-categories-orders-monoids","text":"Last time we talked about sets, sets as sets of elements. That's not a very categorical view, I was trying to reformulate some of the properties of sets in terms of morphisms or functions. It makes sense to reformulate them between sets, then we can ask how does this generalize to an arbitrary category.. but we don't know many categories? Let's broaden our horizons! Let's start with simplest possible category! Very few objects. Zero! But a category is defined with objects and arrows, so there are no objects then there are no arrows . Are the conditions then fulfilled. Well the answer is yes, if there aren't any then it's automatically satisfied, so is there an identity arrow? We can say anything about it then since there are no objects. It sort of sounds like a joke. What's the use of this category? It's useless, however the value is in context. Just like zero by itself is useless. What is the context. The context is a category of all categories . In that context, it's an initial object . The next category has one object. There has to be an identity arrow id . That will be a terminal object Next is two objects. Two objects with two identity arrows, two arrows from a to b and b to a and so on... In general we can always start with a graph . But not every graph is a category. It turns out if we start with a graph we can keep adding additional arrows to get a category. The first thing we need are identity arrows, for every node in the graph. Then we come up with composition. For every pair or composable arrows f and g we have a third arrow g(f) , g after f . We need to satisfy associativity . These compositions produce the same arrows, then some can be identified. This kind of construction is called free construction since we are not imposing any constraints other than constraints of category theory. Order categories - in orders arrows are not functions, arrows represent relations. An interesting relation is less than equal =< . =< a ------> b So this arrow doesn't have any meaning other than a is less than of equal to b . It's a relation. Or we can say a comes before b in some order. There are different types of orded. There is preorder , partial order, total order. A preorder satisfies just the minimum of conditions, it has to be composable, so if a is less than b and b is less than c we want a to be less than c . =< =< a ----> b -----> c ------------> =< We recognize this as composition from category theory. Is it associative ? It is! Why? It's because two objects are either in a relation or not. If there are no relation than there is no arrows, it's a binary choice. Now in total arrow you can say between any two objects there is an arrow. But in preorder that is not true. 2nd condition (identity)! So is it true that for every object this object is less than or equal to itself? That's called reflexivity . Here we have only one arrow going from a to b and another arrow from b to a , but we cannot have multiple arrows. A category like this is called a thin category . The set of arrows between any two objects has a name also, it is called a hom-set :: C (a,b)~ or ~C(a,a) A thin category is one in which a every hom-set is either an empty set or a singleton set. That's the definition in terms of hom-sets . We can now impose additional conditions and the next thing we get is partial order . We don't like preorder, we don't like loops. So partial order has no loops because if there is an arrow from a to b then you cannot have an arrow from b to a . If you look at a graph it corresponds to a DAG , directed acyclic graph and further if you say OK, a total order is an order in which there is an arrow between any two objects. And now with this preorder category I can show you epi and mono. Something whats both epi and mono does not have to be invertible. In sets it corresponded to injective and surjective and if it was both it was reversible, called bijection but that is not true in every category. You can have an epi and mono that is not inversible. mono epi z === h1,h2 ===> a --- f ---> b === g1,g2 ===> c Every arrow in preorder is a monomorphism and every arrow in preorder is a epimorphism, but it's not invertible, especially in partial order than it's definetly not invertible because there are no arrow going back. So that't a counter example. You can think of the most general category as being like a preorder that is thick . It gives you a different intuition. Here when you have order you think of it as a relation and this means it is true or false. So it's a black and white world. Now in a thick category you might say, if I have a number of arrows, and each of these arrows sort of represent a proof of this relation. Here is one proof called g , here is another called f .. so you might think of a category like a proof relevant preorder . A thin category defines a relation and a thick category defines a proof relevant definition. That's a different way of looking. It's not only enough to show that something is related to something, Homotopy type theory studies relations in that sense. In one object category we can have many arrows. We can have many more loops. -- Monoid / <===> m -- <===> id \\ <===> So any category with a single object and many arrows is called a Monoid, sort of a pre-group. Monoid is usually defined as a set of elements with some operation defined on them, let us say multiplication. So it's a binary operator. And this binary operator, we can impose certain conditions. We want one of these elements to be the unit ( identity ), sort of like multiplication by 1 , you always get the same result. And the other condition that can be imposed is associativity . ~(a * b) * c = a * (b * c)~ ~e * a = a * e~ String concatenation, that's an interesting monoid, does it have a unit? Yes, an empty string, you append or preappend an empty string don't change anything. It's associative. It is a good example because it's not symmetric. Multiplication and addition is symmetric, you can change the order, with strings you can't, you append two strings, the result will be different if you append them in the opposite order, so this is a very nice example of a monoid. And lists, appending lists forms a monoid. In Haskell, strings are lists of characters and they form a monoid. So thi is one view of a monoid from set theory. Let's call this Monoid M. There is only one hom-set from M(m,m) since there is only one object. This hom-set is a set, right? This category defines a set, and guess what, there is a third element that corresponds to the composition to these two arrows. Well, let's say this is our multiplication. So the third element is the product of two elements. If you pick any two arrows in hom-set, the end of one is the beginning of the next, so there is a third one, f , g , g after f , so g after f is also an element. And then id is here also. All these arrows are members of this set. Example, arrows would correspond to adding a number in some category. So the binary operator in a monoid has to be defined for all elements of the set, has to be a total function of two arguments. So a category of types corresponds to a strongly typed system. You cannot compose any two functions. The result of one function has to have a type that is the same as the argument of the next function. That's strong typing . Not any two functions are composable. The types have to match. A monoid is a very special category in which every two functions are composable, that corresponds to your languages, that have weak typing, any two functions are composable.","title":"3.1 Examples of categories, orders, monoids"},{"location":"categories/#32-kleisl-category","text":"Let's define a relation on sets, this is a relation of inclusion , what it means to be a subset of another set. It is a relation, the question is what kind of relation is this? Is this an order, preorder? What should we check? Identity in terms of order is reflexivity . If a is a subset of b and b is a subset of c we have composition. Is it associative? Yes. So it is definetly a preorder, is it a partial order? We have no loops. If a =< b and b =< a then a = b so it is a partial order, is it a total order? No. Is it possible to have like a diamond relation? They form a dag . a / \\ b /= c \\ / d I want to introduce a category close to us programmers, not based on types and function, we get to it by solving a real programming problem. The problem is this: We have a library of functions. One day the manager says there is a new requirements, that every function has to have a audit trail, every function has to create a little log that says the name or something, has to be appended to a log. Now go and rewrite our library so that every function leaves a trail. The simplest imperative solution would be: have a global log. A simple solution introducing many dependencies. But logs don't compose, deadlocks... pair <bool, string> negate (bool x, string log) { return makePair (!x, log + \"Not!\"); } The subtle problem is this use of plus, why does a function called negate knows about appending strings? This one function is more local but still it has this element knowing stuff it does not belong. So this is a good solution but not quite. par <bool,string> negate (bool x) { return make_pair (!x, \"not!\"); } Who does appending of this logs? Somebody has to concatenate these logs. So the answer is.. what do we do with these functions? We compose them. What if we modify how we compose functions? Let us define a new way of composing functions. Appending strings is in essence composing of functions. [[https://blog.softwaremill.com/kleisli-category-from-theory-to-cats-fbd140bf396e][Kleisli-category-rom-theory-to-cats]] Note: I didn't get this at all :( but will keep watching the lectures :). I also do not understand the examples in scala above. What I do understand that the composition between a and b resultet in an embellished function where a defines a b and a string, so it is not just a to b but a to b which results in pairing the result with another string. So this makes a monad, a way of composing special functions.","title":"3.2 Kleisl category"},{"location":"categories/#41-terminal-and-initial-objects","text":"Recap of Kleisl categories, important to understand Monads. It seems challenging because you have to hold two categories in your head. So you start with one category in which you have objects and arrows. Now based on this category you are trying to build another category, the Kleisli category, and you are building it this way - you're saying the objects in this category are exactly the same as in the starting category , however the arrows in this category are not the same arrows as here so if I have an arrow from a to b is not the same arrow as the one in category C from a to b . Actually I have something that for every object here in C gives me some other object. Now, we talked about a particular case in which for every type, if I had a type a , I assign to it a new type that's a pair of a and String . A pair of a and String is a different type than a , but it's a mapping of types, so for every type a I have this type. Let me call this f a , but maybe not, it's not a function. So let me call it m . Now, m is a mapping that mapps objects to objects or types to types. Later we will learn that this kind of mapping is called a functor. For type b I will have a type m b (the pair of b and String ) so if there is an arrow from a -> m b this will be my arrow from a -> b in my Kleisli category, so this is equal to this. *C* *Kleisli* a ----> ma a <====> id |\\ (a, String) | | \\ ------------------> | | \\ | b m b b |\\ (b,String) | | \\ | | \\ | c m c c (c,String) So it's like im implementing a new category(Kleisli) in terms of this(C) category, I'm implementing the arrow in this category as an arrow in this category, this is an arrow (Kleisli), this is how it's implemented (C). How do I know it's a category? So what's an arrow from b to c (In Kleisli)? It's not an arrow from b to c (In C). It's implemented as an arrow as b to (c, String) , or in general some mc , right? So in this (C) categry they do not compose because the end of this one, (mb) is not the same as the beginning of this one(b). How do I compose these (Kleisli) guys? In principle I don't know. Now I showed you that in this case when is (b, String) and (c, String), let me call this function first a -> mb and I'll get this pair b, String and I will split this pair into b and String and then I will pass this b ( b,String ) here (points to b a -> b ) and I will get this c, String , I will get c and String , right? And then I can combine these things, I can concatenate these two strings, and return a pair (c, s1 ++ s2) so I have now a way of composing these arrows. Now in general for any kind of mapping it's not true, I was just lucky. There was a composition, I could define a clever way of composing these things! If I find the way of cleverly composing the implementation here (C) then I can say this is how I compose these arrows in this Kleisli category, and for this to be a category I have to have an identity. How is identity implemented? It has to go from a to this m a or in another words (a, String) and it has to be a Unit with respect to my new special kind of composition. I have to pick a string thats an empty string so that the concatenation with an empty string will give back the original string. Once I do that than I can say this is a category and if this is a Kleisli category, then this mapping from a to a string or in general from a -> ma is called a Monad! So this is one of many definitions of a monad. This is a very categorical construction. And now for something completely different. So we talked about sets and there is this category set and there is also set theory, and there are these two views, that very useful. One view is sets are things that have elements and we can define things like function, mapping elements to elements, so a lot of these things can be defined in terms of elements. And then we have this category set and in this category we suddenly got amnesia and we are forbidden to talk about elements, only about arrows. We started from arrows, and we know functions between sets so every time we have a function between two sets there will be an arrow in category of sets. And we know how to compose functions. What is an empty set? How do I define an empty set if I don't know anything about elements, a singleton? A cartesian product (set of pairs)? So all this stuff have to be completely rediscovered. Just in terms of arrows and compositions. There is this method of defining things. It's called Universal construction We use this in category theory to pick a particular kind of pattern. Since we cannot go inside of the object we define the properties of the object in terms of the relation to other objects. So we have to define everything in terms of arrows coming and going to this object. We have to think about the whole universe, and we talked about it with epi and mono. So it's like googling. Think of a simple pattern, OK google in this category, show me all hits, so everything that matches this pattern, and usually you have lots of hits. The next thing you do is you have to rank these hits. If you have two hits, see which one is better. The best match defines the object that you are looking for. We will try to define a singleton set. How does this set relate to other set? Think arrows! There is one property of singleton set that's interesting. It has an arrow coming from every other set. There is an arrow from any other set to singleton set. In programming we call it Unit type , an empty tuple () so from any type or any set there is a function to Unit and this function is called Unit, it's a polymorphic function, it just ignores it's argument and creates a unit and returns it a --- Unit ---> () , or Void --- Unit void --> () . Does it really single out singleton object? Is there any other type that have the same property? Well unfortunately yes because set is a category that is extremely rich in arrows! Only, if you have an non empty set and empty set there is no function there! You can only say they all map to the same element, and I'm fucked! So for instance, OK, Bool, the type bool of two element set, is there an arrow from every other set to it? You bet, right? In fact there are two arrows from any other set. One is called True and False . They just ignore the argument and return true or false. The singleton type or unit type there is always a unique arrow from any other object, so this way we can define using pure arrows we can define what we mean by singleton set, without talking about elements. Let's forget about sets! What would we call this object? We will call it terminal object , for all arows, all arrows will converge on this object. Not every category has a terminal object. We can try, we'll say a terminal object in a category is an object that has a unique arrow coming from any other object. Understand, these are two separate conditions. For all objects/type a there exist an f that goes from a -> () . So this is one condition. And for every two functions from a -> () they have to be equal. That's how you define uniqueness. (for all) a (there exist) f :: a -> () (for all) f :: a -> (), g :a -> () => f = g An empty set can be defined by outgoing arrows (singleton set by incoming arrows) Void --- absurd ---> a So I have just reversed the definition I used for terminal object. By the same token I want this arrow to be unique. This object will be called initial object , the opposite from terminal , it has a unique outgoing arrow to every other object. This corresponds to empty set, or in programming to Void . The property of the terminal object, no matter what path you take to the terminal object you can always shrink it to one arrow and it's always the same arrow, this is where uniqueness comes. With boolean for example there would be two ways of shrinking, some path would become true paths some false, see there are more ways of shrinking these paths. When this object is terminal there is only one true path, leading you to the terminal object. Ok, the next question we might ask, how many of these objects are there? How many empty sets are there? Just one, seems natural to think that, what about terminal object, how many singleton sets are there? Tougher question.. is it the same, the set that contains one apple is it the same as the set containing one orange? I don't know.. from perspective category theory, what does it mean for two objects to be equal? I don't know, there is no equality of objects. There is an equality of arrows, if they have the same ends and beginnings, right? So we can compare arrows for equality but cannot compare objects for equality, instead we can ask if they are isomorphic . Isomorphism is this fact that you have two arrows, one being inverse of the other. Terminal object is unique up to an isomorphism. And even stronger condition is that there is an unique isomorphism between them. Like if you have two element sets, (true and false) and (black and white), true is black, false is white, these are two morphisms, both invertible. Suppose we have two terminal objects a and b , so there will be an arrow from b to a , and it is a unique arrow because a is terminal object, but b is a terminal object so there is unique arrow coming from a to b . What's the composition? It's a loopy thing. <------g---- (id a) a b (id b) ------f-----> -- Unique isomorphism: g . f = id a f . g = id b How does the pattern and ranking relate to this? So my pattern is an object, a simple pattern, now show me all example of this pattern in your category, what will you show me? You will show me all your objects, because I didn't specify anything about it, that's a very imprecise query, it gives you huge recall, but we have the ranking. So if I have two matches I will say a is better than b if there is a unique arrow from b to a. OK, maybe there is no unique arrows.. ok fine well then you don't compare these objects. I didn't say its a total order, its a partial order. What is like the best fit? One that is better than all else, so terminal object is better than any other object. The difference between initial and terminal object is just in the ranking.","title":"4.1 Terminal and Initial Objects"},{"location":"categories/#42-products","text":"There was a question about terminal objects. There is nothing I said about outgoing arrows from the terminal object. I talked about incoming arrows to the terminal object, they have to be unique from every object. It doesn't mean there are no outgoing arrows and in fact there are usually outgoing arrows from the terminal object and these are the arrows that helps us define generalized elements in other objects, every arrow from the terminal object to another object is a definition of a generalized element in this other object. This is what happens in set, when you map a singleton set into some other set, thats equivalent of picking one element and say, this element of singleton set is mapped to this particular element of the other set, so its picking another one, there are many morphisms, each of them picks a different element. Now let's talk... We have now two examples of universal construction, the terminal object and initial object. I talked about reversing the arrows. It turns out this has a much deeper meaning. Every construction in category theory has its opposite construction that is done by reversing arrows. If you define the terminal object you get for free the definition of the initial object. You can allways create a new category which is identital to another category but with arrows reversed. C (a -- f --> b) C op (b -- fop --> a) f . g (g . f) op (g . f) op = f op . g op Cartesian product (the set of pairs), for instance a plane is a cartesian product of two axis, and cartesian product corresponds to a point. For every cartesian project there are these functions, called projections. In Haskell we call then fst and snd . Of a x b there are these two arrows called first and second. First maps to a second maps to b . That's a pattern. I'll call a x b a C , maybe I'll call these two arrows p(fst) and q(snd . There could be many such things, it could be anything. Now one of that is my cartesian product, but which one? Universal construction to the rescue! I have to be able to rank them, that some cartesian product is better than another one. So let us say we have c and c' . We say c is better than c' if there is a morphism, let's call it m . C' / | \\ p'/ | \\ q' / m \\ / p | q \\ a <---- C ----> b p . m = p' q . m = q' Later: a = Int b = Bool p = fst q = snd Is this enough to pick? No it's still not enough. So to summarize, c is better than c' if there is a unique morphism m from c' to c such that, this is true (p.m=p', q.m=q') . How do we read this? If this were multiplication then you would say p' factorizes into p times m and q' factorizes into q times m . So they have a common factor m . So I can like extract a common factor. So this morphism is special, it factorizes these two projections. It takes the worst out of these two projections, condenses them. Why the worst? If you look at different candidates, like the Goldilocks principle , some candidates are too small some too big, they don't fit. Morphism can loose information, it can squeeze, may not cover, so like all this non injectivity is concentrated in this m . This is a bad guy, it does all this non injective non surjective stuff, they are concentrated in there. Like p is this nice clean projection but if you add this uglines you get this p' projection. So the real product of a and b is a pair (a,b) . That's the type. fst (a,_) = a snd (_,b) = b a = Int b = Bool (Int, Bool) Int candidate p :: Int -> Int p = identity q :: Int -> Bool q = True I have to show there is an mapping m . m :: Int -> (Int, Bool) m x = (x, True) -- non-injective badness Correction: I mean non-surjective, It misses pairs of the form (x, False) Let's try a different candidate. We want a richer candidate. Let's have a triple. (Int, Int, Bool) Now I can define a projection. p' :: (Int, Int, Bool) -> Int p' (x,_,_) = x q' :: (Int, Int, Bool) -> Bool q' (_,_,b) = b But this guy (p') is too big, like 3D cube and my product is just a square, so I'm shrinking stuff. What is the m in this case. m would have to be a mapping from the bad candidate which is (Int,Int,Bool) -> (Int,Bool) . m (x,y,b) = (x,b) so it's non-injective . How do we define a product? So a categorical product of two objects a and b is a third object c with two projections p and q . C p : c -> a q : c -> b For any other C' that has some p' from c' to a , and q' from c' to b , for any other pretender there is a unique morphism m that goes from C' to C which factorizes the two projections. p' = p . m and q'= q . m . You remember the picture, the commuting diagram. Two paths through a diagram give you a same result. C is called the product of a and b","title":"4.2 Products"},{"location":"categories/#51-coproducts-sum-types","text":"Today I show you the dual of the product, the same thing but in the opposite category, I will take the product and reverse the arrows and show you whats produced and the thing constructed is called coproduct. Co- is usually called when you reverse something, Monad, Comonad.. So a product is this object with two morphism p and q into /a/ and /b/. So it's a product of a and b and a product C , but there are lots of things that have two projections, so product is the best, the ultimate! But what about this C' prime, it also has projections p' and q' why this is not a product? There is a unique mapping ~m~ that makes these two triangles commute. This C is the best candidate if for any other candidate we can do this direct mapping, we can reduce it to this one C and these two projections (p,q) which in sets were just (a,b) so now if we /reverse/ the arrows, we will try to draw this upside down, we want the diagram flow from top to bottom.","title":"5.1 Coproducts, sum types"},{"location":"categories/#begin_src","text":"a b \\ / \\ / \\ i \\ / j / \\ \\\\ // / i' C j' \\ | / | | C'","title":"+BEGIN_SRC"},{"location":"categories/#end_src","text":"So a coproduct will match this pattern, It's an object with two arrows coming to it from a and b, and these two arrows are called injections. We just reverse the arrows. So instead of mapping the fake candidate into the real thing using a unique morphism we are mapping the real thing using a unique morphism into the fake candidate.","title":"+END_SRC"},{"location":"categories/#begin_src_1","text":"i' = m . i j' = m . j","title":"+BEGIN_SRC"},{"location":"categories/#end_src_1","text":"In programming cartesian product gives us a tuple, a pair of two things. ~(a,b)~ and this is well known thing. However a coproduct? What is it in set theory? So the fact that we have this injection means we are embedding the set a into C and be are embeddin a set b into set C, I'm thinking function, because we want to figure out what this gives us. This is like the best fit for this pattern so we want this to inject the whole set without collapsing, so what happens is that like the best fit would be a set which contains both a and b so it's like s /union/ of these two sets. Injected /faithfully/, means that the whole set a is mapped into this and the whole set b is mapped into this and there is no bloating, no unmapped things, just a and just b and nothing else and this defines absolute best possible fit and this is true of every universal construction, picking the ideal thing. Two objects, two injections, a coproduct. Here we can say that a product is something that every other candidate can be shrunk into this product that we can recognize, map this candidate into the product while here, in coproduct we can recognize a and b inside of C and this unique morphism tells us which parts of C' belong where, what we can find in C'. This does not completely define this thing. In set theory you have this union of two sets. What happens when they overlap? What's a union of set with itself? Just the same set, but you can tag these elements, it's actually duplicated, because it has a tag. I came from the left one, I came from the right one, so this is called a /discriminated union/. It turnse out that it is a discriminated union . There is a mapping from discriminated union to union. a left and a right. So we have twice as many elements, which can be mapped to a single a, a /non-injective/ mapping and it's a unique mapping of this discriminated union into a regular union. The other way around we couldn't. There is no way of mapping of mapping an a from union to a discriminated union because a function cannot split. What is it then in terms of types? In terms of types discriminated union is called a /tagged union/ or a /variant/. It means you have a /data type/ like which if you take a union of Integer and Boolean, this is something that either contains an Int or a Bool, not both of them, a pair is something that contains both, you need both to construct it, in order to construct a /discriminated union/ you either give me a integer and I give you this union, and I tag it, I'm an integer if you look inside me you find an integer, or if you give me a boolen we will have a tag that says it's a bool. The simplest example of this is an /enumeration/. It is an union of things, it can be either this or that. A sum type is not built in and the canonical example is called Either","title":"+END_SRC"},{"location":"categories/#begin_src-haskell","text":"data Either a b = Left a | Right b","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_2","text":"You read it either left a or right b. It means you can construct an element of this type, either by calling this constructor and it contains an a or by calling this constructor and it contains b, so these two constructors correspond to this ~i~ and ~j~, one of them injects a and other one injects b whereas in a pair I had two, lets say /distructors/, they destroyed the pair by picking it a part (fst, snd) se here I have two /projections/ here I have two /injections/. Because this is a dual picture, how do we extract stuff? Somebody gave me something ~x :: Either Int Bool~ I cannot just say give me an integer from this x, I can't do that because maybe it is a boolean, I do not know how is constructed. I have to take into account both possibilities, to write code that will work in either case, and that's called /pattern matching/, code that will match left pattern and right pattern.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_1","text":"f :: Either Int Bool -> Bool f (Left i) = i > 0 f (Right j) = b -- they will only match when f is called with left element -- and will make this i equal to the integer sitting -- inside","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_3","text":"Now we pretty much have the foundation of the type system. In every programming language product types are all over the place.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_2","text":"data Point = P Float Float -- record syntax = { x :: Float , y :: Float }","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_4","text":"Most of programming is done with products. Standard union in C++ is not tagged. Why is is called a product and a sum? Well sort of like a union, maybe.. so this is sort of like multiplication, like a /plus/ (take a break) and we come back to /algebraic data types/. 5.2 Algebraic Data Types So we have products and we have sums, just like in algebra. Product, sort of like multiplication, what does it mean, it means we have a monoid, at least, right? So a monoid would be something that has multiplication, associative and that has a unit. But now we are talking about types. Is there something like an alebgra of types? Is the product in algebra of types actually behaving like multiplication? Let's check a few things in haskall. The product of numbers, it's not true of every monoid, but a product of numbers is symmetric, let's see if a product of two types is symetric. ~(a, b)~ is it the same as ~(b, a)~? No it's not. If you have a function that takes a pair of Int and Bool it will not accept a pair of Bool and Int. So these two types are not the same, however they contain exactly the same information, they encode it slightly differently, which means that actually they are /isomorphic/. And this isomorphism is called /swap/.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_3","text":"swap :: (a, b) -> (b, a) swap p = (snd p, fst p)","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_5","text":"It is symmetric up to isomorphism. The monoid product is associative. What does that mean? ~((a, b) c)~ is it the same as ~(a, (b, c))~. This won't typecheck. But again they contain the same information.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_4","text":"assoc ((a, b) c) = (a (b, c))","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_6","text":"swap is isomorphic because if you swap two times you get the same thing. Does it have a unit of multiplication? What would be the type if you pair it with any other type, you will just get back the same type? Well it has to be a type that has only one element. So the type that has one element is called a unit. ~(a, ()) /= a~","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_5","text":"munit (x, ()) = x munit_inv x = (x, ()) munit = fst munit_inv x = (x, ())","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_7","text":"This follows from sum being associative up to isomorphism.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_6","text":"Either a b ~ Either b a data Triple a b c = Left a | Right c | Middle b","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_8","text":"What's the unit of sum? /Void/.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_7","text":"Either a Void ~ a a + 0 = a","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_9","text":"So we have two monoids but that's not all! We would like to combine these two monoids into a bigger thing. What would be that? From algebra we know that we can multiply to 0, we have this ~a * 0 = 0~ so a pair of (a, Void) ~ Void So I can never construct a pair of a and Void which is the same as Void. So a times zero is zero. There is /distributive law/. ~a * (b + c) = a * b + a * c~","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_8","text":"(a, Either b c) ~ Either (a, b) (a, c)","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_10","text":"What is this structure called when you have multiplication and addition in the same thing. It is called /a ring/. Except a ring has an inverse of addition. And here we don't have inverses. We don't know how to /subtract/ something. What's the inverse of integer of a type, it's nothing.. A ring that has no inverse is called a /Rig/ or /Semiring/ What is the correspondence of 2 = 1 + 1? So 1 is a unit type, we can call left unit true and right ne false, so 2 is a bool. What else? 1 + a is our friend ~Maybe~ ~data Maybe a = Nothing | Just a~ (So Nothing is equivalent to (left) unit ~()~, and right Just a is ~a~. But there is more one interesting trick :) Let's solve equations! So the equation i want to solve is this","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_9","text":"l(a) = 1 + a * l(a) l(a) - a * l(a) = 1 l(a)(1 - a) = 1 l(a) = 1 / 1 - a","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_11","text":"","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_10","text":"data List a = Nil | Cons a (List a)","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_12","text":"Now, unfortunately I cannot do division and subtraction. Does anybody recognize this? It is a sum of geometric sequence. For n = 0 to infinity a to the power of n.","title":"+END_SRC"},{"location":"categories/#begin_src_2","text":"~ = E a^n = 1 + a + (a * a) + (a * a * a) + ... n=0 ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ [] [a] [a,a] [a,a,a]","title":"+BEGIN_SRC"},{"location":"categories/#end_src_13","text":"What is not a trick or is a little trick that can be explained later is that this equation can be solved by substitution. This expantion ends in a /fixed point/. This can be formalized in a fixed point combinator. So this is how you get a list.","title":"+END_SRC"},{"location":"categories/#begin_src_3","text":"L(a) = 1 + a * (1 + a * L(a)) = 1 + a + a * a (1 + a * L(a))","title":"+BEGIN_SRC"},{"location":"categories/#end_src_14","text":"This is why these are called /algebraic data types/. We will get to the point when we do exponentials. Then our algebra will be really really interesting! 6.1 Functors So today I want to talk about functors. So all previous lectures were just introduction to functors. Mathematicians will say /natural transformations/ are important and you need functors for it. Why are they important? There are things in category theory that are formalized that we thought were not. What is really universtal construction about? It about being able to define what is means to be a perfect embodiment of a idea, to be an ideal. It's like we say how do we define a product. Well we have all these possibilities, how do we pick one? Well we just say let's pick the best one. And we have these two types of universal construction, one for product one for coproduct and they sort of define slightly different ways what is more ideal than other thing. And the property of this perfect thing there is a morphism coming from any other candidate (product) which means anything else you throw at me can be /distilled/, any two objects with projections and I can distill it into this perfect thing called a product, finding a unique morphism. With a coproduct it means here is an ideal coproduct and any other candidate has an image in it because there is a morphism from this perfect ideal thing down to any other candidate that's not perfect, that kind of embedds it, finds this element of perfection. So functors now. Mathematically speaking it's a simple idea, Functor is just a mapping from one category to another. When we talked about products and coproduct I used a loose language, looking for a pattern. So we have this pattern which constists of single object (terminal, initial) or an object and two morphisms (product, coproduct) and now we are trying to find this pattern, match this pattern in our big category. What does it mean? So pattern recognition, category theory tells us how to formalize /pattern recognition/. Well if you want to recognize a pattern inside a category, you have to define what you mean by a pattern, a pattern must be some kind of structure and you sort of have to map this pattern into this category but you have to map it in such a way that you recognize this as a pattern, meaning you have to preserve the structure. What is structure? Well category is a definition of structure, its pure structure, dots and arrows. So if you want to say I want to recognize a certain structure it means you want to define your pattern as a category. So if you want to map one category to another category you first map the objects (small category example) and since objects form sets in a small category its just mapping of sets, and thats a function. But there is something funny about functions that i have not mentioned, that functions are sort of primitive, trivial. What we are really interested in are mappings that preserve structure and it so happens that function are mappings between sets and sets have no structure They have just a bunch of objects. It's really hard to implement something that totally disorganized on top of hardware thats organized so people implement trees. But in order to implement a tree you need to be able to compare elements.. what is important is to find mappings that preserve structure . We are not really used to thinking about what does it mean to preserve structure. /A discrete category corresponds to a single set/ Any category that is no discrete by definition has structure. So if we want to preserve structure our mapping has to have arrows. So first we preserve mapping between objects.","title":"+END_SRC"},{"location":"categories/#begin_src_4","text":"C D a -------------- F ------------------> Fa |\\ | \\ | \\ | \\ f g.f | Ff \\ | \\ | \\ F(g.f) | | \\ b------------------------------------> Fb \\ \\ \\ \\ Fg \\Fb \\ \\ c Fc C(a,b) ----------------------------> D(Fa, Fb) F (g.f) = Fg . Ff","title":"+BEGIN_SRC"},{"location":"categories/#end_src_15","text":"/Hom-set/ C(a, b) /Hom-set/ D(Fa, Fb) A functor is /huge/ potentially number of separate functions. One function per every hom-set. And I haven't even talked about preserving structure, which is defined by composition. I have to map composition I want to preserve structure. A /functor/ is this kind of mapping of objects and morphisms that preserves composition and identity. This is sort of obvious way to define a functor. And this also formalizes this idea what does it mean to preserve structure. So functor is something that preserves structure. What else can be say about this functor. Whenever objects are connected in source category they will be connected in target category. Doesn't mean every morphism will have a corresponding morphism, it doesnt have to be surjective of injective, but we can never destroy connections. If you done calculus it is sort of like /continuous transformation/. Essentially thinking about this continuity. Otherwise you can shrink things, you can collapse things, and in particular you can define functors that don't break things. If the mapping of /hom-sets/ is /injective/ than we call such mapping /faithful/. So a /faithful functor/ is injective on all hom-sets So a functor is /full/ when /surjective/ on hom-sets. So it can collapse objects and still be /injective/ on morphisms. Or it can map the whole category into a tiny category and still be surjective on hom-sets. The most beautiful is the /fully faithful functor/, the one that preserves, an isomorphism on hom-sets. Surjectivly, injectivly. What's a functor whose source is just a one object category (will have to have an ~id~)? So if we map it inside another category, well it has to be mapped into an identity as well, functor must map identity to identity. So that's equivalent to just picking just an object in this category. Just like we had with functions, from terminal object to another object. The other way around is a bit interesting. So, a functor that just mapps every object in this category into one single category, so it collapses every single, the whole category into a single object, a black hole, and all morphism collapse into one identity morphism. This is an important functor. This is called /constant functor/. So this one is called ~delta C~ from c to c. Very important. OK, now what does all this have to do with programming? Most common functors in programming will just deal with single category because that is the category of types and functions. But I never said that C and D have to be different categories. In principle it can be the same category. Objects in this category can be mapped into the same objects in that category. That's called an /endo-functor/. (going inside) In Haskell these endofunctors are just called functors. So what is a functor translating this (image above)? A functor has to be a mapping of types, a total mapping, every object has to have an image, which means it's a /type constructor/ but that's just one part of a functor. The type constructor is a mapping on types, function that works on objects, but it also has to map morphisms which means it has to map functions, so let's just grab some examples and see how we can define, starting from a type constructor.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_11","text":"data Maybe a","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_16","text":"So a is a parameter, so for every type a we are defining a new type a. So we are mapping types to types.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_12","text":"a ----- Maybe -----> Maybe a data Maybe a = Nothing | Just a","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_17","text":"Is this a functor? We need to define functions between hom-sets. If we have a and some b, b will go to some Maybe b, we have to define a function that goes from Maybe a to Maybe b, so if this is f this is the mapping of f using the functor. This mapping of functions is called fmap of f","title":"+END_SRC"},{"location":"categories/#begin_src_5","text":"Maybe a -----------------> Maybe a | | | | f | fmap f | | | | b -----------------> Maybe b","title":"+BEGIN_SRC"},{"location":"categories/#end_src_18","text":"","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_13","text":"fmap :: (a -> b) -> (Maybe a -> Maybe b) fmap f Nothing = Nothing fmap f (Just x) = Just (f x)","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_19","text":"fmap for Maybe must be of this signature. What can we put instead of ~fmap f Nothing = Nothing~? .. mental block.. We could say.. It's nothing unless the type a is Integer, Just zero? Why not? There is somthing called /ad-hoc/ polymorphism but we don't want to use it here. We are kind of straying from mathematics when programming with Haskell. We are actually imposing stronger condition, parametric polymorphism, we are making it restrictive, leading to /theorems for free/. This is something that says because in Haskell can actually only implement functions of some limited kind, a certain type of polymorphism, that imposes the conditions of what we can do. * 6.2 Functor II So we have defined a functor. Maybe we have maybe we haven't :) How do we know this preserves composition and identity. Well we cannot express this in Haskell, in type system we cannot encode these conditions. Unlike in other languages there is a way of using haskell on a whiteboard to prove things about the language. We would like to prove this functors preserves identity. ~fmap id = id~ Now this id works on a different object than this id. ~fmap id a = id Maybe a~ and we want it to preserve composition: ~fmap (g . f) = fmap g . fmap f~ What does it mean functions are equal? They have equal values on equal arguments. Whats so special about Haskell is that every definition in Haskell is an equality. And it means what is says. These two things are equal. It is an equation. Left side is same as the right side. In programming this is sometimes called /inlining/, but if you have pure functions you can do /inlining/ and you can do the other way around, /refactoring/, turning an expression into a function call. When ~fmap~ acts on ~id~ it produces a function from ~Maybe a~ to ~Maybe a~, right? So I have two cases to check, this ~Maybe~ could be a ~Nothing~ or it could be a ~Just~ ~fmap id Nothing = Nothing = id Nothing~ /see what did here? I did refactoring, replacing Nothing with id Nothing, so this checks/ ~fmap id (Just x) = Just (id x) = Just x~ ~id (Just x) = Just x~ Let's talk how to define a functor in general in Haskell. /Lifting/","title":"+END_SRC"},{"location":"categories/#begin_src_6","text":"Maybe a--------> Maybe b ^ fmap f ^ | | | | | | a ------------> b","title":"+BEGIN_SRC"},{"location":"categories/#end_src_20","text":"fmap is a higher order polymorphic function. It's not like you write one formula for fmap for all functors. So now you are seeing a different kind of polymorphism, in which depending on what your parameter is, in this case the functor, you get a different implementation of a function, fmap in this case. So this is an example of /ad hoc/ polymorphism. It's just we use a slightly different tool for /ad hoc/ polymorphism which is called a /typeclass/. A Typeclass is, you define a whole family, or a class of types that share some common interface. So in Haskell this is called a class.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_14","text":"class Eq a where (==) :: a -> a -> Bool","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_21","text":"So every type that supports this operator, that takes two a's and produces a boolean, but ~(==)~ is one name that will serve us for many different types, and its implementation will be different for every type, you implement equality different for integers, different for strings. So that's /ad-hoc/ polymorphism. Functors are actually /type constructors/. So Maybe is a functor, because it takes a type and produces a type. So if we want to define a functor we have to define it as a class.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_15","text":"class Functor f where fmap :: (a -> b) -> (f a -> fb)","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_22","text":"/So f here is actually a type constructor! a is a type, b is a type, then f must be something that acts on a type to produce another type./ The most intuitive example of a functor is list.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_16","text":"data List a = Nil | Cons a (List a) instance Functor List where fmap _ Nil = Nil -- h type of a, t type of list of a fmap f (Cons h t) = Cons (f h) (fmap f t)","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_23","text":"fmap = map /map is just a particular implementation of fmap for lists/ But lists came earlier before functors, so they already had this map defined, but it's really fmap.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_17","text":"type Reader r a = r -> a","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_24","text":"The arrow itself ~(->)~ arrow takes two types ~r~ and ~a~ and produces a type of function from ~r~ to ~a~. Now, so far we've been talking about these type constructors that just take one type as an argument, and here we something that takes two types. But we can always just fix one type and say, we only care about the second type. We fix the arrow and we say let's just /vary/ ~a~.","title":"+END_SRC"},{"location":"categories/#begin_src_7","text":"Reader r a -----------> (r -> a)","title":"+BEGIN_SRC"},{"location":"categories/#end_src_25","text":"First one is fixed to bool for example and second one varies. This is called partial application, currying.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_18","text":"g f f g (r --> a --> b) ^ ^ ^ ^ fmap :: (a -> b) -> (r -> a) -> (r -> b) -- functor acting on a functor acting on b fmap f g = f . g = (.) f g -- I cross f g on both sides fmap = f . g = (.) fmap = (.)","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_26","text":"What is the general intuition behind this? I still haven't showed you identity or const functor. There is one intuition that works for endofunctors, some say it's bad some say it's good. The intuition is that a functor when it's acting on some type encapsulates, hides, the values of this type, so an element of the type Functor of a has elements of a in it, and something that has something inside is usually called a /container/. List is a container, it contains a's, a list of integers, a /tree/ which is a functor too is a container of objects, a vector is a container of elements and it's a Functor. But then there are these Functors that are problematic, like Maybe, it may contain an a or maybe not. A container can be empty? It kinda works and this idea that something is a container, what does it mean to apply a function to the contents of the container, just open this container, look at this stuff and apply the function. So this is what we did with Maybe, if it contains Nothing do nothing well if it contains an a just do this. But then we have this Reader guy, how is this a container? Look at a function that a Boolean and returns some other type? How many possible values does this function have. Two, true and false, so can I say it is a container of two values? I can memoize this function, replace it with a table lookup, which contains these two values. What about a function of integer, it is just an infinite sequence of ints, maybe I cannot memoize the whole thing, maybe I can partially and so on. So this distinction between a function and a data type, if you think about it is, /weak/. A list is a container, ok in Haskell I have a list from one to infinity ~[1..]~, obviously I cannot store it in memory, how is this implemented? As a function. All data types in Haskell, are /thunks/, they are function that can be evaluated to return a value, data are really functions, functions are really data. And we will talk about what function types are in category theory and you will see that it is actually an exponential, which is a data type. The only thing about Functor that's important is that you can apply a function to what it contains, there is no way at least the Functor does not provide you a way to retrieve this value, that's not a part of definition of a Functor. I want to leave you with this idea, that functors, /endofunctors/ are containers. 7.1 Functoriality, bifunctors Remember, a functor is like lots of functions put together, there is one major function that mapps objects and that's an actual function only if the category is /small/, objects form a set and functor is just a function on objects, but it also has to /preserve structure/ and that is the most important part, we learned what it means to preserve structure, so functor maps not the only objects but also mapps connections between objects which are morphisms. So for every connection between objects we have this set of arrows between them, which we call a /hom-set/, and as these two ends of a hom-set are mapped from one category to another we define also a mapping of these morphisms between hom-sets, and since hom-sets are sets in a /locally small category/, that's also a function so for every hom-set there is a function that maps it to the corresponding hom-set in the second category. That means preserving connection between objects. Since functors are built from functions, we know that functions compose. Category in which functors are morphisms and categories are objects is called /Cat/. Let's combine two endofunctors we know about, ~Maybe~ and ~List~. There is this function called ~tail~ and tail takes a list of some a's and returns a list of a's. ~tail :: [a] -> [a]~ It's defined so that it just throws away the head of the list, so it's well defined only for lists that are not empty. What if the list is empty? The program dies. The only reason people use it is because it's more optimal, because otherwise you always have to check is it empty? Otherwise it is the /achilles heal/ of ~Prelude~. But if we want to be sure, then let's define something called safe tail, that takes into account the possibility that list can be empty.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_19","text":"safeTail :: [a] -> Maybe [a] safeTail [] = Nothing safeTail (x:xs) = Just xs mis :: Maybe [Int] sq :: Int -> Int fmap (fmap sq) mis (fmap . fmap) sq mis","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_27","text":"Most type constructors that you woul normally used are sort of automatically functorial, what it means is that you have a type constructor and defining fmap for it kinda falls out automatically, it's like most data structures are regular in this sense, what it means, that algebraic data structures are automatically functorial. How do you define them? You form them using things like products or sums and you apply it to a unit or a type variable. If you create an algebraic data type using these operations then you automatically have a functor, so we have to start by asking ourselves is a product a functor? So a product of two types ~(a,b)~, this can be rewriten as a type constructor acting on a,b ~(,) a b~ so we could ask is this a functor in b? If we fix a. It kinda is. It's a type constructor. We can construct a type by pairing it with some fixed type. If a function goes from ~a -> b~ we can lift it ~(e,a)~ and ~(e,b)~ with ~fmap f~ where this fmap takes ~fmap f (e,x) = (e, fx)~. We can define something that is a product of two categories then a functor from a product of two categories would be equivalent to a functor of two arguments, one from one one from another category. We know in category theory products are these beasts and now I'm saying I want to do something bigger, a product of two categories! It turns out that a product of two categories is easier to define than a product from two objects in a category. OK, let me take two categories C and D and we take objects from C and D and we form a pair ~(c,d)~ that we call ~CxD~ (/c cross d/) in which objects are pairs of objects so really the objects are cartesian product of the sets of objects. What else do I need? Morphisms. And again, this is easy to do this pairing, I can do cartesian product of this hom-set and this hom-set. So I have a new category, called a /product category/.","title":"+END_SRC"},{"location":"categories/#begin_src_8","text":"(f', g') . (f, g) = (f'. f, g'. g) (id a, id b) = id (a,b)","title":"+BEGIN_SRC"},{"location":"categories/#end_src_28","text":"Now when I have a product category is just a category, now I can define a functor that goes from this category. ~C x D -> E~ It means for every pair of objects I pick and object in E. And on morphisms, a morphism in C cross D is a pair of morphisms paired with a morphism in E. This functor is called a /bifunctor/. A bifunctor is a functor from a product category. In haskell we would have to lift a morphism from a product category. This would be a product with a same category with itself. ~C x C -> C~ but notice that we are actually talking about a functor that's not set from set, or ~hask~ to ~hask~, it's a functor from some other category to hask, a product of two hasks, so we are already getting outside of hask and getting into /hask \"squared\"/ What does it mean in terms of function we know? We have a mapping from two types into a type. That looks sort of like this ~(,) a b~. What about morphism? It is a pair of functions, we are lifting two functions /at the same time/. So if we want to define a bifunctor in haskell and just like functor it will be defined as a class. It will have to have this way of lifting two functions at the same time. This higher order function corresponding to fmap will be called ~bimap~","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_20","text":"class Bifunctor f where bimap :: (a -> a') -> (b -> b') -> (f a b -> f a' b')","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_29","text":"OK, product is /bifunctorial/. What about /sum/? mmm... Do we have to go through this construction and come up with a sum of two categories? What happens is that, we can use the same bifunctor idea for a sum so ~Either a b~ - thats the sum type, the canonical sum type of two types. Either a b is actually a bifunctor! So we can define the action of two actions of either a b. So Either a b takes a pair of types from a product category! It is a function of two arguments, takes two types and produces a third type of ~Either a b~. What we want to have in general is that if we have a product in a category, then this product /is/ actually a bifunctor and its a bifunctor of this type ~C x C -> C~. Here we have two examples in /hask/ one is the product in hask one is in hask they both are bifunctors like this. So a coproduct is also a bifunctor.","title":"+END_SRC"},{"location":"categories/#begin_src_9","text":"a x b / . \\ ------+ p / / . \\ \\q == ------> / / . \\ \\ ......+ / / . \\ \\ + f.p / . \\g.q + ......+ = f x g a / + \\ b | / a'x b' \\ | | / / \\ \\ | f| / / \\ \\ |g | /p' q' \\ | +/ \\ + a' b' (a, b) -> a x b (C x C) -> C","title":"+BEGIN_SRC"},{"location":"categories/#end_src_30","text":"7.2 Monoidal Categories, Functoriality of ADTs In a monoidal category we would like to define what does it mean to multiply two objects, ha? (waves hands) :) So a product, categorical product is sort of way like multiplying object, we already have one part of a monoid, we have this binary operation, on object, right? What was the unit for product in Haskell. It was the unit type :D which is a singleton set (in set theory). How do we define a singleton set? Terminal object. Is this terminal object maybe good candidate for a unit in our newly formed monoid structure? It would mean if you construct a product in which you have some object ~a~ and you have this terminal object ~()~, so ~a x ()~ with ~a~ and ~()~ projections. So I want to prove that a is actualy product of a and terminal object and if you multiply a by terminal object you get back a. What are the projections here? ~id a~ and this is ~unit~, it's a good candidate, is it the best candidate? Let's try some other, a ~b~, a /candidate/ is really a triple, its an object plus two projections. It has to have a projection that goes to a, lets call it ~p~, and and one that goes to unit, call it ~unit b~ so in order to prove this is the best guy I have to show there is a unique morphism from ~b -> a~","title":"+END_SRC"},{"location":"categories/#begin_src_10","text":"b / | \\ / | \\ p / |p \\ unit b / + \\ / ida / \\ \\ / / \\unit a \\ + / \\ + a + + ()","title":"+BEGIN_SRC"},{"location":"categories/#end_src_31","text":"A product is defined up to unique isomorphism. So a categorical product, then you have this new structure, monoidal structure on objects, I can do the same thing with a coproduct, the unit would be the initial object, so that would also be a monoidal category with coproduct and initial object, in general maybe there are other things like this, but what we really need is a product which is a /bifunctor/, we need this binary operation on object, and we need this unit for the bifunctor and we get the monoidal category. What is a good name that could be a coproduct or a bifunctor, a good name is a /tensor product/. (writen as circle with a cross inside). I started all this discussion because I said ADTs are functorial. So product and coproduct are functorial. What else we use to construct data types? We can construct a datatype that does not depend on a data type. We have this way of constructing a trivial functor from a constant object, called a /Const Functor/, that takes two data types. Constant functor maps every object in one category into a single object in the second category, like a black hole, called ~delta c~ so in Haskell:","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_21","text":"data Const c a = Const c instance Functor (Const c) where -- fmap :: (a -> b) -> Const c a -> Const c b fmap f (Const c) = Const c data Identity a = Identity a fmap f (Identity a) = Identity (f a) data Maybe a = Nothing | Just a -- Either () (Identity a) -- ^ -- ^ Const () a","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_32","text":"There is an extesion in Haskell, ~{-# LANGUAGE DeriveFunctor #-}~, then you can just say ~data Maybe = .... deriving Functor~ and the compiler will derive you the correct ~fmap~. There is one more type constructor that takes two arguments? Right! Function, the arrow. ~(->) a b = a -> b~ So arrow is a type constructor, it takes two types a and b and produces a third type which is a type of functions from a to b. Now strictly speaking I have not yet talked about function types, next lecture..","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_22","text":"newtype Reader c a = Reader (c -> a)","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_33","text":"Is this a bifunctor? Well, let's just check if we fix the second, because here we are fixing c, this argument type, what if we instead fix the return type and vary the argument type, can we create fmap for it?","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_23","text":"data Op c a = Op (a -> c) fmap :: (a -> b) -> Op c a -> Op c b -- ^ a -> c b -> c -- ^ -- wrong arrow","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_34","text":"a -> c, a -> b, makes b -> c not good, we need b -> c to make it work. This kind of functor that works on the hask on inverted arrows is called /contravariant/","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_24","text":"class Contravariant f where contramap :: (b -> a) -> (f a -> f b)","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_35","text":"A contravariant functor, its not like a container, its sort of like a negative container, not only it does not contain it just actually requires a's for its action, needs \"fuel\" of type a. If you say instead of a's I will be providing you b's then you need to show how to convert b's to a's so that the functor accepts the a's. /\"It contains the empty matter of type a, so you have to have a warp converter\" :D/ The arrow is a covariant functor in the second argument in the return type and its a contravariant functor in the first argument. Now if you combine these two things, another interesting thing, arrow itself as a functor thingy ~C^op x C -> C~, you take a pair of morphism but the first one is flipped, so a thing of this kind of type is called a /profunctor/. Why is it called profunctor? I don't know..","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_25","text":"data class Profunctor p where dimap :: (a' -> a) -> (b -> b') -> p a b -> p a' b' -- f g (a->b) (a'->b') -- h -- result: g . h . f {- a' ---f---> a ---h---> b | / result | / | / g | / b' /","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_36","text":"An arrow is indeed the simplest profunctor. * 8.1 Function objects, exponentials Functions are separate from types. So far we've worked with this model in which types are objects in our category and morphisms are functions, types are objects soo.. functions are not on the same footing as types, right? If you are working in the category of set, which is approximation of what we do in programming, you can think of, you have object a and object b, functions between form a /hom-set/ so if types are sets, then functions are sets too. So this is sort of like sefl-referential thing about this particulary category of sets, so hom-set which is a set of morphisms between objects is also a set. But that's not generally true. In an arbitrary category we don't have this object, what we have is the /hom-set/ being an /external thing/. A hom-set being actually member of set. What we would like to have is an /internal hom-set/, an object in a category that corresponds to the sets of morphisms between two objects, a and b, somehow represents this set of morphisms. And it's possible to define it. So how would we go about this universal construction for a function object. First we define a pattern, then we define a ranking for matches and then we find the best match. So this /pattern/ for a function object must involve two types, argument type and return type, object a and b and the third part would be the third object, the candidate function object, let's call it ~z~. We have to have some kind of connection. What would define for us the action of a function on a argument that produces a result? We have to find a morphism between some of these objects that would represent this idea. But it is a relation between three objects. So we would like to put an arrow between these two objects and the third object, we can't do it in a category. How to do it in sets? Let's pick an element of this set that would be an argument and form a pair, function and an argument, this pair can be mapped to result, which is an element of b. So pairing in sets corresponds to taking a cartesian product of these two guys, ok so we can generalize it to a category called product and say ok, so we have a product","title":"+END_SRC"},{"location":"categories/#begin_src_11","text":"z' z' x a |--| |-------| | | | |\\ | | | | \\ |--| | | \\ | ------- \\ h | | \\ | h x id| \\ g' z + + \\ |--| |-------| \\ | | | z x a | \\ a->b | |..| | \\ \\ | | | | \\ \\ ---- ---.---- \\g(eval)\\ . \\ \\ |--------| \\ \\ | | +--------| ---------- | | a ---------- b","title":"+BEGIN_SRC"},{"location":"categories/#end_src_37","text":"","title":"+END_SRC"},{"location":"categories/#begin_quote","text":"So thats the pattern we are looking at. But notice, that in order to define this pattern, we have to have a product in our category. And thats a very important thing. In order to define a function object in a category, you have to first define a product. If a category doesn't have a product, then we cannot perform this construction, and you will see later that it actually makes more sense when you think about function object as algebraicly as exponential, because an exponential is like iterated product, right? So if you don't have a product, how can you have an exponential? So thats the idea behind this.","title":"+BEGIN_QUOTE"},{"location":"categories/#end_quote","text":"In the end we call this /eval/, that morphism is called evaluating a function. So thats the first thing to do. The next thing is ranking. So suppose that we have another candidate, ~z'~ z with g is better than z' with g' only if there is a unique morphism h from z' to z such that this diagram commutes ~g' = g (eval) . (h x id)~ And finally the third part of universal construction is picking the winner, the function object. ~a~ changes name to ~a -> b~ and ~g~ to ~eval~ meaning that for any other candidate ~z~ that has this function ~g~ from ~z x a~ to ~b~, there is a unique morphism ~h~ that maps z to ~a -> b~ such that this triangle commutes. /We can think of g as a function of two arguments, f(x,y,z)/ But now we are seeing something, that a function of two arguments, is equivalent to a single function that takes an argument and returns a /function type/, (hand points to z and then a -> b). There is one to one correspondence between g and h, so I have equivalence of /two ways of thinking/, one way of thinking I have a function of two arguments as a function that takes a product, and the other one its a function of one argument but it produces a function. And that's called currying. ~h :: z -> (a -> b)~ because in Haskell this function object is really represented by an arrow. And ~g :: (z, a) -> b~","title":"+END_QUOTE"},{"location":"categories/#begin_src-haskell_26","text":"curry :: ((a, b) -> c) -> (a -> (b -> c)) curry f = \\a -> (\\b -> f(a, b)) uncurry :: (a -> (b -> c) -> ((a, b) -> c) uncurry f = (a,b) -> (f a) b","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_38","text":"In category theory people often don't call this a function object, they call it an exponential. So a function from a to b would be called b to the power of a, b^a, argument goes to the top, result goes to the bottom. If we have a function that goes from ~Bool -> Int~, so its really a pair of integers, one for false, one for true. So all possible functions from Bool to Int well there are just all possible pairs of Ints, so this is really a cartesian product of ints. So you can write it as ~Int x Int~ or ~Int^2~ (/Int squared/) so also Int to the power of Bool like 1 - () 2 - Bool ... The number of functions from a to b is really b to the power of a, if you look at all possible combinations, so by counting the number of possible functions you get the counting argument, and also this shows you the connection between product and exponential. An iterated product gives you an exponential. What we do want for programming in which we have exponentials, or function types. We definently want products. There are special kinds of categories called CCC /cartesian closed categories/ that are useful in programming. Cartesian category is one that has products for every pair of objects, /closed/ means it has exponentials as well, for every pair of objects a and b it has an exponential a to the power of b, and it also has a terminal object. Terminal object is like a /zeroth/ power of an object. Its like the first power is the object itself, second power of the object is ~a x a~, then we can have these exponentials, but the zero power is terminal object. We actually want a little bit more in programming, we want /coproducts/ and /initial object/ so something that has not only cartesian products but also coproducts is called BCCC /bicartesian closed category/ and in a BCCC we can do our beautiful algebra of types. So far we've seen the algebra using products, coproducts, initial and terminal objects, first we saw that products form a monoid, coproducts form another monoid but we can combine them and they give you this /semiring/ and now we are adding exponentials. With exponentials we can do more algebra! For instance, whats a to the power of zero ~a ^ 0~? One. ~1~. But is it true for types? What is zero? Thats our initial object, it is ~void~ ~a ^ 0 = 1~ ~Void -> a ~ ()~ ~absurd~ So its a function from void to a, the right hand side is /One/, one is a terminal object, that the unit type. Are these two types equivalent. A unit type is a type that has one element. So if this is a singleton as well then we are done. So first of all, is there a function from void to a and how many are there? Well we seen this function it is called absurd. It takes a void and produces an a, so there is a function likee this, absurd. What about ~1 ^ a = 1~ This is a function that takes an argument of type a and produces a unit ~a -> () ~ ()~ so there is only one function like this, it maps all elements of a into this unit element, this is collapsing, a const turning everything into a single values. ~a ^ 1 = a~ First power of an object is the object itself, but it also has this meaning ~() -> a ~ a~ unit to a is isomorphic to type a and remember what this is, its a function that takes one element from a. There is one to one correspondence, I call it generalized element (mathematicians call it a global element) 8.2 Type alegbra, Curry-Howard-Lambek isomorphism. ~a^b + c = a^b x a^c~ ~Either b c -> a~ ~(b -> a, c -> a)~ ~(a^b)^c = a^b x c~ ~c -> (b -> a) ~ (b, c) -> a~ Currying! ~(a x b)^c = a^c x b^c~ ~c -> (a, b) ~ (c -> a, c -> b)~ Sort of like you have to learn just one thing, and then everything else kinda falls out. So probably like the best thing is to start, instead of going to highschool, just start with category theory and then everything else will just follow from this. The other thing that, exactly the same structures for types and categories appear in logic. And thats the basis of famous Curry Howard isomorphism, or sometimes called propositions as types. So this isomorphism between type theory and programming in general and logic on the other side starts with identifying what it means to, what is a proposition. In logic is a statement that can be true or false so these propositions correspond to types in programming. Just as a proposition can be true or false, type can be inhabited or not, so the truth of a proposition means that type that corresponds to it has elements, members, its inhabited and most of the types we deal with are inhabited, so they are kinda like true propositions but there are types that are not inhibited, and they correspond to false propositions, and we know one such type, thats void. So if you want to prove a proposition you just have to prove that a type has an element. In logic there are these two basic values, true and false. So the corresponding things in type theory would be void type which corresponds to false, and unit type which corresponds to true. Unit type is always inhabited with one element. |---------------------------------+----------+---------+-----------------+---------------------+--------| | Curry Howard Lambek isomorphism | | | | | | |---------------------------------+----------+---------+-----------------+---------------------+--------| | /Logic/ | true | false | and | a or b | a => b | |---------------------------------+----------+---------+-----------------+---------------------+--------| | /Types/ | () | Void | (a,b) | Either a b | a -> b | |---------------------------------+----------+---------+-----------------+---------------------+--------| | /Category/ | terminal | initial | a x b (product) | a sum b (coproduct) | b^a | |---------------------------------+----------+---------+-----------------+---------------------+--------| (a => b), a) -> b a => b and a -> b /modus ponens/ 9.1 Natural Transformations Triad of things that are foundations of category theory: 1. Category 2. Functors 3. Natural Transformations. Category is about structure, what it means. Functors are these mappings between categories that preserve structure, intuition is that they take a category and embedd it inside another category, sometimes called modelling. What if we have two different functors? How are two images related? So we would like to be able to compare images given by functors. So natural transformations are defined as mappings between functors. And these mappings have to preserve structure. Let's start with two categories ~C~ and ~D~, lets concentrate on a single object in category C, object ~a~. One functor maps this object into ~F a~ in category D, second functor ~G~ maps the same object to some object ~G a~. A /natural transformation/ would be picking a morphism between these two objects (In D, one morphism from this /hom-set/, ~Fa -> Ga~. In this way I am creating a whole family of morphism, these are /components/ of natural transformation. So the related morphism in D would be called ~alpha a~, between ~Fa -> Ga~ Now we map object b the same way ~Fb -> Gb~ and we have ~alpha b~ Having a natural transformation between two functors means they are somehow related. See naturality square","title":"+END_SRC"},{"location":"categories/#begin_quote_1","text":"personal notu: This seem like an abstract jump to a higher level. Feeling kinda lost now but am continuing to watch the lecture. Need time to process and contemplate. Same actions are used but all together somehow making a much more complex /image/. As Bartosz says natural transformation gives us a higher level language of commuting diagrams. Notice natural transformation is a higher level language in category theory while just describing commuting diagrams is like /assembly language/. And then instead of just talking about commuting diagrams you begin to notice relations between functors. In this sense products and coproduts are just a case of limits and colimits and later there will be adjunctions etc..","title":"+BEGIN_QUOTE"},{"location":"categories/#end_quote_1","text":"In programming, natural transformation would be a family of functions between endofunctors, parametirized by a type, so a natural transformation would be a polymorphic function.","title":"+END_QUOTE"},{"location":"categories/#begin_src-haskell_27","text":"alpha :: forall a . Fa -> Ga","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_39","text":"The subtle difference being, in this haskell example, in this form, we are assuming parametric polymorphism meaning if we want to define this function we must use one single formula for all a's. We cannot say do this thing for integers and do this thing for booleans. One single formula for all. And this is /much stronger/ than a categorical definition.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_28","text":"alpha . fmap f = fmap f . alpha","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_40","text":"Let's pick a list functor and a maybe functor. We talked about safe tail, lets talk about safe head.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_29","text":"safeHead :: [a] -> Maybe a safeHead [] = Nothing safeHead (x:xs) = Just x","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_41","text":"This is a function that works for every ~a~, a total function, parametric polymorphic so it is automatically /natural/. But lets prove it, lets do equational reasoning on it. Show it on both empty list and non-empty list.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_30","text":"safeHead . fmap f [] safeHead [] Nothing . Nothing Nothing safeHead . fmap f (x:xs) f x : fmap f xs Just (f x) \\ \\ safeHead (x:xs) \\ Just x - - -Just (f x)","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_42","text":"If you look at it it is actually an /optimization/. Applying an fmap on a list is expensive, so being able to safeHead first and then fmap is cheaper, of course not in Haskell, because Haskell is lazy. Basically we use a lot of natural transformations in programming so ~a -> [a]~, thats actually a naturally transformation because ~a~ is just an identity functor acting on ~a~. There are also these function that take polymorphic object and return a number, like ~length~ of a list. Takes arbitrary type of list and returns a length. Thats also a functor, the ~const~ functor which ignores its argument. Natural transformation from a list to a const functor. If you have a function from one ADT to another ADT its a natural transformation because algebraic data types are functors. Not all are because we have these /contravariant/ functors, so if we have a polymorphic function which turns weird stuff it would not be a natural transformation. At some point you might learn about generalized natural transformations which operates on mixed convariants.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_31","text":"return :: a -> m a","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_43","text":"is actually a function from identity functor to ~m~, a natural transformation and when we talk about monads this thing will be defined as a natural transformation, so a Kleisli category really has to take into account that these transformations are natural. 9.2 Bicategories OK, lets talk about category theory but just like in a really really wide area, I want to give you the view of category theory as far as I was able to look into it. Because we have these basic part of category theory, categories, functors and natural transformations. Natural transformations are mapping of functors, now every time we have mappings of things we ask ourselves do these mappings compose? That should be the first thing on our mind. What does it mean to compose natural transformations? \\alpha F -> G \\beta G -> H \\beta . \\alpha Is this a natural transformation? We have to check, we have to have another object b a -> b -> (-> F G H) -> (Fa -> Ga -> Ha) -> (Ff Gf Hf) -> Fb -> Gb -> Hb Notation used is [C, D] or D^C, that suggests something :) Note: Feeling lost now, but following along. Its all abstracted while using the same words like before, product, category, functor, it seems simple yet still not reachable, like being there but still blind not able to remember the last movement, thus feeling lost in a known environment. ... 10.1 Monads A monad is really such a simple concept. Why do people have problems with monads. It's the wrong approach of explaining what a monad is. Suppose you explain what function is, it like you go to a tropical island, there are some friendly natives and they want to learn functional programming from you and they ask you what is a function? You try to explain and give them an example, like you can have a function that takes a list of fisherman and orders them by the number of fish they caught, or another function takes person and gives you their age, or another function that takes grain and gives you alcohol, right? And they say wow this is some amazing stuff, it does all these things, and more? Function is some powerful.. then you realize maybe thats not the best approach of explaining what a function is. Maybe we try this, function is like an animal, it has a mouth and it eats input and produces output on the other side, thats a better analogy but you get in big trouble when you try to explain function composition (laugh :D) Why do we use function? So that we can structure our program, so that we can decompose our program into smaller pieces and recompose it, and the power of function is really in the dot. Thats what the power is. Dot is the composition operator in Haskell, combines the output of one into input of the other. Functions are about composition, and so is the Monad. People start by giving examples of monads, there is state monad, there is exception monad, you know, thats, these are completely different things, what do exceptions have to do with state, with input output? Well its just like with functions, functions can be used to implement so many things, but really functions are about composition and so is the monad. Monad is all about composing stuff. It replaces this dot with the /Kleisli/ arrow. ~>=>~ The so called fish operator. Ok dot ~.~ is used for this simple functions, where output matches the input, the most trivial way of composing stuff. The fish operator is used to compose these functions whose output type is /embellished/, it really the output type of a function type would ~b~, but now we are embellishing it with some stuff, with logging, by adding a string to it, the logging Kleisli arrow, but then in order to compose these things, we have like unpack the return type before we can send it to the next so actually inside the /dot/ not much is happening, just one function is called and the result is passed to another function. Inside /fish/ much more is happening because there is unpacking and there is the passing of the argument to the next function, and also, maybe some decision is taken, like in the case of exceptions we will see that. Once we have this additional step of combining functions we can make decisions, maybe we don't want to call the next function at all? A lot of stuff can happen inside the fish. And just like we have the identity function here, identity with respect to the dot, here we have this kleisli arrow that represents identity that return the embellished result of the same type and we call it ~return~ in haskell. And its called return because at some point you want to be able to program like imperative programmer. It's not like imperative programming is bad. It could be good as long as its control, and the monad lets you do in this kinda imperative style, sometimes is easier to understand your code even though it is immediately translated into this style of composing function, so this is just for our convenience. Using the kleisli arrow is equivalent of using the dot. We don't see many programs even in Haskell that are written using dots, this is called point free style where you just compose function after function and never mention arguments to these function, they are hidden, they are not given any names, they just go straight from output from one function to the input of another function. Point free style is popular with some people but it is considered hard to read. So this definition of a monad with fish operator is not the main definition that is used in other languages. ** Fish anatomy","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_32","text":"~(>=>)~ :: (a -> m b) -> (b -> m c) -> (a -> m c) f >=> g = \\a -> let mb = f a in mb >>= g |--> (>>=) :: m b -> (b -> m c) -> m c | | | class Monad m where | (>>=) :: m a -> (a -> mb) -> m b | return :: a -> m a | ------------------------ | class Functor m => Monad m where | join :: m(ma) -> ma | return :: a -> ma | | -- but mathematicians go deeper into the fish, how to implement | -- bind? | |--> ma >>= f = join ( fmap f ma) / (a -> ma) m a / m (mb) / / / join :: m(ma) -> m a","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_44","text":"Remember this is all polymorphic in a, b and c. A, b and c are completely arbitrary types, we have to be able to define the fish operator for /any/ type a,b,c. Once you say this is any type it means you cannot do anything type specific, which means you cannot do anything :D except! you have a function here that takes an ~a~, thats like you only chance to do something to a, well apply ~f~ to ~a~. So the sytax is ~let mb = f a~ like defining a local variable in other languages, in Haskell it just means giving a name, binding a name to some value. Why are we using these monads? Monads are used to provide composition for kleisli arrows, but why kleisli arrows? The magic is not in what a monad is, its just composition, the magis is why are these kleisli arrows so useful, what kind of problems they solve and why? The idea was that functional programming, everything is a pure function, and as programmers we know pure functions cannot express everything, there are so many computation like basic things, input output is not pure, a function that getChar if it returns a different character, it returns a different character every time you call it, so its not a pure functions. Then there are functions that throw exceptions. It turns out and this is a miracle that everything that can be computed using non-pure function, they can all be converted as long as you replace pure functions with functions that return an embellished result. This result is encapsulated in some weird way, this is the interesting part. Where does the monad come in, when we say I have this gigantic function that starts with argument a and produces this embellished result, and do I have to write it all inline? No, I want to chop it into pieces, into hundred side effects and then compose it. This is where monad comes in play, when you want to compose these functions you use the monad. The monad lets you split you computations into smaller computations and glue them together. One example are functions that are not defined for everything, so called /partial functions/. Square root is defined for positive integers for example, what you do? The program can blow up or it can throw an exception, but a function is not pure if it throws an exception. So we can use ~Maybe~.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_33","text":"a -> b a -> Maybe b join :: Maybe (Maybe a) -> Maybe a join (Just (Just a)) = Just a join _ = Nothing -- this doesn't explain the cleverness of maybe so let me -- define bind Nothing >>= f = Nothing Just a >>= f = f a return a = Just a -- Notice we are shortcircuiting, like exceptions, this -- function f serves like continuation. If the first part -- of the computation fails, abandon the computation -- If we want to pass information about the arrow -- then instead of Maybe we use the Either type, either string -- or value","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_45","text":"Second example is /State/","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_34","text":"-- you have a computation from a to b that accesses some -- external state S, reading of modifying it but it can be turned -- into pure function as long as we pass the state -- explicitly a -> b (a, s) -> (b, s) -- using currying. This is a kleisli arrow! a -> (s -> (b, s)) newtype State s a = State (s -> (a, s))","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_46","text":"","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_35","text":"-- categorical terms m -> T join -> \\mu return -> \\eta \\eta :: Id -> T \\mu :: m (m a) -> m a T . T --.--> T Id ----------------> + | + C \\eta | C | ----------------> T T T C -----> C ----> C \\ T / \\ ------------> / \\ | / \\ |\\mu / \\ | / \\ / T","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_47","text":"For better diagrams google /horizontal composition of two natural transformations/. These are now two dimensional diagrams! 10.2 Monoid in the category of endofunctors ** Monad Graham Hutton tutorial Note: At this point Graham Hutton's [[https://youtu.be/t1e8gqXLbsU][Monad tutorial]] seems intuitive so I will follow along and write the code leading up to the monad itself. Monads are a concept invented in mathematics in 1960s, rediscovered in computer science in the 90s and what it gives you is a new way of thinking about programming with effects. This is one of the most important new ideas in programming languages in last 25 years. The example we are going to look at is the idea of writing a function that evaluates simple experessions. What we are going to start with is by defining a simple data type for the kind of expressions that we are going to be evaluating.","title":"+END_SRC"},{"location":"categories/#begin_src-haskell_36","text":"-- data type Expr has two new constructors -- Val builds expressions from Integers -- Div builds expressions from two sub-expressions -- 1 Val 1 -- 6 / 2 Div (Val 6) (Val 2) -- 6 / (3 / 1) Div (Val 6) (Div (Val 3) (Val 1)) data Expr = Val Int | Div Expr Expr -- first version of evaluator eval :: Expr -> Int eval (Val n) = n eval (Div x y) = eval x / eval y safediv :: Int -> Int -> Maybe Int safediv n m = if m == 0 then Nothing else Just (n / m) -- new evaluator eval :: Expr -> Maybe Int eval (Val n) = Just n eval (Div x y) = case eval x of Nothing -> Nothing Just n -> case eval y of Nothing -> Nothing Just m -> safediv n m -- too much management of failure, too much noise -- the idea here is we are going to observe a common pattern -- there are two case analysis. -- when you see the same thing multiple times you -- abstract them out and have them as a definition m case __(maybe value)_____ of Nothing -> Nothing f Just x -> __(function to process result)_____ x m >>= f = case m of Nothing -> Nothing Just x -> f x eval :: Expr -> Maybe Int eval (Val n) = return n eval (Div x y) = eval x >>= (\\n -> eval y >>= (\\m -> safediv n m)) eval :: Expr -> Maybe Int eval (Val n) = return n eval (Div x y) = do n <- eval x m <- eval y safediv n m -- the maybe monad return :: a -> Maybe a = :: Maybe a -> (a -> Maybe b) -> Maybe b","title":"+BEGIN_SRC haskell"},{"location":"categories/#end_src_48","text":"Same idea works for other effects as well Supports pure programming with effects Use of effects is explicit in types Functions that work for any polymorphism","title":"+END_SRC"},{"location":"code/","text":"Code Lambda launcher Input Output Input Output/Plutus Awesome Haskell List Chris Done - a demo web browser engine This is a demonstration program that is able to load a web page up and render it like in the early 90's. It supports laying out text, different font sizes for headings, inline and block elements, hyperlinks, bold and italics. It supports mousewheel scrolling, too. I wrote this in a couple evenings, because it seemed straight-forward to do so given the libraries available today. That's a good sign for Haskell. Also, there's an inarticulate gut feeling I have that tells me maybe it's worth celebrating these days in which the web is still viewable in its simplest, earliest form. Chris Done - subset of Haskell, aimed at aiding teachers teach Haskell -- $ duet run demo.hs (\\x -> x + 5) (2 * 3) (2 * 3) + 5 6 + 5 11 FER UNIZG Haskell programming assignments How to make a random guessing game in Haskell Simple Haskell - Random Number Game Writing a Snake clone in Haskell part 1","title":"Code"},{"location":"code/#code","text":"Lambda launcher Input Output Input Output/Plutus Awesome Haskell List Chris Done - a demo web browser engine This is a demonstration program that is able to load a web page up and render it like in the early 90's. It supports laying out text, different font sizes for headings, inline and block elements, hyperlinks, bold and italics. It supports mousewheel scrolling, too. I wrote this in a couple evenings, because it seemed straight-forward to do so given the libraries available today. That's a good sign for Haskell. Also, there's an inarticulate gut feeling I have that tells me maybe it's worth celebrating these days in which the web is still viewable in its simplest, earliest form. Chris Done - subset of Haskell, aimed at aiding teachers teach Haskell -- $ duet run demo.hs (\\x -> x + 5) (2 * 3) (2 * 3) + 5 6 + 5 11 FER UNIZG Haskell programming assignments How to make a random guessing game in Haskell Simple Haskell - Random Number Game Writing a Snake clone in Haskell part 1","title":"Code"},{"location":"observations/","text":"Observations Explaining Haskell datatypes and constructors to a buddhist/yogi by using the chakra system and seed sounds of each chakra analogy: soundToChakra :: Sound -> Chakra soundToChakra = \\x -> case x of Lam -> Muladhara Vam -> Svadishtana Ram -> Manipura Yam -> Anahata Ham -> Vishuddha Aum -> Ajna stringToSound :: String -> Maybe Sound stringToSound = \\x -> case x of \"lam\" -> Just Lam \"vam\" -> Just Vam \"ram\" -> Just Ram \"yam\" -> Just Yam \"ham\" -> Just Ham \"aum\" -> Just Aum _ -> Nothing What is interesting is that in Haskell you don't say, \"well, maybe this or maybe that\" like I hear often people say in real life, and true, it does seem ambiguous to use maybe for more possibilities than one or nothing at all. You instead say \"Well, maybe just this or nothing at all\" and if you want to express more outputs, or results that involve something more than just nothing you use \"Well, either this or that\". But even more, the Maybe is actually made of \"Just this or nothing at all\" so there is always this layered approach to composition of functions, of expressions. The Maybe itself is defined by Just this or Nothing at all. Why? Well, It seems it is because we can call it Maybe precisely because it somehow produces an action and a terminating non-action ; we can also visualise it as a parent node whose name is Maybe with two children, named Just and Nothing. This intuitive tricky part is that Nothing is not like Just, Just will eventually pass something while Nothing will just say no to what ever is being passed on, simply declaring Nothing if or Maybe gets something which it doesn't need, which is not defined explicitly. In Haskell it seems each expression can have a name itself, so that it is possible to connect it to anything else, while at the same time it is defined totally or at least that is what we strive to do. In real life we sometimes use a name for an expression and then offer an explanation of what we really mean. We even might get irritated if we are asked to explain our Maybe's . Alice: \"Are you gonna do it?\" Bob: \"Maybe\" Alice: \"What do you mean maybe?\" Bob: \"Well I mean that I am going to just do it or I am going to Just sit back and do Nothing . Alice: \"But Bob please if you can't do it then do the other thing please, you can't Just sit back and do Nothing . Bob: \"Ok then, I am going to do Either this thing or that thing if you need it so much, is that ok? Just give me some time. Yet people often use the word maybe for more than one thing. You could maybe do this, or you could maybe do that, or bla.. which does seem a bit unprecise . fromEither :: Either Sound Chakra -> ThisOrThat fromEither = \\x -> case x of Left s -> This s Right c -> That c toEither :: ThisOrThat -> Either Sound Chakra toEither = \\x -> case x of This s -> Left s That c -> Right c Right Chakra == toEither (fromEither (Right Chakra)) That Chakra == fromEither (toEither (That Chakra)) So That is on our Right and This is on our Left . That contains all the 6 chakras we have in our body and This contains all the sounds we utter that work on each chakra. The intuitive distinction that might seem hard to grasp is that we seem to have another layer when we visualise the either containers, or better to say when we visualise the path from chakra container to sound container. We cannot just visualise sound and chakra flowing between but instead we must so to say go up the tree root using two levels of Either to understand the whole structure, this needs to be done in order for us to prove they all relate, that they are all isomorphic to one another. This That 6 sounds 6 chakras \\ fromEither / \\ / | \\ / \\ / | \\ / \\ / | \\ / Left + Right \\ / \\ / \\ / * | | | toEither Right Chakra == toEither (fromEither (Right Chakra)) That Chakra == fromEither(toEither (That Chakra)) Left Sound == toEither (fromEither (Left Sound)) This Sound == fromEither (toEither (This Sound)) Either sound chakra is making a statement saying that we have one of two possible values of types x or y in it. It turns out that Either , or sum types more generally, have a very reasonable and useful dual construction in which we have both x and y . In Haskell, we can convey thia idea as well. Let's call it Pair . data Pair x y = MakePair x y That is, a pair of a Chakra and a Sound will have type Pair Chakra Sound . So while with Maybe we had to just pick one or nothing at all, and in Either we could pick this or that, which were some real alternatives, in a Pair we have this notion of carrying both alternatives with us, they are together and we are not making a choice by choosing either this or that. It does seem like Maybe relates somehow to the notion of Identity since it is just giving us the thing we ask while the Either resembles to a Boolean choice or either this or that, either though we could say Maybe as well resembles a boolean choice since Just could be understood as true and Nothing as false . It is interesting to find relations with basic mathematical functions. In that case Either could be understood as showing two truth values and yet we are only supposed to pick one, now that really resembles to OR ! But bool values always pick just one so even though this thought process smells wrong it does bring benefit when learning Haskell to compare it with previous concepts. Quote from atop: We say Either and Pair are fundamental because once we can group or tell apart two things, we can group or tell apart as many things as we want. Once we begin to extend our space of pairs we realize pairs are just like containers of two. Can we imagine triples? Or quads? If we jump to a musical analogy, a pair is a musical interval, a pair of two tones in any scale makes an interval. Once we hear a tone it is like hearing a point in space, though it can sound pretty we know very little about it since the whole sound is coming from one source (aha but what when both sources sing the same tone? Where is the source then?). To get back a pair of two tones is like a line, and a triplet of three tones can be understood as a triangle, a musical chord, defining a space between three tones. Technically speaking a musical chord is a cluster of three or more tones. Let us add three tones, c, e, and g that will make a C chord add3tones :: (Tone, Tone, Tone) -> Tone add3tones = \\(c, e, g) -> c + e + g Let vs Where It seems the difference between let and where lies in the order of declaration. When we want to first name the things we want to use in our action we use let in the sense, let there be light, let there be this, let there be that and then after we have declared all out lets we will begin an action like \"ok now create this\". We could understand it like this as well: let there be a human called \"Bob\" let there be a human called \"Alice\" in \"Alice\" loves \"Bob\" -- further let human = \"Bob\" let human = \"Alice\" \"Bob\" loves \"Alice\" -- further we ommit using let twice let firstHuman = \"Bob\" secondHuman = \"Alice\" in \"Bob\" loves \"Alice\" {- This was not valid Haskell code, but it is written like this in order to increase conceptual understanding of let vs where expressions -} -- valid Haskell let x = 2 y = 3 in x + y Now, when we use where we like to first define our action or an event happening and then tell something about our actors who define the event itself. Something like \"where some event is happening, its components are this and that, or where Alice loves Bob, Bob is human and Alice is human too.\" If we miss describing a single component of our event the event will not compile, Haskell will not accept our declaration. Notice we could define a component and not use it in our action, but it will still go through, and the event will not fail, same as declaring \"Alice\", \"Bob\" and \"Hayka\" and then telling \"Alice\" loves \"Hayka\", the program would run even though \"Bob\" was left all by himself. The above let expression can be written with where like: love = human1 + human3 where human1 = \"Bob\" human2 = \"Alice\" human3 = \"Hayka\" Let's use correct Haskell code for our love program. We will use '++' instead of just '+' since '++' is used to add or concatenate words, aka strings (of characters). love = human2 ++ \" loves \" ++ human3 where human1 = \"Bob\" human2 = \"Alice\" human3 = \"Hayka\" -- and our previous number example with let add = x + y where x = 2 y = 3 More of where Let us use the where in a slightly developed example, this is a variation on \"Haskell from first principles\" chapter 3 exercise, but we will use first and last name instead of hello world. The difference is in two examples is that we will abstract our name into first and last while in the book \"hello\" and \"world\" is used for expression and for the function name as well. module MyName where {- before we define our name, I am just using my own here, we say that myName is a type of a string, as in a string of characters, then we define myName by concatenating the first and the last name. We are not explicitly mentioning first and last name, but instead just providing the value of the first and last name, which is \"domagoj\" and \"miskovic\" -} myName :: String myName = \"domagoj\" ++ \" miskovic\" -- but now we will declare the firstName is \"domagoj\" firstName :: String firstName = \"domagoj\" -- and last name as \"miskovic\" lastName :: String lastName = \"miskovic\" {- and now we will print out the myName and then we will link together our firstName and lastName into one expression by calling our firstName and lastName and Haskell should print out the values we provided before when we run the program. -} main :: IO () main = do putStrLn myName putStrLn firstAndLast where firstAndLast = concat [firstName, \" \", lastName] We introduced firstAndLast function with a where which was not maybe necessary. We could have just linked together before declared firstName and lastName without the where by writing putStrLn (concat [firstName, \" \", lastName]) but in Haskell, even from simplest examples, one is practicing composability, abstracting from the tiniest to the grossest. Since we learned where which is like a basic tool for abstracting, describing our processes, like separating the what and the who, we should use it in our exploratory learning and try to apply it when ever we can. Here is a simple sum of squares example with where: -- sumOfSquares.hs sumOfSquares x y = square x + square y where square n = n * n Let us use lambda for our Sum of Squares module SOS where sumOfSquares :: Integer -> Integer -> Integer sumOfSquares = \\x y -> square x + square y where square = \\n -> n * n Here also, we could have just used sumOfsquares x y = x*x + y*y instead. But there is something else too, there are too many squares, square this square that, though as a learning aid repetition is very good. Little children like to repeat things they learn, so do we. -------------------------------------------------------; Scope Note: Haskell from first principles builds intuition by first going through various syntax examples and then plays with local and top level definitions by using let and where as basic tools for abstraction. What we notice is the importance of spacing in Haskell code, from whitespace that has a silent like mysterious apply function because a simple f x meaning f is applied to x , to the silent matrix like grid system or invisible columns and rows define the play between local and global or top-level definitions. Global might be a wrong word so we use top-level which brings us somewhat closer to the code at hand, and a bit away from all expansive global notions. a top-level definition is basically like a tree and local definitions are like branches which have a life of their own within that same tree. /----- + ===[ ------ | \\----- | | | /----- + ==={------ \\_____ -----------------------------------------------------; Sometimes in real life conversations you will listen to two people talking, and one of them will ask a question to another but you will not really know what they mean by it. The second person might already answer it and you will still ponder what the first person really meant. Then later when the second person leaves you ask the person who was answering: \"Hey what did she mean with that question? I did not understand it.\" \"Oh, you know sometimes she talks like this, she knows I know what she really means so she just cuts out the whole question and asks me implicitly.\" What basically happened is that I was left out of the scope of the conversation so I could not put together what they meant by it. They were talking about local definitions without providing explicit values to those definitions so I was left out wondering what those values were. Unfortunately sometimes people get angry too when you do not ask them explicitly something but force you to provide explicit local definitions, remebering something they wish to tell . People use these aggresive tactics sometimes. Somebody might tell you: \"Hey, when are you going to cut your hair!?\" What they mean by it it that your hair is too long but they do not explicitly say it out loud. You might play naive and ask them, well I do not know, why do you ask? Then the other person expects you to know what they really mean and that is that your hair is too long and that you should cut it. Unfortunately these situations sometimes happen in various shapes and sizes. ---------------------------------------------------; To String or not to String Let's go through some of the exercises from chapter 3 in HaskellBook and try out basic abstractions. We would like to append or concatenate two separate strings into one string. Our first string will be \"Curry is awesome\" and our second string will be an exclamation mark '!'. Our '!' exclamation mark is actually a character, and characters are embraced by single quotes ' ' while actual strings which are a collection of characters are embraced by double quotes \" \" so a letter a is written as 'a' while a string abcd is written as \"abcd\" . Now, can we write a character a as a string by putting double quotes around it, as in \"a\" . Yes we can. But how is that possible? > \"a\" == 'a' ..error: Couldn't match expected type '[Char]' with actual type 'Char' > \"a\" == ['a'] True > [' ', 'a', ' '] \" a \" > \" a \" == \"a\" False Hmm.. what is actually happening here? Let us try appending two chars, a and b and at the same time check with == sign if our expression is true > appendTwoChars = \"a\" ++ \"b\" == \"ab\" > appendTwoChars True -- by trial and error we see that > appendTwoChars = 'a' : 'b' : [] == \"ab\" > appendTwoChars True Looking at this we could say that a string is a list of characters, represented by square brackets [ ] while characters have only single quotes meaning double quotes are like single quotes embraced by a bracked. > ['a'] == \"a\" True Ok, so back again to our \"curry is awesome\" and \"!\" example. Let's concatonate them and abstract these examples into a function that will concatenate any two strings. > concatenateTwoStrings = (\"Curry is awesome\" ++ \"!\") == \"Curry is awesome!\" > concatenateTwoStrings True > concatenateTwoStrings firstString secondString = firstString ++ secondString > concatenateTwoStrings \"first\" \" second\" \"first second\" -- using beloved lambda > concatenateTwoStrings = \\firstString secondString -> firstString ++ secondString > concatenateTwoStrings \"hello\" \" world\" \"hello world\" Types Though usually first basic dataype that is introduced in Haskell books is the Bool datatype we will start with something different, which might bring our intuition closer to Bool. It is important to note that while HaskellBook begins the whole type story with introducing data Bool = False | True , same as Graham Hutton's Programming Haskell , A Type of Programming introduces types as four seasons of the year. data Bool = False | True data Season = Winter | Spring | Summer | Fall We too will define a Human datatype with its two data constructors Male or Female data Human = Male | Female -- let's change some genders, for now just keeping it as a binary choice for -- making things simple changeGender :: Human -> Human changeGender Male = Female changeGender _ = Male > changeGender Male Female > changeGender Female Male We are using or instead of and because the | symbol, the so called Pipe, indicating logical disjunction \"or\", indicates that this is a sum type . So a human can be a female or a male but not both at the same time. But is this the only way we can write data declarations? How about using and and various what if this and that happens relations. Types are never so pure in real life, and we often need to ponder before we implement a solution. Seems to me a Haskeller can spend an eternity just pondering basic questions. \"A type!\" a haskeller says, \"What the hell is a type?\" It is important not to over think the type relations but somehow mix and match while progressing in our composition. The good part is that type driven design can eliminate many failures that can happen along the way, and in turn enable us to deal with future turns with much greater success. HaskellBook tells this in a very clear way by describing the basic flow of a data type declaration. The whole thing is called a data declaration. Data declaration do not always follow precisely the same pattern - there are datatypes that use logical conjuction ( and ) instead of disjunction, and some type constructors and data constructors may have arguments. The thing they have in common is the keyword data followed by the type constructor (or name of the type that will appear in type signatures), the equals sign to denote a definition, and then data constructors (or names of values that inhabit your term-level code). data Mood = Happy | Sad changeMood :: Mood -> Mood changeMood Happy = Sad changeMood _ = Happy > changeMood Happy Sad > changeMood Sad Happy Five Types of Buddhas Playing with sum types and five Buddha families data Buddha = Vairocana | Amoghasiddhi | Amitabha | Ratnasambhava | Akshobhya data Color = White | Green | Red | Gold | Blue data Element = Space | Air | Fire | Earth | Water data Cardinality = Center | North | West | South | East data Stress = Ignorance | Jealousy | Selfishness | Pride | Aggresion data Season = None | Summer | Spring | Autumn | Winter data Wisdom = Meditation | Perfection | Observation | Equanimity | Reflection data Symbol = Wheel | Vajra | Lotus | Jewel | Sceptre data Means = Turn | Protect | Magnetize | Enrich | Pacify colorOfBuddhas :: Buddha -> Color colorOfBuddhas = \\x -> case x of Vairocana -> White Amoghasiddhi -> Green Amitabha -> Red Ratnasambhava -> Gold Akshobhya -> Blue seasonOfBuddhas :: Buddha -> Season seasonOfBuddhas = \\x -> case x of Vairocana -> None Amoghasiddhi -> Summer Amitabha -> Spring Ratnasambhava -> Autumn Akshobhya -> Winter wisdomOfBuddhas :: Buddha -> Wisdom wisdomOfBuddhas = \\x -> case x of Vairocana -> Meditation Amoghasiddhi -> Perfection Amitabha -> Observation Ratnasambhava -> Equanimity Akshobhya -> Reflection This looks clear and yet we feel something missing. We would like to now define more complex constructors, and we will get back to this when we understand a bit more about Either and Maybe which will enrich our flow of types. I showed this chart to a person who knows something about these Buddhas and yoga and intuitively the person understood the flow of types. There five families are also a nice way to put various groups of five together and think on various functions between them. --------------------------------------------------; Addition Curry addOne :: Natural -> Natural addOne = \\x -> x + 1 addTwo :: Natural -> Natural -> Natural addTwo = \\x -> addOne (addOne x) addTwo ----------------------------------- / / /\\ / / / \\ / addOne / addOne / ----------------- ----------------- | \\x ------ || \\x ------ | | | | || | | | x ----------->| + |----------->| + |-----------> | 1--->| | || 1--->| | | | ------ || ------ | | || | ----------------- ----------------- Our addOne can be also understood an an incrementor function because it increases or increments the number we give it to. So if we give a number 0 to the incrementor function it gives us back the number 1, then if we give it the number 1 it gives us back the number 2; we can give it any number and it will always give back one number back. You could also say it sends the number it recieves to the next one. It is a very simple function. What does simple mean? Maybe better to use the word primitive or small in the sense that it is easy to understand conceptually. Simple is a complex word because people have different notions on what is simple and what is complex. The opposite of our increment function would be a function that would send the number we provide to the previous number, so if we gave it a number 3 it would give us the number 2. It decreases the number we give it to, so we will call it the decrement function. So within our natural number realm our increment function behaves the same as our addOne function because it adds a number 1, and our decrement function behaves the same as the function subtractOne which subtracts a number 1 from the one we give it to. addOne :: Natural -> Natural addOne = \\x -> x + 1 subtractOne :: Natural -> Natural subtractOne = \\x -> x - 1 -- rewritten as increment and decrement would be incr :: Natural -> Natural increment = \\x -> x + 1 decr :: Natural -> Natural decrement = \\x -> x - 1 -- hmm can we make these functions to add any number we want? incAny n = \\x -> x + n decAny n = \\x -> x - n -- Maybe you haven't applied a function to enough arguments? incAny x n = \\x -> x + n decAny x n = \\x -> x - n -- Maybe you haven't applied a function to enough arguments? incAny x n = \\x n -> x + n decAny x n = \\x n -> x - n -- Maybe you haven't applied a function to enough arguments? incAny = \\x n -> x + n decAny = \\x n -> x - n > incAny 3 4 7 > decAny 3 4 -1 Addition is a function that maps two natural numbers to another one. Let's see different ways we could add two numbers using our incrementor and decrementor function addition = \\x y -> (increment x) + (decrement y) > addition 3 4 > (increment 3) + (decrement 4) > ((\\x -> x + 1) 3) + ((\\x -> x - 1) 4) > ([x=3] 3 + 1) + ([x=4] 4 - 1) > (3 + 1) + (4 - 1) > 4 + 3 > 7 addition = \\x y -> 1 + ((decrement x) + y) addition 3 4 1 + ((decrement 3) + 4 1 + ((\\x -> x - 1) 3) + 4 1 + ([x=3] 3 - 1) + 4 1 + (3 - 1) + 4 1 + 2 + 4 1 + 6 7 > :type addition addition :: Integer -> Integer -> Integer Now let's expand our calculation with more steps. addition = \\x y -> (increment x) + (decrement y) addition 3 4 = (\\x y -> (increment x) + (decrement y)) 3 4 = (increment 4) + (decrement 3) = (increment 5) + (decrement 2) = (increment 6) + (decrement 1) = 7 + 0 = 7 addition = \\x y -> 1 + ((decrement x) + y) addition 3 4 = (\\x y -> 1 + ((decrement x) + y)) 3 4 = 1 + ((decrement 3) + 4) = 1 + (1 + (decrement 2) + 4) = 1 + (1 + (1 + (decrement 1) + 4)) = 1 + (1 + (1 + 0 + 4)) = 1 + (1 + (1 + 4)) = 1 + (1 + 5) = 1 + 6 = 7 I still can't explain how but I find the second version so pretty, it remembers its steps, as in takes much more care when calculating, it just seems more intelligent way to add things, even though it requires more steps. I always would find it strange when people talk about something taking less time as being better. I could see my mind stopping and pondering why would that be true, why would optimizing have this guideline which seems so narrow. Yes, there are valid explanations and reasons for this but still, the notion of embedding more information within each level of your computation somehow seems much more intelligent approach in the long run. Who knows maybe artificial intelligence is precisely one long extensive computation where each step is carrying a billion other steps all in sync so why not explore that direction instead. Well, for more input on these increment and decrement function check the amazing SICP lectures by Abelson and Sussman. These are just pure awesomeness, I have yet to write about them and how they helped in motivating this functional pursuit into haskell. Who knows, maybe Lisp lurks somewhere in Haskell heart too. Addition is associative ((x + y) + z = x + (y + z) meaning if three people x y and z want to go out of the house, no matter who comes out first by the time all three come out of the house there will be three of them out of the house. Also notice if you haven't actually witnessed these people coming out of the house how can you tell who came out first and what actual difference does it make? If we the three of us are going to a party and all three of us come out of the house to take the bus, there is no difference who comes out first or who goes in the bus first since all three of us are going to the party anyway. Well maybe it does matter for some mysterious spooky action at a distance reason, but our coming out of the house is an associative operation. Also when we have to pay for the ticket 10 dollars, it makes no difference if we give the driver first a 5 dollar note and then a 2 dollar notes and then a 1 dollar note or if we give the driver the money in some other order. We still have to pay 10 dollars. But these are simple examples. I'm thinking on what would be an associative operation in music, as in if I have a chord of three notes does the order matter when I play them? The similar idea to our three little friends would be if I had to play the three notes at the same time, then there would be no difference, but still, then I cannot see the whole process of two separate groupings of three notes. Still, you could visualise giving more awareness to some two notes within a three note chord and they will then sound a bit different even though you will play them at the same time. This is one of the key features of great musicians that even when they play three notes at the same time, they never really play them in the same way, they often do subtle movements that somehow increment your perception of music. These movements are so subtle they could be barely described as movements and still, if you are attentive you can hear them. ----------------------------------------------------; In Mathematics, the associative property is a property of some binary operations. Within an expression containing two or more occurrences in a row of the same associative operator, the order in which the operations are performed does not matter as long as the sequence of the operands is not changed. Associativity is not the same as commutativity, which addresses whether or not the order of two operands changes the result. Seems to me my whole previous rambling was describing commutativity and not associativity ! But they do seem awfully similar: x + (y + z) = (x + y) + z x + (y + z) = (x + y) + z 1 + (2 + 3) = (1 + 2) + 3 1 + (1 + 1) = (1 + 1) + 1 1 + 5 = 3 + 3 1 + 2 = 2 + 1 6 = 6 Still what seems clear from this is we can shift things around, we can commute them around and we get the same result while with associative property we can group things differently but the operations we apply to them will not change the result. Why does this require so much pondering? There is this smell that associative properties and commutative properties can change and often seem intermixed depending on the operations we use with them. All functions in Haskell take one argument and return one result. This is because functions in Haskell are nested like Matryoshka (Babushka) dolls in order to accept \"multiple\" arguments. The way the (->) type constructor for functions works means a -> a -> a represents successive function applications, each taking one argument and returning one result. The difference is that the function at the outermost layer is returning another function that accepts the next argument. This is called currying . Our addTwo was defined for just two input numbers. What if we provide three numebrs to add? > 1 + 2 3 > 1 + 2 + 3 6 > (+) 1 2 3 > (+) 1 2 3 error:Non type-variable argument in the constraint.. -- hmm.. ;; this is written in lisp, a functional language but here plus ;; is applied to every argument when written in prefix notation. > (+ 1 2) 3 (+ 1 2 3) 6 (+ 1 2 3 4 5) 15 Let's get back to Haskell. Though it is possible to put plus before the arguments similar to lisp it does not really work the same way, it still somehow takes only two arguments. Is this limited? Not really, what I understand now is that the plus function itself is defined minimally in Haskell, like the very essence of plus is defined as something that adds at least two things, but to go even further in Haskell, only two things! If you need to add three things then use two pluses, use two functions to add three things. If we wanted to apply plus to more than two arguments without defining how many, it seems we would need a function that would map somehow this plus to any number of numbers we want. This seems like we actually need two functions to get a similar behaviour like in lisp example. This does seem super cool, meaning functions in Haskell are interlinked somehow each having one input and one output. Even our imaginary mapping function would not really add a million numbers but would tell plus to add a million numbers, orchestrating the event coordinating many little pluses to do the addition, collapsing the numbers like dominoes. So how is lisp then just adding many numbers by having a plus function at the beginning of the expression? Ponder the pond.. The way the type constructor for functions, (->), is defined makes currying the default in Haskell. This is because it is an infix operator and right associative. Because it associates to the right, types are implicitly parenthesized like so: f :: a -> a -> a -- associates to f :: a -> (a -> a) -- and map :: (a -> b) -> [a] -> [b] -- associates into map :: (a -> b) -> ([a] -> [b]) The association here, or grouping into parentheses, is not to control precedence or order of evaluation; it only serves to group the parameters into argmuents and results, since there can only be one argmuent and one result per arrow... But this very technical desription of this action does not fully explain this behaviour except by mapping out every movement into words. What I do realize from previous pondering is that as with our plus sign visually it is supposed to be right associative merely because the result is the last thing we are going to get if the function completes so the result is embraced with the last argument meaning the last function is going to take the numbers given to it by all the previous functions and give us the result. At least this idea helped me to deepen the understanding of this movement. Notice also the explanation says that This is because it is an infix operator and right associative.. Now the expression is saying that something happens because something is like this or that. Such definitions only provoke further questions but why is is like that then? Next it continues with again just repeating the previous expression with Because it associates to the right.. And again I know and see it associates to the right but why is the word because used to explain something by just repeating what was already said? And that is why it is parenthesized like that. But I already see that myself, what idea is actually expressed here? I am sure many might stop and ponder what this really means. By thinking on functions that only pass on what was given and at the same time do just a small calculation that in whole creates the final result we can see that it might be obvious that it will be parenthesized to the right. Again it is difficult to put this thinking flow into words since words are so vague and well, code is really explicit. Maybe that is why I am pondering so much on this plus sign, associativity and commutativity. > map (+) [1,2,3,4] error > map (1+) [1,2,3,4,5] [2,3,4,5,6] -- hmm Mappings Consider the expression 7 - 4 + 2 . The result could be either (7 - 4) + 2 = 5 or 7 - (4 + 2) = 1 . The former result corresponds to the case when + and - are left-associative, the latter to when + and - are right-associative. When I type this expression into ghci or an android calculator I get 5 as the result. A mapping function applies a given function to each element of a list or other collemtion. (mapcar 'car '((a 1) (b 2) (c 3))) => (a b c) (mapcar 'cdr '((a 1) (b 2) (c 3))) => (1 2 3) (mapcar 'string \"abc\") => (\"a\" \"b\" \"c\") How is this mappping mapcar defined in emacs lisp? (defun mapcar (function &rest args) \"Apply FUNCTION to successive cars(heads) of all ARGS. Return the list of results.\" ;; If no list is exhausted, (if (not (memq nil args)) ;; apply function to cars. (cons (apply function (mapcar 'car args)) (apply 'mapcar function ;; recurse for rest of elements (mapcar 'cdr args))))) (mapcar 'cons '(a b c) '(1 2 3 4)) => ((a . 1) (b . 2) (c . 3)) Compare the elisp mapcar with the Haskell definition of map map :: (a -> b) -> [a] -> [b] map _ [] = [] map f (x:xs) = f x : map f xs We notice that from line f x : map f x the f x looks like (mapcar 'car args)) and f xs looks like (mapcar 'cdr args). car in lisp lingo is actually same as haskell's first and cdr is same as rest , meaning x and xs are written in the same style, like saying after the first beginning come many more beginningS . Actually after the first comes always the rest, no matter how many or in which way they appear. There wouldn't be any firsts if there were no rests, right? In Haskell they like to use variable names like n and ns for number and numbers and some more polymorphic like variables for x and xs Notice also that cons from elisp definition is the cons operator which is represented in haskell as infix operator : stuck between f x and map f xs . Also elisp mapcar definition first says (if (not (memq nix args)) which relates to haskell's map _ [] = [] . Haskell seems to be just saying if anything, meaning _ as an empty placeholder which is used in pattern matching, is provided to an empty map _ [] then return an empty list [] Why? Because we have nothing to do with an empty list. We could define an amazing function but if we do not provide any arguments to it nothing will happen, because an empty list has no members, it is just empty. And out of an empty list input how would we create a filled list of an output? This imaginary function would have to be some sort of an artist that would create something out of nothing, worth pondering about.. map f (x:xs) = f x : map f xs map (+3) [1, 2, 3, 4, 5] [(1 + 3), map (+3) [2, 3, 4, 5]] [(1 + 3), (2 + 3), map (+3) [3, 4, 5]] [(1 + 3), (2 + 3), (3 + 3), map (+3) [4, 5]] [(1 + 3), (2 + 3), (3 + 3), (4 + 3), map (+3) 5] [(1 + 3), (2 + 3), (3 + 3), (4 + 3), (5 + 3)] [(1 + 3), (2 + 3), (3 + 3), (4 + 3), 8] [(1 + 3), (2 + 3), (3 + 3), 7, 8] [(1 + 3), (2 + 3), 6, 7, 8] [(1 + 3), 5, 6, 7, 8] [4, 5, 6, 7, 8] Let's see how map is defined in another functional lisp dialect, Clojure Returns a lazy sequence constisting of the result of applying f to the set of first items of each coll, followed by applying f to the set of second items of each coll, until any one of the colls is exhausted. Any remaining items in other colls are ignored. Function f should accept number-of-colls arguments. Returns a transducer when no collection is provided. (map inc [1 2 3 4 5]) ;;=> (2 3 4 5 6) (map + [1 2 3] [4 5 6]) ;;=> (5 6 7) From Brave Clojure I think of abstractions as named collections of operations. If you can perform all of an abstraction's operation on an object, then that object is an instance of the abstraction. I think this way even outside of programming. This seems like a nice thinking process for building something like abstractions but I feel this notion of an abstraction being a named collection of operations should also have a unique property, the result which transformed the collection itself. This abstraction principle does not have to go both ways just by unfolding the operation which was applied to the collection. In some sense, you could abstract the mapping of pluses to a completely different operation that would in turn do the addition as a side effect while just mapping pluses to an arbitrary number of operands does seem like a trivial form of abstraction principle. The prejudice and respect for abstract thinking are so great that sensitive nostrils will begin to smell some satire or irony at this point Seems to me Hegel would be a wonderful Haskeller, this short description somehow relates in a nice way to our mapping definitions, just think of the difference between a total and a partial function: For Hegel, only the whole is true. every stage or phase or moment is partial, and therefore partially untrue. Hegel's grand idea is \" totality \" which preserves within it each of the ideas or stages that it has overcome or subsumed. Overcoming or subsuming is a developmental process made up of \"moments\" (stages or phases). The totality is the product of that process which preserves all of its \"moments\" as elements in a structure, rather than as stages or phases. -------------------------------------------; Viewing type signatures In chapter five exercises page 221 of HaskellBook it is asked to figure out how the type would change and why, and make a note of what we think of the new inferred type would be. -- Type signature of general function (++) :: [a] -> [a] -> [a] -- How might that change when we apply -- it to the following value? myConcat x = x ++ \" yo\" We will get myCOncat :: [Char] -> [Char] but what is interesting that somehow the type did not seem to change but only the frame of reference from which the signature is being observed, as in first we saw the type signature by unfolding the (++) which said that it concatenates two things to get another thing , and then we see the myConcat function which is made out of concatenating a thing with some other thing called \" yo\". The first frame is seeing just the (++) and how it interlinks [a] with [a] into an [a] while the second frame is seeing the same (++) but this time we are witnessing a type of a function which is using the (++) to interlink two [Char] s. That's why we have only two [Char] and not three [a] s like in the pure (++) signature. Did the type really change as the exercise says it does? I find this actually confusing because the viewpoint seems to change while the signature stays the same. Let's try with the next one. (*) :: Num a => a -> a -> a myMult x = (x / 3) * 5 :t myMult myMult :: Fractional a => a -> a And here we witness that the myMult has a signature which seems to be an instance of the more general Num a typeclass. Again we see the conversion of the view, in the first type signature the (*) is being seen as an operator that interlinks from one to the second which gives the third while our myMult function is some fractional instance or a result that is being interlinked from two variables a and a . We can intuitively understand that a fractional number is an instance of a larger family of numbers. After all we do say a fractional number and Haskell's type system works in the same way, type classes are like large families of instances of minor type families which are often automatically inferred by Haskell, inferred meaning Haskell can sign our functions with appropriate types. But it cannot do that for all. If we use our Buddha example from before Haskell would not know anything about the Buddha typeclass and all the instances of deities, buddhas, sages, etc that belong to the Buddha typeclass. In the next example we can see this changing view even better. (>) :: Ord a => a - > a -> Bool myCom x = x > (length [1..10]) :t myCom myCom :: Int -> Bool Here it seems that the myCom signature is telling us just the beginning and the end of our function which is defined as some operation that takes an Int and produces a Bool while the (>) signature has nothing at all to do with this function because it is defined just by itself, how it an instance of some Order typeclass apparently having something to do with ordering things and that it will take something and compare it with something else, finally producing a Bool which will tell us True or False , meaning if the first thing we provide it is trully greater than the second thing. myCom on the other hand is just saying: \"Hey I'm some function that is going to tell you if your number is true or not\". Seems to me we cannot really see what is happening from this type signature myCom :: Int -> Bool , what is this type actually doing? Credit Card Digits CIS 194: Homework 1 When solving the homework, strive to create not just code that works, but code that is stylish and concise. Try to write small functions which perform just a single task, and then combine those smaller pieces to create more complex functions. Don't repeat yourself: write one function for each logical task, and reuse functions as necessary. Validating Credit Card Numbers How do websites validate your credit card number when you shop online? They don't check a massive database of numbers, and they don't use magic. In fact, most credit providers rely on a checksum formula for distinguishing valid numbers from random collections of digits. What is a checksum? Wiki: Checksum A checksum is a small-sized datum derived from a block of digital data for the purpose of detecting errors that may have been introduced during its transmission or storage. By themselves, checksums are often used to verify data integrity but are not relied upon to verify data authenticity. The procedure which generates this checksum is called a checksum function or checksum algorithm. Depending on its design goals, a good checksum algorithm will usually output a significantly different value, even for small changes made to the input. In this section, you will implement the validation algorithm for credit cards. It follows these steps: Double the value of every second digit beginning from the right. That is, the last digit is unchanged; the second-to-last digit is doubled; the third-to-last digit is unchanged; and so on. For example, [1,3,8,6] becomes [2,3,16,6] Add the digits of the doubled values and the undoubled digits from the original number. For example, [2,3,16,6] becomes 2+3+1+6+6 = 18 :confused: Note: I do not understand how is this calculation done. How am I supposed to add [2,3,16,6] and [1,3,8,6] Note: So 16 is not a digit? Numerical digit A numerical digit is a single symbol (such as \"2\" or \"5\") used alone, or in combinations (such as \"25\"), to represent numbers (such as the number 25) according to some positional numeral systems. The single digits (as one-digit-numerals) and their combinations (such as \"25\") are the numerals of the numeral system they belong to. For example,the decimal system (base 10) requires ten digits (0 through to 9), whereas the binary system (base2) has two digits (e.g.: 0 and 1) Note: add the digits of the doubled values and the undoubled digits from the original number. so [2,3,16,6] = 2 + 3 + 1 + 6 + 6 = 18 and [1,3,8,6] = 1 + 3 + 8 + 6 = 18 oh wow now I see, this is cool, never seen this one before. calculate the remainder when the sum is divided by 10. rem 18 10 => 8 Exercise 1 We need to first find the digits of a number. Define the functions. toDigits :: Integer -> [Integer] toDigitsRev :: Integer -> [Integer] -- toDigits should convert positive Integers to a list of digits. -- for 0 or negative inputs, toDigits should return the empty list. -- toDigitsRev should do the same, but with the digits reversed. Note: So it is about the positional system, we somehow need to decompose a decimal number into its digits, aka it's tenths, hundreds?, thousands,.. example: a number 12345 into 5, 45, 345, 2345.. at least that is the first idea. Later we have to somehow add single digits, for now be begin to extract the last digit. lastDigit :: Integer -> Integer lastDigit = (`mod` 10) -- this will give us the last digit, or the 10th column -- for example to get 100ths or 1000ths we would done lastDigit 1234 4 lastTwoDigits = (`mod` 100) lastTwoDigits 1234 34 -- or lastThreeDigits = (`mod` 1000) lastThreeDigits 1234 234 -- also divMod 1234 10 (123,4) -- now check this magic dropLastDigit :: Integer -> Integer dropLastDigit = (`div` 10) dropLastDigit 1234 123 So our dropLastDigit and lastDigit are like basic operators on lists! Usual haskell tutorials mention these but too bad there are no examples like the credit card because here we have extraction of digits from a single number which at first seems like a difficult problem. What are our list operators? init and.. last -- have to put tea :) -- gosh, still have to learn vim.. head [1,2,3,4] 5 tail [1,2,3,4] [2,3,4] last [1,2,3,4] 4 init [1,2,3,4] [1,2,3] Next we have to write a function that splits a number into its digits in reverse order called, toDigitsRev toDigitsRev :: Integer -> [Integer] toDigitsRev n | n <= 0 = [] | otherwise = lastDigit n : toDigitsRev (dropLastDigit n) But how to just get digits of some number in Haskell? stack overflow question 135 `div` 10 = 13 135 `mod` 10 = 5 generalize into a function: digits :: Integral x => x -> [x] digits 0 = [] digits x = digits (x `div` 10) ++ [x `mod` 10] -- and reverse toRevDigits :: Integral n => n -> [n] toRevDigits 0 = [] toRevDigits n = n `mod` 10 : toRevDigits (n `div` 10) [ ] - Check more interesting answers on stack on this question. Note: Notice lexi-lambdas solution is more elegant since the functions are already abstracted. Before beginning to just solve the function provided here we see two smaller functions composed into one. Next lexi-lambda observes: Sure enough, that fixed things. Still, something about me doesn't like the asymmetry between the two expressions on either side of the cons (is it still called that in Haskell?). I remember something about where clauses; let's try that. toDigitsRev :: Integer -> [Integer] toDigitsRev n | n <= 0 = [] | otherwise = d : toRevDigits ds where d = lastDigit n ds = dropLastDigit n Well, that seemed to work, but is it nicer...? I don't know. Now it just seems more verbose, honestly. I can't really decide.. (break) -- gotta do lunch.. :gem: We see that easily we can decompose our thinking process into smaller and smaller functions, or maybe better to say our flow of thinking includes creating small functions chained together, each chain creating in turn a higher function, and so it continues. Sometimes, as in the first lexi-lambda example where we see only a partial application of div and mod abstracted into a unique function while in stack overflow answers these calculations are not abstracted but included in the final toDigitsRev function. What seems better? Am I to abstract partial applications as well? As a learning method this seems good. Chaining smallest possible entities into larger ones. How flexible is that actually? Is there some cognitive upper bound since there are many abstracted functions. This also brings to the point the qestion on naming functions. Our names should reflect the intention of the function. Also seems to me that abstracted functions are easier to read since there are visually fewer calculations. Hm.. End of Exercise 1 and 2 (break) -- gotta do lunch.. Exercise 3 The output of doubleEveryOther has a mix of one-digit and two-digit numbers. Define the function: sumDigits :: [Integer] -> Integer to calculate the sum of all digits. Example : sumDigits [16,7,12,5] = 1+6+7+1+2+5=22 So here as well we need to reuse the function toRevDigits to split the number into a digit seems to me. sumDigits :: [Integer] -> Integer sumDigits = sum . concatMap toRevDigits stack answer on concatMap Yes, the concatMap function is just concat and map put together. Hence the name. Putting functions together simply means composing them: (.) :: (b -> c) -> (a -> b) -> a -> c However concat and map cannot be put together by simply using function composition because of the type signature of map : map :: (a -> b) -> [a] -> [b] ^^^^^^ ^^^ ^^^ a b c [ ] - why is this understood as a, b, c , I do not really understand since this now seems to merely put visual blocks between arrows into another layer of variable letters? Let's see concat, map, and concatMap in action! map (+1) [1,2,3,4] > [2,3,4,5] Will this work on strings too? Compare how map still keeps the list while concatMap somehow has a list without the commas in between, without the elements separated. What is the length of \"some string\" ? map (++\"!\") [\"one\", \"two\", \"three\"] > [\"one!\", \"two!\", \"three!\"] concatMap (++\"! \") [\"one\", \"two\", \"three\"] > \"one! two! three!\" length [\"one!\", \"two!\", \"three!\"] > 3 length \"one! two! three!\" > 16 As you can see function composition expects a function of type a -> b , but map is of type a -> b -> c . To compose concat with map you need to use the .: operator instead: (.:) :: (c -> d) -> (a -> b -> c) -> a -> b -> d The concat function has a type signature of: concat :: [[a]] -> [a] ^^^^^ ^^^ c d Hence concat .: map is of type: concat .: map :: (a -> [b]) -> [a] -> [b] ^^^^^^^^^^ ^^^ ^^^ a b d Which is the same as that of concatMap : concatMap :: (a -> [b]) -> [a] -> [b] [ ] - the stack answer is very good, do check it out again because it includes also an interesting part where if you flip the arguments of concatMap you get the >>= (bind) function of the list monad! [ ] - on next coding session do an exploration of this part and redo the previous calculations again. How is monad connected to this? Exercise 4 define the function validate :: Integer -> Bool that indicates whether an Integer could be a valid credit card number. This will use all functions defined in the previous exercises. Example : validate 4012888888881881 = True Example : validate 4012888888881882 = False Note: now what bugs me is that I do not understand why is the first example True and the second example false? Immediatelly I thought about putting any number into the validate function that will extract the digits, reverse them and then concatenate and then.. but how does it actually know is some number is true if there is no action of comparing two credit card numbers? Confused :confused: Off to google lands.. and stack overflow fields.. codereview.stack on the same exercise Write functions in terms of other functions You have written both toDigits as well as toDigitsRev . However, you only need one of them. THe other one is the reverse d variant: toDigits :: Integer -> [Integer] toDigits = reverse . toDigitsRev Note: Why did we define teDigitsRev in the first place? The exercise does say \"double the value of every second digit beginning from the right. That is, the last digit is unchanged; the second-to-last digit is doubled; the third-to-last digit is unchanged; and so on.\" But why are we starting from the right in the first place? What difference does it make to begin reading the card number from the last digit since we actually read from left to right? Shouldn't these kind of statements be explained in some terms or is this going too much into details? So calleds details, since these kind of obscurations may just interfere with the cognitive process. I do not understand why are we beginning from left to right so now I must investigate further why is the exercise stated in these terms. Thus, I must go away from the code yet again, and explore. While stumbling through these exercises and code explanations online we see that many just go over these details, which may seem superficial of super obvious to some people, and yet there are some idiots ? like me who do wonder why are we suddenly reading the numbers from left to right. Also, often just basic operations, or syntactic operations are explained as in \"look, this is some forrest, you can do this and you can do that, now go into the forrest and find me this or that\" while another maybe much subtler explanation would be to take the student into the forrest and just tag along offering advice on each step, since there are many rabbit holes in which one can fall into or explore. Sometimes experienced Haskellers just go over these seemingly obvious steps while the rest of us stumble and even though we might just follow through what happen after a while is that not enough spaced repetition in learning was done so these details will melt away. Maybe then, when we reach higher planes of abstraction we will not be equiped with enough functional lingo , enough explored rabbit holes, to fully undertand all the varieties of solving specific problems. Ok, only in this exercise I realized basic operation on lists, taking out single elements, like taking the head or the tail , or taking out the init or the whats the other one.. can be compared with this extraction of the digits from a number. In this example there is abstraction and composing of small functions and at the same time, we can understand with a much richer context the operations on lists themselves which might seem boring if we are just told, \"hey this is how you get the first and the rest of the list\". This might seem like a newbie ranting, but what I am aiming at is at defining a much richer context in which simple abstractions contain a higher level view, or like the exercise itself contains two higher level contexts which might enable one to better understand what is happening and how it all connects. We can see in the stack answer on concatMap which is a wonderful example how a seemingly simple composing of concat and map can take us all the way to the Monad itself. While learning Haskell I feel I have to explore each of these rabbit holes and then write about it too, just merely linking each Haskellers exploration but at the same time deepening my own exploration. Obviously this could be automated in the future where an AI could just in a wiki-like manner explore all angles of an exercise and tailor the deepening trajectory for each individual student depending on the progress. This could be modified in real time, so a student might progress on its own from a simple concat function to a monad. And no book would be the same since it would define itself by the student that interacts with it. Kinda like a smartbook . Notice the finishing touch on lexi-lambda's post on this exercise: Done. Well, that wasn't terribly interesting, though it was helpful to get used to working in the language again. I think I'll go through the next assignment, and if it doesn't get better, I might have to switch to something else. For now, though, I feel somewhat accomplished, despite having written an extremely trivial set of functions. I am grateful for lexi's post, it helped and motivated this pursuit and I haven't felt progressing in Haskell like this since I started, at least not in practical terms. Everything up to this point has been exploration and now this exploration has somehow interlinked into this one exercise. But the bitter taste remains because I wish other Haskell books would look like simple from super abstract exploration within a single space, a single assignment that can transform itself into a whole field of language.. well I can't really put this into words for now, but I feel nice about the progression :) To get back at our exercise, lexi wrote mysterious luhn instead of validate . What is luhn ? We are grateful for lexi's boon and we follow the luhn into the Haskell forrest of doom! :) search google.com -> luhn gives us Luhn algorith Luhn algorithm The Luhn algorithm or Luhn formula , also known as the \"modulus 10\" or \"mod 10\" algorithm, named after its creator, IBM scientist Hans Peter Luhn, is a simple checksum formula used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the United States, ... and some other.. notice: It is not inteded to be a cyptographically secure hash function; it was designed to protect against accidental errors, not malicious attacks. Note: Here we come to the \"Why do be begin reading our card numbers from the right\" question from before. The formula verifies a number against its included check digit, which is usually appended to a partial account number to generate the full account number. This number must pass the following test: Note: And here is the 'right to left' bit: This is similar to the exercise description. Would be cool if the exercise had this article linked too.. From the rightmost digit(excluding the check digit) and moving left, double the value of every second digit. The check digit is neither doubled nor included in this calculation; the first digit doubled is the digit located immediately left of the check digit. Note: So we see that the digits not included in the calculation are called 'the check digit'. These are the digits we will not double. If the result of this doubling operation is greater than 9 (e.g., 8 x 2 = 16), then add the digits of the result (e.g., 16: 1 + 6 = 7, 18: 1 + 8 = 9) or, alternatively, the same final result can be found by subtracting 9 from that result (e.g., 16: 16 - 9 = 7, 18: 18 - 9 = 9). Note: Ok so we see another way how to get digits from doubledigit numbers, this was not mentioned before. Take the sum of all the digits. If the total modulo 10 is equal to 0 (if the total ends in zero) then the nmuber is valid according to the Luhn formula; otherwise it is not valid. Note: There are more complex algos such as Verhoeff and Damm algorithma and Luhn mod N algorithm is an extension that supports non-numerical strings. The algorithm appeared in a United States Patent for a hand-held, mechanical device for computing the checksum. Therefore, it was required to be rather simple. Note: I still need to wrap my head around this algo, it seems simple but the underlying logic is not super clear.. but we see that this exercise comes in Programming in Haskell from Graham Hutton. That's great! Check this stack overflow on implementing the luhn in Haskell luhn :: Int -> Int -> Int -> Int -> Bool luhn w x y z = (luhnDouble w + x + luhnDouble y + z) `mod` 10 == 0 Check this stack overflow on luhn doubleAndSum :: [Int] -> Int doubleAndSum = fst . foldr (\\i (acc, even) -> (acc + nextStep even i, not even)) (0,False) where nextStep even i | even = (uncurry (+) . (`divMod` 10) . (*2)) i | otherwise = i myLuhn :: Int -> Bool myLuhn = (0 ==) . (`mod` 10) . doubleAndSum . (map (read . (: \"\"))) .show testCC :: [Bool] testCC = map myLuhn [.... , .... , .... , ...] -- => [True, False, False, True] Note: Well this version is too verbose for me, currently beyond 'alien speak'. I am aware of the names of these functions but do not really understand how they relate to each other. But writing Haskell code out like reading a difficult piece of music seems like a good practice too. There is also Luhn on Hackage, Haskell's repository of programs and libraries, the source is commented and seems nice to follow. Check it out, it is defined as a module and has test's as well. This one seems like the most comprehensible solution so far.","title":"Observations"},{"location":"observations/#observations","text":"Explaining Haskell datatypes and constructors to a buddhist/yogi by using the chakra system and seed sounds of each chakra analogy: soundToChakra :: Sound -> Chakra soundToChakra = \\x -> case x of Lam -> Muladhara Vam -> Svadishtana Ram -> Manipura Yam -> Anahata Ham -> Vishuddha Aum -> Ajna stringToSound :: String -> Maybe Sound stringToSound = \\x -> case x of \"lam\" -> Just Lam \"vam\" -> Just Vam \"ram\" -> Just Ram \"yam\" -> Just Yam \"ham\" -> Just Ham \"aum\" -> Just Aum _ -> Nothing What is interesting is that in Haskell you don't say, \"well, maybe this or maybe that\" like I hear often people say in real life, and true, it does seem ambiguous to use maybe for more possibilities than one or nothing at all. You instead say \"Well, maybe just this or nothing at all\" and if you want to express more outputs, or results that involve something more than just nothing you use \"Well, either this or that\". But even more, the Maybe is actually made of \"Just this or nothing at all\" so there is always this layered approach to composition of functions, of expressions. The Maybe itself is defined by Just this or Nothing at all. Why? Well, It seems it is because we can call it Maybe precisely because it somehow produces an action and a terminating non-action ; we can also visualise it as a parent node whose name is Maybe with two children, named Just and Nothing. This intuitive tricky part is that Nothing is not like Just, Just will eventually pass something while Nothing will just say no to what ever is being passed on, simply declaring Nothing if or Maybe gets something which it doesn't need, which is not defined explicitly. In Haskell it seems each expression can have a name itself, so that it is possible to connect it to anything else, while at the same time it is defined totally or at least that is what we strive to do. In real life we sometimes use a name for an expression and then offer an explanation of what we really mean. We even might get irritated if we are asked to explain our Maybe's . Alice: \"Are you gonna do it?\" Bob: \"Maybe\" Alice: \"What do you mean maybe?\" Bob: \"Well I mean that I am going to just do it or I am going to Just sit back and do Nothing . Alice: \"But Bob please if you can't do it then do the other thing please, you can't Just sit back and do Nothing . Bob: \"Ok then, I am going to do Either this thing or that thing if you need it so much, is that ok? Just give me some time. Yet people often use the word maybe for more than one thing. You could maybe do this, or you could maybe do that, or bla.. which does seem a bit unprecise . fromEither :: Either Sound Chakra -> ThisOrThat fromEither = \\x -> case x of Left s -> This s Right c -> That c toEither :: ThisOrThat -> Either Sound Chakra toEither = \\x -> case x of This s -> Left s That c -> Right c Right Chakra == toEither (fromEither (Right Chakra)) That Chakra == fromEither (toEither (That Chakra)) So That is on our Right and This is on our Left . That contains all the 6 chakras we have in our body and This contains all the sounds we utter that work on each chakra. The intuitive distinction that might seem hard to grasp is that we seem to have another layer when we visualise the either containers, or better to say when we visualise the path from chakra container to sound container. We cannot just visualise sound and chakra flowing between but instead we must so to say go up the tree root using two levels of Either to understand the whole structure, this needs to be done in order for us to prove they all relate, that they are all isomorphic to one another. This That 6 sounds 6 chakras \\ fromEither / \\ / | \\ / \\ / | \\ / \\ / | \\ / Left + Right \\ / \\ / \\ / * | | | toEither Right Chakra == toEither (fromEither (Right Chakra)) That Chakra == fromEither(toEither (That Chakra)) Left Sound == toEither (fromEither (Left Sound)) This Sound == fromEither (toEither (This Sound)) Either sound chakra is making a statement saying that we have one of two possible values of types x or y in it. It turns out that Either , or sum types more generally, have a very reasonable and useful dual construction in which we have both x and y . In Haskell, we can convey thia idea as well. Let's call it Pair . data Pair x y = MakePair x y That is, a pair of a Chakra and a Sound will have type Pair Chakra Sound . So while with Maybe we had to just pick one or nothing at all, and in Either we could pick this or that, which were some real alternatives, in a Pair we have this notion of carrying both alternatives with us, they are together and we are not making a choice by choosing either this or that. It does seem like Maybe relates somehow to the notion of Identity since it is just giving us the thing we ask while the Either resembles to a Boolean choice or either this or that, either though we could say Maybe as well resembles a boolean choice since Just could be understood as true and Nothing as false . It is interesting to find relations with basic mathematical functions. In that case Either could be understood as showing two truth values and yet we are only supposed to pick one, now that really resembles to OR ! But bool values always pick just one so even though this thought process smells wrong it does bring benefit when learning Haskell to compare it with previous concepts. Quote from atop: We say Either and Pair are fundamental because once we can group or tell apart two things, we can group or tell apart as many things as we want. Once we begin to extend our space of pairs we realize pairs are just like containers of two. Can we imagine triples? Or quads? If we jump to a musical analogy, a pair is a musical interval, a pair of two tones in any scale makes an interval. Once we hear a tone it is like hearing a point in space, though it can sound pretty we know very little about it since the whole sound is coming from one source (aha but what when both sources sing the same tone? Where is the source then?). To get back a pair of two tones is like a line, and a triplet of three tones can be understood as a triangle, a musical chord, defining a space between three tones. Technically speaking a musical chord is a cluster of three or more tones. Let us add three tones, c, e, and g that will make a C chord add3tones :: (Tone, Tone, Tone) -> Tone add3tones = \\(c, e, g) -> c + e + g","title":"Observations"},{"location":"observations/#let-vs-where","text":"It seems the difference between let and where lies in the order of declaration. When we want to first name the things we want to use in our action we use let in the sense, let there be light, let there be this, let there be that and then after we have declared all out lets we will begin an action like \"ok now create this\". We could understand it like this as well: let there be a human called \"Bob\" let there be a human called \"Alice\" in \"Alice\" loves \"Bob\" -- further let human = \"Bob\" let human = \"Alice\" \"Bob\" loves \"Alice\" -- further we ommit using let twice let firstHuman = \"Bob\" secondHuman = \"Alice\" in \"Bob\" loves \"Alice\" {- This was not valid Haskell code, but it is written like this in order to increase conceptual understanding of let vs where expressions -} -- valid Haskell let x = 2 y = 3 in x + y Now, when we use where we like to first define our action or an event happening and then tell something about our actors who define the event itself. Something like \"where some event is happening, its components are this and that, or where Alice loves Bob, Bob is human and Alice is human too.\" If we miss describing a single component of our event the event will not compile, Haskell will not accept our declaration. Notice we could define a component and not use it in our action, but it will still go through, and the event will not fail, same as declaring \"Alice\", \"Bob\" and \"Hayka\" and then telling \"Alice\" loves \"Hayka\", the program would run even though \"Bob\" was left all by himself. The above let expression can be written with where like: love = human1 + human3 where human1 = \"Bob\" human2 = \"Alice\" human3 = \"Hayka\" Let's use correct Haskell code for our love program. We will use '++' instead of just '+' since '++' is used to add or concatenate words, aka strings (of characters). love = human2 ++ \" loves \" ++ human3 where human1 = \"Bob\" human2 = \"Alice\" human3 = \"Hayka\" -- and our previous number example with let add = x + y where x = 2 y = 3","title":"Let vs Where"},{"location":"observations/#more-of-where","text":"Let us use the where in a slightly developed example, this is a variation on \"Haskell from first principles\" chapter 3 exercise, but we will use first and last name instead of hello world. The difference is in two examples is that we will abstract our name into first and last while in the book \"hello\" and \"world\" is used for expression and for the function name as well. module MyName where {- before we define our name, I am just using my own here, we say that myName is a type of a string, as in a string of characters, then we define myName by concatenating the first and the last name. We are not explicitly mentioning first and last name, but instead just providing the value of the first and last name, which is \"domagoj\" and \"miskovic\" -} myName :: String myName = \"domagoj\" ++ \" miskovic\" -- but now we will declare the firstName is \"domagoj\" firstName :: String firstName = \"domagoj\" -- and last name as \"miskovic\" lastName :: String lastName = \"miskovic\" {- and now we will print out the myName and then we will link together our firstName and lastName into one expression by calling our firstName and lastName and Haskell should print out the values we provided before when we run the program. -} main :: IO () main = do putStrLn myName putStrLn firstAndLast where firstAndLast = concat [firstName, \" \", lastName] We introduced firstAndLast function with a where which was not maybe necessary. We could have just linked together before declared firstName and lastName without the where by writing putStrLn (concat [firstName, \" \", lastName]) but in Haskell, even from simplest examples, one is practicing composability, abstracting from the tiniest to the grossest. Since we learned where which is like a basic tool for abstracting, describing our processes, like separating the what and the who, we should use it in our exploratory learning and try to apply it when ever we can. Here is a simple sum of squares example with where: -- sumOfSquares.hs sumOfSquares x y = square x + square y where square n = n * n Let us use lambda for our Sum of Squares module SOS where sumOfSquares :: Integer -> Integer -> Integer sumOfSquares = \\x y -> square x + square y where square = \\n -> n * n Here also, we could have just used sumOfsquares x y = x*x + y*y instead. But there is something else too, there are too many squares, square this square that, though as a learning aid repetition is very good. Little children like to repeat things they learn, so do we. -------------------------------------------------------;","title":"More of where"},{"location":"observations/#scope","text":"Note: Haskell from first principles builds intuition by first going through various syntax examples and then plays with local and top level definitions by using let and where as basic tools for abstraction. What we notice is the importance of spacing in Haskell code, from whitespace that has a silent like mysterious apply function because a simple f x meaning f is applied to x , to the silent matrix like grid system or invisible columns and rows define the play between local and global or top-level definitions. Global might be a wrong word so we use top-level which brings us somewhat closer to the code at hand, and a bit away from all expansive global notions. a top-level definition is basically like a tree and local definitions are like branches which have a life of their own within that same tree. /----- + ===[ ------ | \\----- | | | /----- + ==={------ \\_____ -----------------------------------------------------; Sometimes in real life conversations you will listen to two people talking, and one of them will ask a question to another but you will not really know what they mean by it. The second person might already answer it and you will still ponder what the first person really meant. Then later when the second person leaves you ask the person who was answering: \"Hey what did she mean with that question? I did not understand it.\" \"Oh, you know sometimes she talks like this, she knows I know what she really means so she just cuts out the whole question and asks me implicitly.\" What basically happened is that I was left out of the scope of the conversation so I could not put together what they meant by it. They were talking about local definitions without providing explicit values to those definitions so I was left out wondering what those values were. Unfortunately sometimes people get angry too when you do not ask them explicitly something but force you to provide explicit local definitions, remebering something they wish to tell . People use these aggresive tactics sometimes. Somebody might tell you: \"Hey, when are you going to cut your hair!?\" What they mean by it it that your hair is too long but they do not explicitly say it out loud. You might play naive and ask them, well I do not know, why do you ask? Then the other person expects you to know what they really mean and that is that your hair is too long and that you should cut it. Unfortunately these situations sometimes happen in various shapes and sizes. ---------------------------------------------------;","title":"Scope"},{"location":"observations/#to-string-or-not-to-string","text":"Let's go through some of the exercises from chapter 3 in HaskellBook and try out basic abstractions. We would like to append or concatenate two separate strings into one string. Our first string will be \"Curry is awesome\" and our second string will be an exclamation mark '!'. Our '!' exclamation mark is actually a character, and characters are embraced by single quotes ' ' while actual strings which are a collection of characters are embraced by double quotes \" \" so a letter a is written as 'a' while a string abcd is written as \"abcd\" . Now, can we write a character a as a string by putting double quotes around it, as in \"a\" . Yes we can. But how is that possible? > \"a\" == 'a' ..error: Couldn't match expected type '[Char]' with actual type 'Char' > \"a\" == ['a'] True > [' ', 'a', ' '] \" a \" > \" a \" == \"a\" False Hmm.. what is actually happening here? Let us try appending two chars, a and b and at the same time check with == sign if our expression is true > appendTwoChars = \"a\" ++ \"b\" == \"ab\" > appendTwoChars True -- by trial and error we see that > appendTwoChars = 'a' : 'b' : [] == \"ab\" > appendTwoChars True Looking at this we could say that a string is a list of characters, represented by square brackets [ ] while characters have only single quotes meaning double quotes are like single quotes embraced by a bracked. > ['a'] == \"a\" True Ok, so back again to our \"curry is awesome\" and \"!\" example. Let's concatonate them and abstract these examples into a function that will concatenate any two strings. > concatenateTwoStrings = (\"Curry is awesome\" ++ \"!\") == \"Curry is awesome!\" > concatenateTwoStrings True > concatenateTwoStrings firstString secondString = firstString ++ secondString > concatenateTwoStrings \"first\" \" second\" \"first second\" -- using beloved lambda > concatenateTwoStrings = \\firstString secondString -> firstString ++ secondString > concatenateTwoStrings \"hello\" \" world\" \"hello world\"","title":"To String or not to String"},{"location":"observations/#types","text":"Though usually first basic dataype that is introduced in Haskell books is the Bool datatype we will start with something different, which might bring our intuition closer to Bool. It is important to note that while HaskellBook begins the whole type story with introducing data Bool = False | True , same as Graham Hutton's Programming Haskell , A Type of Programming introduces types as four seasons of the year. data Bool = False | True data Season = Winter | Spring | Summer | Fall We too will define a Human datatype with its two data constructors Male or Female data Human = Male | Female -- let's change some genders, for now just keeping it as a binary choice for -- making things simple changeGender :: Human -> Human changeGender Male = Female changeGender _ = Male > changeGender Male Female > changeGender Female Male We are using or instead of and because the | symbol, the so called Pipe, indicating logical disjunction \"or\", indicates that this is a sum type . So a human can be a female or a male but not both at the same time. But is this the only way we can write data declarations? How about using and and various what if this and that happens relations. Types are never so pure in real life, and we often need to ponder before we implement a solution. Seems to me a Haskeller can spend an eternity just pondering basic questions. \"A type!\" a haskeller says, \"What the hell is a type?\" It is important not to over think the type relations but somehow mix and match while progressing in our composition. The good part is that type driven design can eliminate many failures that can happen along the way, and in turn enable us to deal with future turns with much greater success. HaskellBook tells this in a very clear way by describing the basic flow of a data type declaration. The whole thing is called a data declaration. Data declaration do not always follow precisely the same pattern - there are datatypes that use logical conjuction ( and ) instead of disjunction, and some type constructors and data constructors may have arguments. The thing they have in common is the keyword data followed by the type constructor (or name of the type that will appear in type signatures), the equals sign to denote a definition, and then data constructors (or names of values that inhabit your term-level code). data Mood = Happy | Sad changeMood :: Mood -> Mood changeMood Happy = Sad changeMood _ = Happy > changeMood Happy Sad > changeMood Sad Happy","title":"Types"},{"location":"observations/#five-types-of-buddhas","text":"Playing with sum types and five Buddha families data Buddha = Vairocana | Amoghasiddhi | Amitabha | Ratnasambhava | Akshobhya data Color = White | Green | Red | Gold | Blue data Element = Space | Air | Fire | Earth | Water data Cardinality = Center | North | West | South | East data Stress = Ignorance | Jealousy | Selfishness | Pride | Aggresion data Season = None | Summer | Spring | Autumn | Winter data Wisdom = Meditation | Perfection | Observation | Equanimity | Reflection data Symbol = Wheel | Vajra | Lotus | Jewel | Sceptre data Means = Turn | Protect | Magnetize | Enrich | Pacify colorOfBuddhas :: Buddha -> Color colorOfBuddhas = \\x -> case x of Vairocana -> White Amoghasiddhi -> Green Amitabha -> Red Ratnasambhava -> Gold Akshobhya -> Blue seasonOfBuddhas :: Buddha -> Season seasonOfBuddhas = \\x -> case x of Vairocana -> None Amoghasiddhi -> Summer Amitabha -> Spring Ratnasambhava -> Autumn Akshobhya -> Winter wisdomOfBuddhas :: Buddha -> Wisdom wisdomOfBuddhas = \\x -> case x of Vairocana -> Meditation Amoghasiddhi -> Perfection Amitabha -> Observation Ratnasambhava -> Equanimity Akshobhya -> Reflection This looks clear and yet we feel something missing. We would like to now define more complex constructors, and we will get back to this when we understand a bit more about Either and Maybe which will enrich our flow of types. I showed this chart to a person who knows something about these Buddhas and yoga and intuitively the person understood the flow of types. There five families are also a nice way to put various groups of five together and think on various functions between them. --------------------------------------------------;","title":"Five Types of Buddhas"},{"location":"observations/#addition-curry","text":"addOne :: Natural -> Natural addOne = \\x -> x + 1 addTwo :: Natural -> Natural -> Natural addTwo = \\x -> addOne (addOne x) addTwo ----------------------------------- / / /\\ / / / \\ / addOne / addOne / ----------------- ----------------- | \\x ------ || \\x ------ | | | | || | | | x ----------->| + |----------->| + |-----------> | 1--->| | || 1--->| | | | ------ || ------ | | || | ----------------- ----------------- Our addOne can be also understood an an incrementor function because it increases or increments the number we give it to. So if we give a number 0 to the incrementor function it gives us back the number 1, then if we give it the number 1 it gives us back the number 2; we can give it any number and it will always give back one number back. You could also say it sends the number it recieves to the next one. It is a very simple function. What does simple mean? Maybe better to use the word primitive or small in the sense that it is easy to understand conceptually. Simple is a complex word because people have different notions on what is simple and what is complex. The opposite of our increment function would be a function that would send the number we provide to the previous number, so if we gave it a number 3 it would give us the number 2. It decreases the number we give it to, so we will call it the decrement function. So within our natural number realm our increment function behaves the same as our addOne function because it adds a number 1, and our decrement function behaves the same as the function subtractOne which subtracts a number 1 from the one we give it to. addOne :: Natural -> Natural addOne = \\x -> x + 1 subtractOne :: Natural -> Natural subtractOne = \\x -> x - 1 -- rewritten as increment and decrement would be incr :: Natural -> Natural increment = \\x -> x + 1 decr :: Natural -> Natural decrement = \\x -> x - 1 -- hmm can we make these functions to add any number we want? incAny n = \\x -> x + n decAny n = \\x -> x - n -- Maybe you haven't applied a function to enough arguments? incAny x n = \\x -> x + n decAny x n = \\x -> x - n -- Maybe you haven't applied a function to enough arguments? incAny x n = \\x n -> x + n decAny x n = \\x n -> x - n -- Maybe you haven't applied a function to enough arguments? incAny = \\x n -> x + n decAny = \\x n -> x - n > incAny 3 4 7 > decAny 3 4 -1 Addition is a function that maps two natural numbers to another one. Let's see different ways we could add two numbers using our incrementor and decrementor function addition = \\x y -> (increment x) + (decrement y) > addition 3 4 > (increment 3) + (decrement 4) > ((\\x -> x + 1) 3) + ((\\x -> x - 1) 4) > ([x=3] 3 + 1) + ([x=4] 4 - 1) > (3 + 1) + (4 - 1) > 4 + 3 > 7 addition = \\x y -> 1 + ((decrement x) + y) addition 3 4 1 + ((decrement 3) + 4 1 + ((\\x -> x - 1) 3) + 4 1 + ([x=3] 3 - 1) + 4 1 + (3 - 1) + 4 1 + 2 + 4 1 + 6 7 > :type addition addition :: Integer -> Integer -> Integer Now let's expand our calculation with more steps. addition = \\x y -> (increment x) + (decrement y) addition 3 4 = (\\x y -> (increment x) + (decrement y)) 3 4 = (increment 4) + (decrement 3) = (increment 5) + (decrement 2) = (increment 6) + (decrement 1) = 7 + 0 = 7 addition = \\x y -> 1 + ((decrement x) + y) addition 3 4 = (\\x y -> 1 + ((decrement x) + y)) 3 4 = 1 + ((decrement 3) + 4) = 1 + (1 + (decrement 2) + 4) = 1 + (1 + (1 + (decrement 1) + 4)) = 1 + (1 + (1 + 0 + 4)) = 1 + (1 + (1 + 4)) = 1 + (1 + 5) = 1 + 6 = 7 I still can't explain how but I find the second version so pretty, it remembers its steps, as in takes much more care when calculating, it just seems more intelligent way to add things, even though it requires more steps. I always would find it strange when people talk about something taking less time as being better. I could see my mind stopping and pondering why would that be true, why would optimizing have this guideline which seems so narrow. Yes, there are valid explanations and reasons for this but still, the notion of embedding more information within each level of your computation somehow seems much more intelligent approach in the long run. Who knows maybe artificial intelligence is precisely one long extensive computation where each step is carrying a billion other steps all in sync so why not explore that direction instead. Well, for more input on these increment and decrement function check the amazing SICP lectures by Abelson and Sussman. These are just pure awesomeness, I have yet to write about them and how they helped in motivating this functional pursuit into haskell. Who knows, maybe Lisp lurks somewhere in Haskell heart too. Addition is associative ((x + y) + z = x + (y + z) meaning if three people x y and z want to go out of the house, no matter who comes out first by the time all three come out of the house there will be three of them out of the house. Also notice if you haven't actually witnessed these people coming out of the house how can you tell who came out first and what actual difference does it make? If we the three of us are going to a party and all three of us come out of the house to take the bus, there is no difference who comes out first or who goes in the bus first since all three of us are going to the party anyway. Well maybe it does matter for some mysterious spooky action at a distance reason, but our coming out of the house is an associative operation. Also when we have to pay for the ticket 10 dollars, it makes no difference if we give the driver first a 5 dollar note and then a 2 dollar notes and then a 1 dollar note or if we give the driver the money in some other order. We still have to pay 10 dollars. But these are simple examples. I'm thinking on what would be an associative operation in music, as in if I have a chord of three notes does the order matter when I play them? The similar idea to our three little friends would be if I had to play the three notes at the same time, then there would be no difference, but still, then I cannot see the whole process of two separate groupings of three notes. Still, you could visualise giving more awareness to some two notes within a three note chord and they will then sound a bit different even though you will play them at the same time. This is one of the key features of great musicians that even when they play three notes at the same time, they never really play them in the same way, they often do subtle movements that somehow increment your perception of music. These movements are so subtle they could be barely described as movements and still, if you are attentive you can hear them. ----------------------------------------------------; In Mathematics, the associative property is a property of some binary operations. Within an expression containing two or more occurrences in a row of the same associative operator, the order in which the operations are performed does not matter as long as the sequence of the operands is not changed. Associativity is not the same as commutativity, which addresses whether or not the order of two operands changes the result. Seems to me my whole previous rambling was describing commutativity and not associativity ! But they do seem awfully similar: x + (y + z) = (x + y) + z x + (y + z) = (x + y) + z 1 + (2 + 3) = (1 + 2) + 3 1 + (1 + 1) = (1 + 1) + 1 1 + 5 = 3 + 3 1 + 2 = 2 + 1 6 = 6 Still what seems clear from this is we can shift things around, we can commute them around and we get the same result while with associative property we can group things differently but the operations we apply to them will not change the result. Why does this require so much pondering? There is this smell that associative properties and commutative properties can change and often seem intermixed depending on the operations we use with them. All functions in Haskell take one argument and return one result. This is because functions in Haskell are nested like Matryoshka (Babushka) dolls in order to accept \"multiple\" arguments. The way the (->) type constructor for functions works means a -> a -> a represents successive function applications, each taking one argument and returning one result. The difference is that the function at the outermost layer is returning another function that accepts the next argument. This is called currying . Our addTwo was defined for just two input numbers. What if we provide three numebrs to add? > 1 + 2 3 > 1 + 2 + 3 6 > (+) 1 2 3 > (+) 1 2 3 error:Non type-variable argument in the constraint.. -- hmm.. ;; this is written in lisp, a functional language but here plus ;; is applied to every argument when written in prefix notation. > (+ 1 2) 3 (+ 1 2 3) 6 (+ 1 2 3 4 5) 15 Let's get back to Haskell. Though it is possible to put plus before the arguments similar to lisp it does not really work the same way, it still somehow takes only two arguments. Is this limited? Not really, what I understand now is that the plus function itself is defined minimally in Haskell, like the very essence of plus is defined as something that adds at least two things, but to go even further in Haskell, only two things! If you need to add three things then use two pluses, use two functions to add three things. If we wanted to apply plus to more than two arguments without defining how many, it seems we would need a function that would map somehow this plus to any number of numbers we want. This seems like we actually need two functions to get a similar behaviour like in lisp example. This does seem super cool, meaning functions in Haskell are interlinked somehow each having one input and one output. Even our imaginary mapping function would not really add a million numbers but would tell plus to add a million numbers, orchestrating the event coordinating many little pluses to do the addition, collapsing the numbers like dominoes. So how is lisp then just adding many numbers by having a plus function at the beginning of the expression? Ponder the pond.. The way the type constructor for functions, (->), is defined makes currying the default in Haskell. This is because it is an infix operator and right associative. Because it associates to the right, types are implicitly parenthesized like so: f :: a -> a -> a -- associates to f :: a -> (a -> a) -- and map :: (a -> b) -> [a] -> [b] -- associates into map :: (a -> b) -> ([a] -> [b]) The association here, or grouping into parentheses, is not to control precedence or order of evaluation; it only serves to group the parameters into argmuents and results, since there can only be one argmuent and one result per arrow... But this very technical desription of this action does not fully explain this behaviour except by mapping out every movement into words. What I do realize from previous pondering is that as with our plus sign visually it is supposed to be right associative merely because the result is the last thing we are going to get if the function completes so the result is embraced with the last argument meaning the last function is going to take the numbers given to it by all the previous functions and give us the result. At least this idea helped me to deepen the understanding of this movement. Notice also the explanation says that This is because it is an infix operator and right associative.. Now the expression is saying that something happens because something is like this or that. Such definitions only provoke further questions but why is is like that then? Next it continues with again just repeating the previous expression with Because it associates to the right.. And again I know and see it associates to the right but why is the word because used to explain something by just repeating what was already said? And that is why it is parenthesized like that. But I already see that myself, what idea is actually expressed here? I am sure many might stop and ponder what this really means. By thinking on functions that only pass on what was given and at the same time do just a small calculation that in whole creates the final result we can see that it might be obvious that it will be parenthesized to the right. Again it is difficult to put this thinking flow into words since words are so vague and well, code is really explicit. Maybe that is why I am pondering so much on this plus sign, associativity and commutativity. > map (+) [1,2,3,4] error > map (1+) [1,2,3,4,5] [2,3,4,5,6] -- hmm","title":"Addition Curry"},{"location":"observations/#mappings","text":"Consider the expression 7 - 4 + 2 . The result could be either (7 - 4) + 2 = 5 or 7 - (4 + 2) = 1 . The former result corresponds to the case when + and - are left-associative, the latter to when + and - are right-associative. When I type this expression into ghci or an android calculator I get 5 as the result. A mapping function applies a given function to each element of a list or other collemtion. (mapcar 'car '((a 1) (b 2) (c 3))) => (a b c) (mapcar 'cdr '((a 1) (b 2) (c 3))) => (1 2 3) (mapcar 'string \"abc\") => (\"a\" \"b\" \"c\") How is this mappping mapcar defined in emacs lisp? (defun mapcar (function &rest args) \"Apply FUNCTION to successive cars(heads) of all ARGS. Return the list of results.\" ;; If no list is exhausted, (if (not (memq nil args)) ;; apply function to cars. (cons (apply function (mapcar 'car args)) (apply 'mapcar function ;; recurse for rest of elements (mapcar 'cdr args))))) (mapcar 'cons '(a b c) '(1 2 3 4)) => ((a . 1) (b . 2) (c . 3)) Compare the elisp mapcar with the Haskell definition of map map :: (a -> b) -> [a] -> [b] map _ [] = [] map f (x:xs) = f x : map f xs We notice that from line f x : map f x the f x looks like (mapcar 'car args)) and f xs looks like (mapcar 'cdr args). car in lisp lingo is actually same as haskell's first and cdr is same as rest , meaning x and xs are written in the same style, like saying after the first beginning come many more beginningS . Actually after the first comes always the rest, no matter how many or in which way they appear. There wouldn't be any firsts if there were no rests, right? In Haskell they like to use variable names like n and ns for number and numbers and some more polymorphic like variables for x and xs Notice also that cons from elisp definition is the cons operator which is represented in haskell as infix operator : stuck between f x and map f xs . Also elisp mapcar definition first says (if (not (memq nix args)) which relates to haskell's map _ [] = [] . Haskell seems to be just saying if anything, meaning _ as an empty placeholder which is used in pattern matching, is provided to an empty map _ [] then return an empty list [] Why? Because we have nothing to do with an empty list. We could define an amazing function but if we do not provide any arguments to it nothing will happen, because an empty list has no members, it is just empty. And out of an empty list input how would we create a filled list of an output? This imaginary function would have to be some sort of an artist that would create something out of nothing, worth pondering about.. map f (x:xs) = f x : map f xs map (+3) [1, 2, 3, 4, 5] [(1 + 3), map (+3) [2, 3, 4, 5]] [(1 + 3), (2 + 3), map (+3) [3, 4, 5]] [(1 + 3), (2 + 3), (3 + 3), map (+3) [4, 5]] [(1 + 3), (2 + 3), (3 + 3), (4 + 3), map (+3) 5] [(1 + 3), (2 + 3), (3 + 3), (4 + 3), (5 + 3)] [(1 + 3), (2 + 3), (3 + 3), (4 + 3), 8] [(1 + 3), (2 + 3), (3 + 3), 7, 8] [(1 + 3), (2 + 3), 6, 7, 8] [(1 + 3), 5, 6, 7, 8] [4, 5, 6, 7, 8] Let's see how map is defined in another functional lisp dialect, Clojure Returns a lazy sequence constisting of the result of applying f to the set of first items of each coll, followed by applying f to the set of second items of each coll, until any one of the colls is exhausted. Any remaining items in other colls are ignored. Function f should accept number-of-colls arguments. Returns a transducer when no collection is provided. (map inc [1 2 3 4 5]) ;;=> (2 3 4 5 6) (map + [1 2 3] [4 5 6]) ;;=> (5 6 7) From Brave Clojure I think of abstractions as named collections of operations. If you can perform all of an abstraction's operation on an object, then that object is an instance of the abstraction. I think this way even outside of programming. This seems like a nice thinking process for building something like abstractions but I feel this notion of an abstraction being a named collection of operations should also have a unique property, the result which transformed the collection itself. This abstraction principle does not have to go both ways just by unfolding the operation which was applied to the collection. In some sense, you could abstract the mapping of pluses to a completely different operation that would in turn do the addition as a side effect while just mapping pluses to an arbitrary number of operands does seem like a trivial form of abstraction principle. The prejudice and respect for abstract thinking are so great that sensitive nostrils will begin to smell some satire or irony at this point Seems to me Hegel would be a wonderful Haskeller, this short description somehow relates in a nice way to our mapping definitions, just think of the difference between a total and a partial function: For Hegel, only the whole is true. every stage or phase or moment is partial, and therefore partially untrue. Hegel's grand idea is \" totality \" which preserves within it each of the ideas or stages that it has overcome or subsumed. Overcoming or subsuming is a developmental process made up of \"moments\" (stages or phases). The totality is the product of that process which preserves all of its \"moments\" as elements in a structure, rather than as stages or phases. -------------------------------------------;","title":"Mappings"},{"location":"observations/#viewing-type-signatures","text":"In chapter five exercises page 221 of HaskellBook it is asked to figure out how the type would change and why, and make a note of what we think of the new inferred type would be. -- Type signature of general function (++) :: [a] -> [a] -> [a] -- How might that change when we apply -- it to the following value? myConcat x = x ++ \" yo\" We will get myCOncat :: [Char] -> [Char] but what is interesting that somehow the type did not seem to change but only the frame of reference from which the signature is being observed, as in first we saw the type signature by unfolding the (++) which said that it concatenates two things to get another thing , and then we see the myConcat function which is made out of concatenating a thing with some other thing called \" yo\". The first frame is seeing just the (++) and how it interlinks [a] with [a] into an [a] while the second frame is seeing the same (++) but this time we are witnessing a type of a function which is using the (++) to interlink two [Char] s. That's why we have only two [Char] and not three [a] s like in the pure (++) signature. Did the type really change as the exercise says it does? I find this actually confusing because the viewpoint seems to change while the signature stays the same. Let's try with the next one. (*) :: Num a => a -> a -> a myMult x = (x / 3) * 5 :t myMult myMult :: Fractional a => a -> a And here we witness that the myMult has a signature which seems to be an instance of the more general Num a typeclass. Again we see the conversion of the view, in the first type signature the (*) is being seen as an operator that interlinks from one to the second which gives the third while our myMult function is some fractional instance or a result that is being interlinked from two variables a and a . We can intuitively understand that a fractional number is an instance of a larger family of numbers. After all we do say a fractional number and Haskell's type system works in the same way, type classes are like large families of instances of minor type families which are often automatically inferred by Haskell, inferred meaning Haskell can sign our functions with appropriate types. But it cannot do that for all. If we use our Buddha example from before Haskell would not know anything about the Buddha typeclass and all the instances of deities, buddhas, sages, etc that belong to the Buddha typeclass. In the next example we can see this changing view even better. (>) :: Ord a => a - > a -> Bool myCom x = x > (length [1..10]) :t myCom myCom :: Int -> Bool Here it seems that the myCom signature is telling us just the beginning and the end of our function which is defined as some operation that takes an Int and produces a Bool while the (>) signature has nothing at all to do with this function because it is defined just by itself, how it an instance of some Order typeclass apparently having something to do with ordering things and that it will take something and compare it with something else, finally producing a Bool which will tell us True or False , meaning if the first thing we provide it is trully greater than the second thing. myCom on the other hand is just saying: \"Hey I'm some function that is going to tell you if your number is true or not\". Seems to me we cannot really see what is happening from this type signature myCom :: Int -> Bool , what is this type actually doing?","title":"Viewing type signatures"},{"location":"observations/#credit-card-digits","text":"CIS 194: Homework 1 When solving the homework, strive to create not just code that works, but code that is stylish and concise. Try to write small functions which perform just a single task, and then combine those smaller pieces to create more complex functions. Don't repeat yourself: write one function for each logical task, and reuse functions as necessary.","title":"Credit Card Digits"},{"location":"observations/#validating-credit-card-numbers","text":"How do websites validate your credit card number when you shop online? They don't check a massive database of numbers, and they don't use magic. In fact, most credit providers rely on a checksum formula for distinguishing valid numbers from random collections of digits. What is a checksum? Wiki: Checksum A checksum is a small-sized datum derived from a block of digital data for the purpose of detecting errors that may have been introduced during its transmission or storage. By themselves, checksums are often used to verify data integrity but are not relied upon to verify data authenticity. The procedure which generates this checksum is called a checksum function or checksum algorithm. Depending on its design goals, a good checksum algorithm will usually output a significantly different value, even for small changes made to the input. In this section, you will implement the validation algorithm for credit cards. It follows these steps: Double the value of every second digit beginning from the right. That is, the last digit is unchanged; the second-to-last digit is doubled; the third-to-last digit is unchanged; and so on. For example, [1,3,8,6] becomes [2,3,16,6] Add the digits of the doubled values and the undoubled digits from the original number. For example, [2,3,16,6] becomes 2+3+1+6+6 = 18 :confused: Note: I do not understand how is this calculation done. How am I supposed to add [2,3,16,6] and [1,3,8,6] Note: So 16 is not a digit? Numerical digit A numerical digit is a single symbol (such as \"2\" or \"5\") used alone, or in combinations (such as \"25\"), to represent numbers (such as the number 25) according to some positional numeral systems. The single digits (as one-digit-numerals) and their combinations (such as \"25\") are the numerals of the numeral system they belong to. For example,the decimal system (base 10) requires ten digits (0 through to 9), whereas the binary system (base2) has two digits (e.g.: 0 and 1) Note: add the digits of the doubled values and the undoubled digits from the original number. so [2,3,16,6] = 2 + 3 + 1 + 6 + 6 = 18 and [1,3,8,6] = 1 + 3 + 8 + 6 = 18 oh wow now I see, this is cool, never seen this one before. calculate the remainder when the sum is divided by 10. rem 18 10 => 8","title":"Validating Credit Card Numbers"},{"location":"observations/#exercise-1","text":"We need to first find the digits of a number. Define the functions. toDigits :: Integer -> [Integer] toDigitsRev :: Integer -> [Integer] -- toDigits should convert positive Integers to a list of digits. -- for 0 or negative inputs, toDigits should return the empty list. -- toDigitsRev should do the same, but with the digits reversed. Note: So it is about the positional system, we somehow need to decompose a decimal number into its digits, aka it's tenths, hundreds?, thousands,.. example: a number 12345 into 5, 45, 345, 2345.. at least that is the first idea. Later we have to somehow add single digits, for now be begin to extract the last digit. lastDigit :: Integer -> Integer lastDigit = (`mod` 10) -- this will give us the last digit, or the 10th column -- for example to get 100ths or 1000ths we would done lastDigit 1234 4 lastTwoDigits = (`mod` 100) lastTwoDigits 1234 34 -- or lastThreeDigits = (`mod` 1000) lastThreeDigits 1234 234 -- also divMod 1234 10 (123,4) -- now check this magic dropLastDigit :: Integer -> Integer dropLastDigit = (`div` 10) dropLastDigit 1234 123 So our dropLastDigit and lastDigit are like basic operators on lists! Usual haskell tutorials mention these but too bad there are no examples like the credit card because here we have extraction of digits from a single number which at first seems like a difficult problem. What are our list operators? init and.. last -- have to put tea :) -- gosh, still have to learn vim.. head [1,2,3,4] 5 tail [1,2,3,4] [2,3,4] last [1,2,3,4] 4 init [1,2,3,4] [1,2,3] Next we have to write a function that splits a number into its digits in reverse order called, toDigitsRev toDigitsRev :: Integer -> [Integer] toDigitsRev n | n <= 0 = [] | otherwise = lastDigit n : toDigitsRev (dropLastDigit n) But how to just get digits of some number in Haskell? stack overflow question 135 `div` 10 = 13 135 `mod` 10 = 5 generalize into a function: digits :: Integral x => x -> [x] digits 0 = [] digits x = digits (x `div` 10) ++ [x `mod` 10] -- and reverse toRevDigits :: Integral n => n -> [n] toRevDigits 0 = [] toRevDigits n = n `mod` 10 : toRevDigits (n `div` 10) [ ] - Check more interesting answers on stack on this question. Note: Notice lexi-lambdas solution is more elegant since the functions are already abstracted. Before beginning to just solve the function provided here we see two smaller functions composed into one. Next lexi-lambda observes: Sure enough, that fixed things. Still, something about me doesn't like the asymmetry between the two expressions on either side of the cons (is it still called that in Haskell?). I remember something about where clauses; let's try that. toDigitsRev :: Integer -> [Integer] toDigitsRev n | n <= 0 = [] | otherwise = d : toRevDigits ds where d = lastDigit n ds = dropLastDigit n Well, that seemed to work, but is it nicer...? I don't know. Now it just seems more verbose, honestly. I can't really decide.. (break) -- gotta do lunch.. :gem: We see that easily we can decompose our thinking process into smaller and smaller functions, or maybe better to say our flow of thinking includes creating small functions chained together, each chain creating in turn a higher function, and so it continues. Sometimes, as in the first lexi-lambda example where we see only a partial application of div and mod abstracted into a unique function while in stack overflow answers these calculations are not abstracted but included in the final toDigitsRev function. What seems better? Am I to abstract partial applications as well? As a learning method this seems good. Chaining smallest possible entities into larger ones. How flexible is that actually? Is there some cognitive upper bound since there are many abstracted functions. This also brings to the point the qestion on naming functions. Our names should reflect the intention of the function. Also seems to me that abstracted functions are easier to read since there are visually fewer calculations. Hm.. End of Exercise 1 and 2 (break) -- gotta do lunch..","title":"Exercise 1"},{"location":"observations/#exercise-3","text":"The output of doubleEveryOther has a mix of one-digit and two-digit numbers. Define the function: sumDigits :: [Integer] -> Integer to calculate the sum of all digits. Example : sumDigits [16,7,12,5] = 1+6+7+1+2+5=22 So here as well we need to reuse the function toRevDigits to split the number into a digit seems to me. sumDigits :: [Integer] -> Integer sumDigits = sum . concatMap toRevDigits stack answer on concatMap Yes, the concatMap function is just concat and map put together. Hence the name. Putting functions together simply means composing them: (.) :: (b -> c) -> (a -> b) -> a -> c However concat and map cannot be put together by simply using function composition because of the type signature of map : map :: (a -> b) -> [a] -> [b] ^^^^^^ ^^^ ^^^ a b c [ ] - why is this understood as a, b, c , I do not really understand since this now seems to merely put visual blocks between arrows into another layer of variable letters? Let's see concat, map, and concatMap in action! map (+1) [1,2,3,4] > [2,3,4,5] Will this work on strings too? Compare how map still keeps the list while concatMap somehow has a list without the commas in between, without the elements separated. What is the length of \"some string\" ? map (++\"!\") [\"one\", \"two\", \"three\"] > [\"one!\", \"two!\", \"three!\"] concatMap (++\"! \") [\"one\", \"two\", \"three\"] > \"one! two! three!\" length [\"one!\", \"two!\", \"three!\"] > 3 length \"one! two! three!\" > 16 As you can see function composition expects a function of type a -> b , but map is of type a -> b -> c . To compose concat with map you need to use the .: operator instead: (.:) :: (c -> d) -> (a -> b -> c) -> a -> b -> d The concat function has a type signature of: concat :: [[a]] -> [a] ^^^^^ ^^^ c d Hence concat .: map is of type: concat .: map :: (a -> [b]) -> [a] -> [b] ^^^^^^^^^^ ^^^ ^^^ a b d Which is the same as that of concatMap : concatMap :: (a -> [b]) -> [a] -> [b] [ ] - the stack answer is very good, do check it out again because it includes also an interesting part where if you flip the arguments of concatMap you get the >>= (bind) function of the list monad! [ ] - on next coding session do an exploration of this part and redo the previous calculations again. How is monad connected to this?","title":"Exercise 3"},{"location":"observations/#exercise-4","text":"define the function validate :: Integer -> Bool that indicates whether an Integer could be a valid credit card number. This will use all functions defined in the previous exercises. Example : validate 4012888888881881 = True Example : validate 4012888888881882 = False Note: now what bugs me is that I do not understand why is the first example True and the second example false? Immediatelly I thought about putting any number into the validate function that will extract the digits, reverse them and then concatenate and then.. but how does it actually know is some number is true if there is no action of comparing two credit card numbers? Confused :confused: Off to google lands.. and stack overflow fields.. codereview.stack on the same exercise Write functions in terms of other functions You have written both toDigits as well as toDigitsRev . However, you only need one of them. THe other one is the reverse d variant: toDigits :: Integer -> [Integer] toDigits = reverse . toDigitsRev Note: Why did we define teDigitsRev in the first place? The exercise does say \"double the value of every second digit beginning from the right. That is, the last digit is unchanged; the second-to-last digit is doubled; the third-to-last digit is unchanged; and so on.\" But why are we starting from the right in the first place? What difference does it make to begin reading the card number from the last digit since we actually read from left to right? Shouldn't these kind of statements be explained in some terms or is this going too much into details? So calleds details, since these kind of obscurations may just interfere with the cognitive process. I do not understand why are we beginning from left to right so now I must investigate further why is the exercise stated in these terms. Thus, I must go away from the code yet again, and explore. While stumbling through these exercises and code explanations online we see that many just go over these details, which may seem superficial of super obvious to some people, and yet there are some idiots ? like me who do wonder why are we suddenly reading the numbers from left to right. Also, often just basic operations, or syntactic operations are explained as in \"look, this is some forrest, you can do this and you can do that, now go into the forrest and find me this or that\" while another maybe much subtler explanation would be to take the student into the forrest and just tag along offering advice on each step, since there are many rabbit holes in which one can fall into or explore. Sometimes experienced Haskellers just go over these seemingly obvious steps while the rest of us stumble and even though we might just follow through what happen after a while is that not enough spaced repetition in learning was done so these details will melt away. Maybe then, when we reach higher planes of abstraction we will not be equiped with enough functional lingo , enough explored rabbit holes, to fully undertand all the varieties of solving specific problems. Ok, only in this exercise I realized basic operation on lists, taking out single elements, like taking the head or the tail , or taking out the init or the whats the other one.. can be compared with this extraction of the digits from a number. In this example there is abstraction and composing of small functions and at the same time, we can understand with a much richer context the operations on lists themselves which might seem boring if we are just told, \"hey this is how you get the first and the rest of the list\". This might seem like a newbie ranting, but what I am aiming at is at defining a much richer context in which simple abstractions contain a higher level view, or like the exercise itself contains two higher level contexts which might enable one to better understand what is happening and how it all connects. We can see in the stack answer on concatMap which is a wonderful example how a seemingly simple composing of concat and map can take us all the way to the Monad itself. While learning Haskell I feel I have to explore each of these rabbit holes and then write about it too, just merely linking each Haskellers exploration but at the same time deepening my own exploration. Obviously this could be automated in the future where an AI could just in a wiki-like manner explore all angles of an exercise and tailor the deepening trajectory for each individual student depending on the progress. This could be modified in real time, so a student might progress on its own from a simple concat function to a monad. And no book would be the same since it would define itself by the student that interacts with it. Kinda like a smartbook . Notice the finishing touch on lexi-lambda's post on this exercise: Done. Well, that wasn't terribly interesting, though it was helpful to get used to working in the language again. I think I'll go through the next assignment, and if it doesn't get better, I might have to switch to something else. For now, though, I feel somewhat accomplished, despite having written an extremely trivial set of functions. I am grateful for lexi's post, it helped and motivated this pursuit and I haven't felt progressing in Haskell like this since I started, at least not in practical terms. Everything up to this point has been exploration and now this exploration has somehow interlinked into this one exercise. But the bitter taste remains because I wish other Haskell books would look like simple from super abstract exploration within a single space, a single assignment that can transform itself into a whole field of language.. well I can't really put this into words for now, but I feel nice about the progression :) To get back at our exercise, lexi wrote mysterious luhn instead of validate . What is luhn ? We are grateful for lexi's boon and we follow the luhn into the Haskell forrest of doom! :) search google.com -> luhn gives us Luhn algorith Luhn algorithm The Luhn algorithm or Luhn formula , also known as the \"modulus 10\" or \"mod 10\" algorithm, named after its creator, IBM scientist Hans Peter Luhn, is a simple checksum formula used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the United States, ... and some other.. notice: It is not inteded to be a cyptographically secure hash function; it was designed to protect against accidental errors, not malicious attacks. Note: Here we come to the \"Why do be begin reading our card numbers from the right\" question from before. The formula verifies a number against its included check digit, which is usually appended to a partial account number to generate the full account number. This number must pass the following test: Note: And here is the 'right to left' bit: This is similar to the exercise description. Would be cool if the exercise had this article linked too.. From the rightmost digit(excluding the check digit) and moving left, double the value of every second digit. The check digit is neither doubled nor included in this calculation; the first digit doubled is the digit located immediately left of the check digit. Note: So we see that the digits not included in the calculation are called 'the check digit'. These are the digits we will not double. If the result of this doubling operation is greater than 9 (e.g., 8 x 2 = 16), then add the digits of the result (e.g., 16: 1 + 6 = 7, 18: 1 + 8 = 9) or, alternatively, the same final result can be found by subtracting 9 from that result (e.g., 16: 16 - 9 = 7, 18: 18 - 9 = 9). Note: Ok so we see another way how to get digits from doubledigit numbers, this was not mentioned before. Take the sum of all the digits. If the total modulo 10 is equal to 0 (if the total ends in zero) then the nmuber is valid according to the Luhn formula; otherwise it is not valid. Note: There are more complex algos such as Verhoeff and Damm algorithma and Luhn mod N algorithm is an extension that supports non-numerical strings. The algorithm appeared in a United States Patent for a hand-held, mechanical device for computing the checksum. Therefore, it was required to be rather simple. Note: I still need to wrap my head around this algo, it seems simple but the underlying logic is not super clear.. but we see that this exercise comes in Programming in Haskell from Graham Hutton. That's great! Check this stack overflow on implementing the luhn in Haskell luhn :: Int -> Int -> Int -> Int -> Bool luhn w x y z = (luhnDouble w + x + luhnDouble y + z) `mod` 10 == 0 Check this stack overflow on luhn doubleAndSum :: [Int] -> Int doubleAndSum = fst . foldr (\\i (acc, even) -> (acc + nextStep even i, not even)) (0,False) where nextStep even i | even = (uncurry (+) . (`divMod` 10) . (*2)) i | otherwise = i myLuhn :: Int -> Bool myLuhn = (0 ==) . (`mod` 10) . doubleAndSum . (map (read . (: \"\"))) .show testCC :: [Bool] testCC = map myLuhn [.... , .... , .... , ...] -- => [True, False, False, True] Note: Well this version is too verbose for me, currently beyond 'alien speak'. I am aware of the names of these functions but do not really understand how they relate to each other. But writing Haskell code out like reading a difficult piece of music seems like a good practice too. There is also Luhn on Hackage, Haskell's repository of programs and libraries, the source is commented and seems nice to follow. Check it out, it is defined as a module and has test's as well. This one seems like the most comprehensible solution so far.","title":"Exercise 4"},{"location":"online-lectures/","text":"Online lectures 2018 Haskell School in the Networked Imagination Laboratory Bartosz Milewski Super Awesome Category Theory Lectures! Functional programming in Haskell: Supercharge your coding Bartosz Milewski - School of Haskell Functional programming in Haskell C9 Lectures: Dr. Erik Meijer - Functional Programming Fundamentals Haskell Summer Course Brent Yorgey - CSCI 360: Programming Languages (Fall 2016) from: My new programming languages course I decided to use class time in an unconventional way: for each class meeting I created a \u201cmodule\u201d, consisting of a literate Haskell file with some example code, explanatory text, and lots of holes where students needed to write answers to exercises or fill in code. I split the students into groups, and they spent class time just working through the module. Instead of standing at the front lecturing, I just wandered around watching them work and answering questions. It took a bit of getting used to\u2014for the first few classes I couldn\u2019t shake the feeling that I wasn\u2019t really doing my job\u2014but it quickly became clear that the students were really learning and engaging with the material in a way that they would not have been able to if I had just lectured. A happy byproduct of this approach is that the modules are fairly self-contained and can now be used by anyone to learn the material. Reading through all the modules and working through the exercises should be a great option for anyone wishing to learn some basics of programming language design and implementation. For example, I know I will probably reuse it to get summer research students up to speed. Note that the course assumes no knowledge of Haskell (so those familiar with Haskell can safely skip the first few modules), but introduces just enough to get where I want to go.","title":"Online-lectures"},{"location":"online-lectures/#online-lectures","text":"2018 Haskell School in the Networked Imagination Laboratory Bartosz Milewski Super Awesome Category Theory Lectures! Functional programming in Haskell: Supercharge your coding Bartosz Milewski - School of Haskell Functional programming in Haskell C9 Lectures: Dr. Erik Meijer - Functional Programming Fundamentals Haskell Summer Course Brent Yorgey - CSCI 360: Programming Languages (Fall 2016) from: My new programming languages course I decided to use class time in an unconventional way: for each class meeting I created a \u201cmodule\u201d, consisting of a literate Haskell file with some example code, explanatory text, and lots of holes where students needed to write answers to exercises or fill in code. I split the students into groups, and they spent class time just working through the module. Instead of standing at the front lecturing, I just wandered around watching them work and answering questions. It took a bit of getting used to\u2014for the first few classes I couldn\u2019t shake the feeling that I wasn\u2019t really doing my job\u2014but it quickly became clear that the students were really learning and engaging with the material in a way that they would not have been able to if I had just lectured. A happy byproduct of this approach is that the modules are fairly self-contained and can now be used by anyone to learn the material. Reading through all the modules and working through the exercises should be a great option for anyone wishing to learn some basics of programming language design and implementation. For example, I know I will probably reuse it to get summer research students up to speed. Note that the course assumes no knowledge of Haskell (so those familiar with Haskell can safely skip the first few modules), but introduces just enough to get where I want to go.","title":"Online lectures"},{"location":"papers/","text":"Papers Ralf Hinze collection of papers Graham Hutton: Universality and expressiveness of fold Eugenio Moggi: Notions of computation and monads Raul Rojas: A Tutorial Introduction to the Lambda Calculus Why calculating is better than scheming","title":"Papers"},{"location":"papers/#papers","text":"Ralf Hinze collection of papers Graham Hutton: Universality and expressiveness of fold Eugenio Moggi: Notions of computation and monads Raul Rojas: A Tutorial Introduction to the Lambda Calculus Why calculating is better than scheming","title":"Papers"},{"location":"talks/","text":"Talks Adventures with Types - SPJ Conal Elliott - Denotational Design: From Meanings to Programs youtube link: Conal Elliott - Denotational Design: From Meanings to Programs Main job of software designer, which is to build abstractions. What do we build? We build programs. But I don't think we build programs mainly, the program in the sense something you can run. For every program that I make, what's really more powerful in my output, what's more reusable, what's more value, is the libraries I create, so the components out of which I build programs. So every time I do programming I'm really looking for what abstractions are a great fit. If I can already find these abstractions supported of the shelf of some library I'll use them but most often I'm fairly dissatisfied with the building blocks, and so I wanna build my own abstractions, and then build a particular application on top. This quote I like very much, from the man I respect very much, Edsger Dijkstra: The purpose of abstraction is not to be vague, but to create a new semantic level in which one can be absolutely precise. So we can be precise and low level, and be precise without being abstract but then it's quite complicated and our cognitive abbilities will soon reach our limit before we realise our aspirations. There's a wonderful paper by Dijkstra called The Humble Programmer . Highly recommended talking about how we can use our abbilities, by walking into the project humbly realising that I'm gonna reach my cognitive limitations quite soon, then it informs the choices I want to make, I want to use abstractions that allow me to continue making progress even though I have a brain that has to fit in a skull. So there are three goals, I think of these as my three main goals when I'm doing a software project. Abstractions: precise, elegant, reusable. Implementations: correct, efficient, maintainable. Documentation: clear, simple, accurate. I'm gonna talk about precision a lot, and it's not that I think that precision is an end to itself, it's not. It's really these other things, like elegance and reusability. The reason I harp on precision is because it is so easy to fool ourselves and precision is what keeps us away from doing that. My implementation I'd like it to be fast, right?, but I don't want just fast, I want it to be correct, I care about what I implement. By writing no code I've written a very fast implementation but it's not gonna satisfy any interesting requirements. And of course, I want my program, my implementation to be maintainable, to sort of capture and hold my insights so that somebody coming back at me later can understand what I understood in putting them together. And then the documentation should also be simple, clear and accurate. So one way to make things simple is to make them inaccurate, I can tell you a simple story that isn't true and at some point I'm gonna harm you by doing that. So I want to tell the truth and to do so simply. Conventional programming is precise only about how, not what. So, I'm gonna say something, maybe a little jarring which is the commonly practiced state of the art of software is something that is, it's precise about how not about what. And I think maybe it's a historical accident, it's kind of the nature of our tool, our computer tool that we have to speak to it unambiguously, but it's only a tool for how, only a tool for sort of half of what we are talking about, or some fraction that we are talking about. And it imposes requirement on precision on us, on ambiguity, and the what , which to me is much more important than the how , it's a more fundamental part of the game, the tool doesn't impose it on us. And precision is hard and hard becomes unpleasant, so I think just for this kind of historical accident we've developed willingness and abbilities to be precise about hows. It is not only not right, it is not even wrong. Wolfgang Pauli came up with this description which I think applies to most software development. So it's not even wrong because there's no clear question, it's like an answer without a question. So that's what I would say: software without a specification is an answer without a question. Is it right is it wrong? We don't even know what it means. It's not even wrong I would say. And why does this matter? I love this quote from Bertrand Russell Everything is vague to a degree you do not realize till you have tried to make it precise. So Bertrand Russell in case you don't know, devoted years and years of his life, this was his life mission, to make things precise that people didn't really understand vague. So he found himself in the european mathematical community and was uncomfortable with the level of precision that was accepted in the day and he started poking at you know, professors and other practicing mathematicians, and they weren't really very receptive. They didn't like having it pointed out that the conclusions they were drawing were perhaps not sound, as they were based perhaps on unsound foundations. So what does it have to do with software? You might not want to write down a clear specification because you're comfortable, you think I basically know what I'm doing, I've got this kind of assignment, I've had a chat with my coworkers or my boss, or just internally in my head, I basically know what it's for, now let's get to the real work and do an implementation. All I suggest is what Bertrand Russell points out here which is that if you think you know what you're doing but you haven't made precise, you're probably wrong, or you're very likely wrong because the imprecisions don't come to the surface until you really try to make them precise. So don't be reasurred by a kind of informal like, ahaaa.., I pretty much know what I'm doing, it's time to go on. This applies particulary to computer programming, and there's this old,old quote by Demosthenes: What we wish, that we readily believe. And as a software delevoper, when I'm in the mode of trying to get something done, which is a lot of the time, sometimes I'm like trying to understand, or I'm trying to experince art or something, when I'm in the mode of trying to acomplish a goal, then, then what? The idea that my program might not be correct, or I might even understand what the question means, is it odds with my desire to move on to the next thing ? I wanna take a break, I wanna raise, I might just wanna do you other things that are more interesting, so I wish that my program were correct. That means I readily believe that it's correct. So I'm suggesting we have a built in bias to ignore the possibility that we don't even understand what the question is. So not even is it a possibility as Bertrand Russel pointed out that there's an important vaguery hidding but also I think there's, we have an internal psychological bias that makes it quite likely. So that's my pitch for precision, not because precision is important in itself, because it's a way to help us not fool ourselves and we have a tendency to fool ourselves. I prefer interactive talks to monologues even if I'm a monologist . So absolutely, questions whatever, please pipe up. Denotative programming Peter Landin recommended \"denotative\" to replace ill-defined \"functional\" and \"declarative\" Preperties: Nested expression structure. Each expression denotes something, depending only on denotations of subexpressions. \"... gives us a test for whether the notation is genuinely functional or merely masqerading. (The Next 700 Programming Languages, 1966) So, we are this community that we call functional programming and that term has been around for quite a while. In 1966 Peter Landin has published a seminal paper, The Next 700 Programming Languages , and in this paper he points out that this term that was being used even at the time, functional programming, is really ill defined, it's this fuzzy term along with non-procedural I think is one, and declarative is another. And if you've ever used the internet :smiley:, particulary programming language communities,programmers blogs, that kind of thing, you might have seen people arguing over whose language is more functional, is Scala really a functional language, is Lisp or Scheme functional, is Haskell with IO functional? I've poked around that one. And what I see is that it's easy to start with these conversations and it's very hard to get anywhere helpful. And I think that Peter Landin explained in his 1966 paper why that was, and it's because these terms are not well defined. And he recommended the alternative, and this is what I recommend: I would say, deprecate the term functional and let's talk about something that has clear meaning and then revisit all these conversations if we still want to revisit about whose language is functional and whose not and how to be more in that direction. And Peter Landin suggested the term denotative , defined by having three properties. One is that, we have a nested expression structure, so it's an expression within a expression. And the second is that every expression denotes something. So now we are really getting to the heart of things, it's not just syntax, so structured expressions is syntactic property, but that expressions mean something, denotes something, and that something depends only on the denotations of the subexpressions. So these are his three properties of what he calls denotative language, or denotative programming style. And these three properties, and in particular the third one he says, it gives us the test for whether the notation is genuinely functional, or merely masquerading. So that's why I sometimes I kind of poke around and say maybe that's functional but it's not genuinely functional. Question from the audience : When Landin is saying denotation and like today when I'm saying denotation are we talking about the same thing? Answer: Yes, we are. So this paper was published in 1966, there was early work by Chris Strachey, at that time, what he called denotational semantics , and Peter Landin, if you haven't heard Peter Landin, Peter Landin is one the few, really kind of founding fathers, founding parents of our discipline, and not just of functional programming but of understanding programming languages. So Peter Landin is extremely important figure in our history, and made contributions. For instance, Peter Landin is the one who realized: \"Hey, wait a minute, lambda calculus and programming have something to do with each other.\" Peter Landin figured out something called SECD machine , that's something like, how can we actually evaluate lambda expressions with a machine, OK? So this is soo, we take this so deeply for granted. Peter Landin was on the ALGOL comittee, and that was where he noticed and he wrote a famous paper called A Correspondence Between ALGOL 60 and Church's Lambda-Notation , that's where he was realizing. This is the seminal paper on the idea of domain specific embedded languages. This paper gave rise, I believe, to Scheme, ML and Haskell among other languages. So very infuential. So that's the idea of denotative programming and it's a clearer question. If you ask is Scala functional, it's a non-question, right?, because functional doesn't mean anything, but is Scala denotative ? Well, that's a much more interesting question. The question is can we give a semantics to is, in this compositional style, the meaning of an expression depends on the meaning of its components and if so, what does it look like and are we happy with the results? And if we're happy with them celebrate, if we're not happy let's steer. Denotational design Design methodology for \"genuinely functional\" programming: Precise, simple, and compelling specification. Informs use and implementation without entangling them. Standard algebraic abstractions. Free of abstraction leaks. Laws for free. Principled construction of correct implementation. So denotational design is a term I came up with a few years ago, and it's a subject of this workshop and here's a little pitch, a little commercial, so, denotational design, I think it's like a wonderful, natural fit for functional programming, particulary this programming in which we understand specifications in a clear way, and it has these properties, I'm claiming. It gives us precise, simple and compelling specifications. It addresses both the use of a library, the language so to speak, and it's implementation, but it doesn't entagle them. So, it's entirely clear about what the language means, what that distracting us with the implementation and concerns, and it entirely defines what it means for an implementation to be correct or not. It's correct if and onlf if it's outcome agrees exactly with the denotation. Doesn't mean they have to work at all similarly, in fact, denotations don't work, they just are. Implementations do , denotations are . It relates nicely to a set of abstractions that have come out of mathematics for last few hundred years, which we have also been using in some modern typed programming language like Haskell and Scala, so standard algebraic abstractions, things like functor, monoid and applicative, and things like that. Foldable, traversable, monad... And if you follow this principle, where ever you follow this principle, what I am suggesting to you, and if I don't convince during this talk, I want you to challange me on it. So, what I'm claiming is that, where you apply this principle you do not have abstraction leaks. And another way to say this, I mentioned earlier today, is that, if you try to apply this principle and you reach a point where it just doesn't work, you have an abstracion leak, and that's a very useful information. It tells me where my abstraction leaks are, I redesign, I eliminate the abstraction leak, and to do it by finding a way to use this principle. And these abstractions are not, so, a standard abstraction like functor or monoid, it's a set of operations, but those operations have properties so there are laws. If you use this methodology the laws will hold. You cannot use this methodology and define an instance in which the laws do not hold. And sometimes to give you examples today, sometimes this process will also lead you to a correct implementation, in addition to giving you a specification that you can use, you can derive a correct implementation. It may not be the most efficient implementation but definitely it will be correct, and if you have a more efficient implementation in mind then it offers a path, or it suggests there is a path to look for, how do I get to my efficient implementation, from this specification, by sort of, correctness (?) steps, how do I kind of calculate, even though if it takes some ingenuity and creativity. Broad outline: Example, informally Pretty pictures Principles More examples Reflection So this is what I wanna talk about today. I am going to give you one example that I intend to give rather slowly and thoroughly, and it is a kind of, something I want to invite you to participate in. So this part is not so much a talk, as a seminar or something like that, for us to investigate together how we might do a library design, and the library design in particular is one for inventory, for image synthesis. Something based on some, example came from something I did around the year 2000. So we will go through this process and I hope you get a feel for what it is like for me to do, to apply the denotational design, and hopefully fun example. And then I'll show you some pretty pictures, which, you know, maybe I'll show you the pretty pictures first and then we will reflect and say what are the principles that we used, and then I have a few more smaller examples, hopefully to give some sense this isn't just a one-off thing. And absolutely, discussion, participation, questions, objections, moral indignation, what ever you have, I invite it. And I'm explicitly not asking you to believe anything I say today . OK? I don't want you to believe anything I say because I say it, I want you to try it on and see if it, how it feels to you, which is what I did. Example I want to talk about is image synthesis and manipulation. OK? That's the assignment. It's either your hobby or you just joined the group and that's the goal. We want to have some kind of groovy, a flexible fun to use library for doing image manipulation. How to start? What is success? We want it to be efficient to, but I don't think I'm gonna get into that, and in fact my library that I did this is extremely efficient, it runs insanely fast. So couple of questions is, where do we even start? Like, we're this team, a product development team, and this is our edict, image synthesis library, high-level, fun to use. So how do we start, how we define success? Any thought on how we start on this project? Assuming some of you are programmers :smiley: audience: \"What are the things we are manipulating with?\" OK, yeah, so what are the things we are going to be manipulating, so, imagery, well, let's get more specific than that, OK? So what kind of things, and then what is success. Well? That's probably very tightly related to this question, so what kind of functionality, so when we implement this functionality and provide it, conveniently, correctly, maybe that's success. We might understand better these question when we come back to them. So, what is we are trying to build? I'm gonna suggest, we can do this brainstorming, what kind of operations, (...) think this phase is terrific and illuminatig so I just kind of threw out a bunch here. So this is a kind of function that I would imagine would come out of either you know, kind of a clear requirements statement or a brainstorm. Functionality Import & export Spatial transformation: Affine: translate, scale, rotate Non-affine: swirls, lenses, inversions, ... Cropping Monochrome Overlay Blend Blur & sharpen Geometry, gradients, ... So some way of getting stuff into the system manipulation out the system, so it's purely synthetic imagery it's gonna be nothing interesting at all coming in from the outside. If I'm gonna do some kind of manipulation of given imagery well I'm gonna want to import something from maybe some standard file formats, and I'm gonna do all my manipulations, and then at the end I want to give something out, so maybe I have some kind of import/export kind of thing, and those are just the endpoint, now all the interesting stuff is in the middle. So one thing that comes up alot is spatial transformation so that can be things like translate, scale, rotate, OK? These are all linear, affine transformations they are called, but there are also interesting things like doing a twist in the middle or a bulge, or something like that. And I think I said I will show you some pictures so let's pretend that this is part of our information we get about what we are trying to do, let's see, conal.net/Pan/Gallery , so here are a bunch of pictures I made a long time ago with this language, but not really, these aren't the pictures that I describe, these are croppings and samplings of the pictures I described, so these are things I know how to put on the web, these are the kind of things that can be exported, and each of these is a link to a much higher res version. OK? So this is like a playful exploration of image synthesis. All of these are kind of collections of lots of examples. These maybe give you some sort of feel, as you can see, it's not toward kind of practical problem solving, advertisements or something like that, you know, more playful kind of stuff. All right, In some of these examples, there's kind of bulges that happen, like a magnifying glass effect, and some there are spiraly things happening which you might think of as, I have something linear and then I wrap it into a spiral, infinitely out, infinitely in. So there are all these interesting non-affine, and then there is turning things inside out, makes beutiful pictures. And there is cropping. I've got some image, and I just wanna like maybe crop a circular region out of it or some funny shape, there is this monochrome image, it may sound kind of silly, but it's actually quite useful, I may just want the images red, just red. Now that's likely to be my final output, maybe I'm a modern artist or something, but still likely even in that case, but combined with cropping and overlaying and things like that it's useful. So another operation is overlay , if I have two images and put one of them in front of the other one, so that shows up quite alot, reality in our image systems are doing that all the time, by looking at here there's all kinds of overlays happening. And then there is blending . I have two images and I want some sort of blend between the two, maybe some kind of linear interpolation, or something like that, some midway point between the two, and then there are area operations which are like blurring and sharpenning so I might have an image that I want all of it or some part of it to be blurred, or I might want to exaggarate trasitions sharpenning it, and there is all kinds of other toys, various geometric things, funny shapes, and then smooth gradients and things like that so, these are the kind of things that we'd like to be able to express. API first pass So we're gonna define an API that's gonna be used in some language. We get to choose a functional language, maybe we are told to, maybe we get to. So, I wanna talk about designing functional APIs, so what kind of things show up in a functional API? I don't mean what kind of functionality, I mean... :smiley: Questions: So the observations that transform are a monoid. Answer: Yes, transforms are a monoid in an interesting sort of way, and I want to suggest we come back to that when we think about how can we elegantly structure. So in a functional language, in an imperative API the operations are gonna be about how to make up state, initialize it, and side effect it. OK? I've looked at alot of imperative imaging API's and they are like that. Build up some graphic state, set some brushes, you know, this kind of stuff, modify some output that's being accumulated, maybe it's on the screen or an offscreen buffer. So it's all about creating, initializing, modifying state, releasing state, that kind of thing. Functional API's are not like that. Functional API's, we want to describe things that are , and then building blocks for combining them. So we're gonna have instead of state and operations on it, we're gonna have types and values of those types and values of those types, just because that's the kind of programming that we do. So a good question to ask is what types are gonna be involved? OK, we don't yet have to know what those types mean, we can be a little fuzzy about it, because this is a process of clarifying our ideas. type Image over :: Image -> Image -> Image transform :: Transform -> Image -> Image crop :: Region -> Image -> Image monochrome :: Color -> Image -- shapes, gradients, etc. fromBitmap :: Bitmap -> Image toBitmap :: Image -> Bitmap This is an example of taking the previous slide and starting to give it some concrete shape, some concrete specific shape and again, so far it's just the API, it's not what anything means, and certainly not how to implement it. So we might have a type Image , so imperative API's don't have a type Image or if they do, they mean something off at of a file, or something like that, they don't mean it in compositional sense. But we instead of composing side effects, we're composing images. It's a functional language, so the idea of rendering or displaying or updating screen is not even part of the API, we're just talking about what the images are. So in each of those operations I mentioned in the previous slide, it's gonna show up somewhere as an operation with a type. So overlay for instance, is something that takes two images and gives an image, and in imperative API you dont' see overlay. You see a couple of pieces of code that do side effects, one of them being executed after the other, that's how overlay shows up in an imperative API, and our API overlay is an actual operation in it and it is about, or the expression of overlay of two images, it doesn't mean it goes anywhere, it doesn't mean it modifies anything yet. For instance, we might never use the result, or we might use it more than once, or we might transform it somewhere before it shows up. So that's overlay. And then there's transformation. If I have an image I might want a bigger version of that same image, or a version somewhere else, or a version that's been interestingly warped, so that's what transformation does, that's what transformations are for. Makes sense so far? Cropping also, we want to crop an image, what we need to describe some shape, I'll call that a region, so, what do I mean by image, what do I mean by region, what do I mean by transform, all this is gonna become clear, that's the creative process. And then, a monochrome image, I just give it a color, and I get an image of that color. And you might expect me to say, a color and how big it is or something like that, but we'll come back to that question if we have enough argument here. question from the audience: You're wondering why monochrome doesn't say give me a color and an image and I'll give you an image? Aha.. What would that operation intuitively, what would it mean informaly? Oh! I see, I have an image, and I want you to make that image red or something like that. Yeah, that's interesting. So here's a question which we kind of put up in the air, can we describe that operation, can we build it out of simpler parts? One of my goals, I'll make that explicit, is I want to define the simplest possible building blocks. If I have something that is somewhat complex operation I'd really like to make it not be permanent but definable out of simpler pieces. Why? So that I have a simpler set of primitives that I have to figure out what they mean and how to implement them. Like Robbies talk this morning, his keynote about this macro system, it's very nice to have a small set of language primitives, this is part of the ethos of Scheme, very small set of language primitives and then the abbility to define richer things in terms of simpler things and it means that my core is quite simple and implementation of those more complex can be quite simple by translation or if you want that specification you can more efficient implementation(?). question from the audience: So already some critique of this API, overlay , that sounds a bit ad-hoc, aren't there other way to combine images besides this one, and maybe this one can be built out of something simpler or more general. Yes, and I completely agree. So, what I am doing here, and absolutely welcome what you are doing, you're getting a little ahead which is fine, this is participatory, this is your talk, this is our session together, so we could, throw these all ideas up, and then critique them or we could critique them while we throw them up, it doesn't matter. There may be some personality issues, like I'll get really offended if you challange my idea or something but I actually won't so we don't need to do that. Yeah.. So already overlaying sounds suspiciously specialized. And in this last two we're just getting the information in and out of the system. But it's important, I'm not assuming that Bitmap and Image is the same thing, so what I am assuming is that there's this thing called bitmap which is like a jpeg, or who knows, it's just this kind of array of pixels which you know, have some kind of depth and that kind of thing, assuming there's going to be some stuff outside that we're going to want to bring some information in, do cool stuff, and then also after we're done with cool stuff, we're going to put out into. But I don't want to assume that that's what we mean by Image , and we will discuss there are very good reasons to not make that choice. How to implement? Now let's talk about how to implement. :smiley: wrong first question Let's not talk about how to implement, OK? So, I enjoy implementation, I am guessing you enjoy implementation or you wouldn't be here, but it's not a good first question, in part because it's not a well defined question. How is an answer, what is a question. How do we implement what, how do we implement this, No, we don't even know what this means, what's an image, these are just kind of intuitive and formal ideas, so let's be more clear about what these mean and then ask the question of how to implement. Does that makes sense, or let me put it differently.. Is that? Is there anybody who will like to go more into, more about the separation between why we would wanna make something more clear other than how to implement it at this point? I don't hear it talked about a lot anymore this distinction between specification and implementation. Question from audience: Identify symmetries? ... Ah, are you asking am I sort of content with this API? Is that what you are asking? I'm just making a guess, I don't think I'm right, I'm just trying... How they would be used? ... OK, by symmetries, do you mean, hm, maybe there's some structure that I realize I could refactor? Or... do you mean something like visual symmetries, I'm guessing you don't but maybe you do?... Oh, yeah, yeah, understanding maybe the set of images that are expressible in this form? :smiley:, I think I'm not converging, but I'm roughly, infinitely patient, (Smiles) about understanding what you are wanting to say. OK, ok, come back to it, and say oh! now I know how to get this through your thick skull Conal. So, how to implement ? NO (:smiley), different question, OK? What to implement? OK, and we did not just say what to implement in a precise way, so let's get more precise, so. What do these operations mean? What do I mean by mean? There's a more fundamental question which is what do the types mean. More centrally: What do the types mean? Once I know what the types mean, then I can at least understand the question of what do the operations mean, because the operations say, over takes two images and gives an image, we can't possibly talk about what over means without understanding what an image means, right? Because it's a function on images and gives an image. So this is the central question. Doing the API design we're going to define some types, types of values, because we are doing functional programming, and type functional programming in particular and we want to say what do the operations mean. Question from the audience: OK, let me see if I got you, and I want to encourage people to sit closer, for two reason, one is to be more included, and the other is for me to hear you more easily. If you don't want to do that I'll just try harder to hear. So, I think what you are asking is, when you say mean , do you mean how to implement them, and if you don't mean that, what do you mean? OK, thanks. Yeah, I absolutely do not intend to say how do we represent the types or how do we implement the operations, so that would be confusion. What I really mean is to clearly specify, I want to give some formal specification, somehow, that tells me as an implementor exactly what it means ... It's not that it tells me whether my implementation is correct or not, it tells me what it would mean for an implementation to be correct or not, that defines correctness. Then I know what to aim for. OK? It also tells me the breadth, so I want to be constrained enough to know what correctness is, I don't want to be over-constrained because I don't want to be told how to implement something, because I may have a lot of ideas, and here I'll maybe touch on some of the ways I've implemented this vastly different from each other, OK? So I don't want to be constrained in an implementation, or representation. I do want to get very clear, about, something about the specification, and there's a particular style of specification, and that's the one I'm talking about today, which is denotational . And I'll show you supersoon what that looks like. I am so sorry that's freezing in here, right? I think it's even worse for you than for me there. It's blowing here, yeah I don't know... So I appreciate you hanging in, if you have to leave, if your face turns blue, I won't be offended. So, central question for me is what types are there and what do they mean? . What is a mathematical model that I can use to think about those types in a precise way. So here, so far, the central type of interest is Image . There were some other types, transforms, colors, regions. So let's start with one, let's start with images. So what is an image? Specification goals: Adequate Simple Precise Before entering that question what do we want out of an answer and I am suggesting these three properties. I want my mathematical model to be adequate, if it's not adequate then I can't express the things I want to express. Either I want to or my users. It has to be simple. Why does it have to be simple? It's because I'm basically not very smart, I mean, I mean I'm smarter than a mouse, right? But I'm hopefully not as smart as my children will be and so on, and all of us collectively so I have very limited ability. So I need the specification to be simple enough that I can reason practicably and reliably. And then precision. And why? So that I really have simplicity and adequacy, not just think I do. So precision is all about helping me not fool myself, because I'm psychologically inclined to fool myself, so precision is a coping mechanism. OK? Now, let's go back to this question what an image is.:smiley:. This is a collaboration. What is an image? OK? And I want a lot of bad suggestions, I want lots of stupid ideas, what I really mean is, you know, please don't be shy about making suggestion, because every suggestion is illuminating either for it's strengths or it's weaknesses or both, and I apology for flashing my answer which I didn't mean to bias you with. When I've done this exercise, people have quite a variety of intuitions. It's what? A matrix of pixels? Sure, totally. So by matrix of pixels I assume you mean some kind of rectangual, regulary spaced collection of pixels and a pixel is... yeah, colors or maybe something that represents like 24-bit values to be interpreted in certain way. Yeah, so that's a very common answer, so it could be an array or matrix of pixels. Other? Oh, OK, so already we are getting criticism from the very start, programmers.. yeah, so maybe there's a little bit of a discrete bias here that we might wanna question. (Listening to a question)... OK, so it's sort of functioney view, yeah, and last year I talked about this before I talked about FRP, and so nobody made that suggestion but that's certainly one possibility as absurd as it sounds. :smiley: Anybody else? You'll see I'm joking, hopefully at least... A way of collecting and saving light? Sure! Yeah, absolutely. And so we might think that it's a way, hmm, that might being an algorithm, a function, a recipe, something like that, yeah, so, not to hurl insults, but an object oriented sort of perspective might be about it's an interface that has some kind of operations.. yeah, other thought? Yeah, sure, so an image might be represented by a collection of shapes, so for instance, in something like a pdf or postscript, yes, so pdf and postscript are all about image synthesis and manipulation, and they are all about geometry, so it could be some shapes, in other words some descriptions and then... totally, yeah (seeing?) graph, or it's called linear that's all that was. No, I worked in computer graphics, (shiver of disgust?)... so yeah absolutely, It could be some kind of data structury thing which contains in it some kind of descriptions of objects, 2 or 3d dimensional, some description of lights and cameras, kind of some information out of which one can synthesise an image, and a lot of particulary 3D API's are that way... OH! That sounds like a poets answer to me. So, Dicks contribution is, yeah an image is a short line or a short body of text that conjures a what in someones mind? Oh, conjures a picture in the readers mind, I see. I might recursively ask you what a picture is and wonder if you're gonna tell me it conjures an image in readers mind but I won't do that. Thank you. Yeah, absolutely, so again it's something that's like evocative in a sort of more poetic end of the spectrum than like a 3D scene graph API kind of a thing... Cool, so here is an interesting perspective is that maybe we were a little too focused on color, so an image is being sort of color over the discrete, continuous and finite, infinite whatever space, maybe there are other things as well that we maybe want to array over space. Maybe it's the overspaceness that matters more. Yeah, OK. So we can take all of these suggestion, if we are doing kind of classic brainstorming write them up on the board. We have our goals which I suggested here and we can evaluate them all and see where they lead. If we leave out precision we are not gonna get a very accurate estimate of where they lead, because for instance, simplicity, many ideas are simple only because they are not precise and when made precise they become quite complex, or their complexity becomes apparent. It's cold, sorry, god.. So my personal answer which especially if you've been listening me talk about FRP is perhaps somewhat perdictable, so my answer is it's an assignment of colors to 2D locations and I'll be more clear what I mean by that. My answer: assignment of colors to 2D locations. This is sort of the same answer in a less specific form than an array or a matrix of pixels. So that's, you know, an array it's an assignment for every slot in my array I assign one color to it, but it leaves kinda little more open ended what the shape of the container is. How to make precise? type Image If we start with this intuition, and then we want to make it precise so that we can understand the implications of it, we can do this to every idea that we've talked about, and some of them we might learn very early what some of it's limitations are, and that's an extremely fruitful part of this exercise. So let's make this precise, and here's the form in which I do it and this is the denotational form. mu :: Image -> (Loc -> Color) So we have a function, I'm calling it mu (greek letter), mu is supposed to suggest meaning , m right? So mu is a meaning function and the idea is that I want to say what an image means, or give you a model for relating to images, by, it's a mapping so mu is a mapping from images to something, and that something I'm saying is assignment of colors to locations and there's a simple precise to say that which is a function from location to colors. OK? Now over here on the right, so on the left of this main arrow, is the type we are interested in ( Image ), on the right is another type ( Loc -> Color ), for the most part you can think of that type as being, what we mean by functions in Haskell, what I really mean is mathematical functions, which is what we mean by functions in Haskell, but it's not what we mean by functions in most other languages, like an ML function, a LISP function, a Scala function, is it a function because it has side effects? I mean really a mathematical function. So you can either think of it as a math function, or you can think of it as a kind of a function in a purely functional language. But it's kind of important to make that distinction because it doesn't have to be implementable, OK?, because we're never gonna implement mu , we're just gonna use it to understand our library design and evaluate it as it emerges. What about regions? OK, so but Image was just one type, and I'm not saying that you should somehow know why this is a good choice, we're gonna look at it, I think. And then we can examine alternatives. So what about regions? Well, anybody have an idea what a region might mean? Locations to what? Ah... to opacity? Interesting, OK. So we might also talk about opacity over space, any other thoughts? Oh, a list of locations, yeah, that's actually quite common, used, in awful image, I don't mean this is an awful suggestion, what I mean is it so happens that a lot of the image, strike the word afwul, that choice shows up in a lot of them, often in a kind of compressed form, it maybe like a set of rectangles, that's quite common?... A function from location to boolean, quite similar suggestion as well, yeah, OK, same suggestion. And then... yeah sure, (?) polygon, so I might have some primitives for describing my regions, like polygons, and then I might mean a bunch of them, a union of them, perhaps countable, interesting choice, countable... mu :: Region -> (Loc -> Bool) So this is my choice, this is one of them, which is it's a function from locations to booleans, if you known this. Function to boolean is isomorphic to another mathematical notion that we use a lot, I hope you know this, which is sets. So function from location to boolean is the same as set of locations, something called characteristic function if you took math a while ago like I did. So another model is to say a region, the meaning of a region is a set of locations, that's very similar to the suggestion of a list of locations. So we could also say it's a set of polygons but a set of polygons is a more complex, it's maybe more efficient, but uh wait a minute, we're not talking about efficiency, that's a non issue yet, we may want to represent out set by a collection of polygons for instance... OK, let try to get what you're saying, so one was an idea and the other was a kind of motivation for it I think. OK, so I might say that a region is a function from, it's something that takes an image and a point and, an image and a location and says, it either says there's nothing here, or there is something here and this is what color there is. OK, so that would be some kind of relationship between like a region and an image right? I would hope that something like that because it involves both could be describable in terms of simpler pieces, let's see if that's the case... Oh, I see.. So let me try and tie this idea. Suppose we had a notion of a region which is a set of locations, in other words, a function from locations to bool, I mean, maybe that's not obvious to everybody. So the equivelancy or those two notions is, if I have a function from locations to bools I can think of it as a set, a set is just a question of are you in or are you out, and that's what the question that location and bool answers. So if I have a set I can form a function, just by saying are in the set or not. If I have the set I can form the function, but by asking the question given any location, are you in it, and if so yes, if not no. So given this simpler notion of regions, yeah, we can I think take that region, aha.. interesting, OK. So we can ask questions like, what is the set of all points in space that have some color, or what's the set of all points that have a particular color, I'm not sure we get to the expressiveness of what you're suggesting, that's interesting, we might come back to it. Ok, so here are a couple of models and this is the style that I mean of giving a meaning to image. Once we have, once we choose some, now we can go back to the operations and say what do they mean on the meanings . How do the operations work as if they were on meanings of images rather than on images themselves. So an image is going to be some datatype, it's going to have some representation, some runtime existance, the meaning is going to be a mathematical model, and have no runtime representation, no runtime existance, but it'll tell us what it means, what these operations mean, it'll give us a specifications I hope we'll see. Specifying Image operations mu (over top bot) == ... mu (crop reg im) == ... mu (monochorme c) == ... mu (transform tr im) == ... Aaa, thank you, why didn't I just not use set? Hmm.. I haven't thought about that question in a while, and absolutely I find myself explaining this idea of region as a characteristic function in terms of the set, so I think that tells you and me, now that I think about it, that a set definitions, it feels a little more natural, a region is a set of points, that's it. So, let me change my mind, and say a region is going to be a set of points. Then I'll change my mind and get back to this one. OK? That does seem more natural, let's do that instead. So I don't mean something of type set in Haskell, for instance, which is finite, I don't even mean countable sets, because I want to say like, a disc is a region and it has uncountable, well, I'm getting ahead of myself, it's got a lot in it, so yeah, I mean set in the mathematical sense. So now given this model... yeah absolutely... OK, so there is awful a lot of sets, :smiley:, an awful lot of sets, maybe more sets than I really am interested in using, there's definitely more sets than I can possibly express in any language, right? That's one of the things, a corollary of Cantor's work in the 1800s. Yeah, sets seem maybe awfully ambitious, or awfully sloppy when you think about it. I really may want a much smaller, yeah, so am I tracking? Yeah, so one of the tradeoffs I make is I might make the denotation less precise, in another words have room for a lot more than I can express in order to make them simpler. And we might come back to that if there's interest and talk about that tradeoff... Interesting, so will we need some properties of sets which is going to restrict the kind of sets we can talk about, for instance, measurability, and measurability might be quite relevant to our ability to render something because we're going to render in pixels, and pixels cover entire areas, yeah, and if we want something like the integral of a function over the area, or something like that, then measurability is going to come into play, some form of measurability. So I don't know, we'll see, great question. (56.56) Brian Beckman: Don't fear the Monad What I want to do is take the mystery out of monads. Monads without mystery or maybe monads without misery . Because if you've looked at link(?), or you've looked at F-sharp, or a much smaller number have looked at Haskell, you've encountered monads and they're scary the first time you encounter them. The main reason they're scary is you read the papers and they tell you oh, well, you need to know category theory before you can understand this, and while that's strictly speaking is true, it is not true that you need to know category theory to understand how monads work, what they're good for and even better yet, how to creatively use them yourself. Let me talk about foreign languages for a second, if you learn french or something, there are sort of three phases that you go through, the first is you hear french and you say, oh my goodness I'm scared to death of this, the second is, you learn a little bit of french, and you can recognize french words and you can read a little bit with a dictionary, and then the third and final phase is you can use it creatively in your own work, just speak freely. Those are the stages of learning any kind of new foreign concept. Monads are the same way. Everybody goes through this the first... The first encounter is horrifying, there is this strange concept and are you telling me I need to understand this strange bizzare concept in order to do anything with functional programming. Well, yeah the answer is yes, but what I want to take out of it is the strange and horrible part. It isn't strange and horrible, I'm going to explain it in terms of things you already know, I promise. Question: What the hell is functional programming? In three minutes or less, just quickly so that people can get to speed as to, they can then put the pieces together why monads would make sense in the context of functional programming? Not only do they make sense, they are a natural outgrowth of functional programming, you would invent them yourself if you've worked at it long enough, you would know the category theory, you would know the column monads, you would just call them better functions, that's what I am going to show you, but functional programming in a nutshell, is programming with functions. Yeah, right on... But normally, let me start scribbling on the board. Normally we are used to doing things like this in say, in C#: int x: static int f(nt x) { ... y = Math.Exp( (double) x) returns y; } Int x , and then we'll go some place and say, just for simplicity let's make everything static, we're inside some class here, right? And we say, static int f(int x) and it does something with the x , and it returns, oh maybe returns some y that's inside a function, let's make it do something, like y = Math.Exp ( (double) x) , it's already starting to look like a lot of stuff, but the important part is, we have a function f (int x) that takes an int as an argument and returns an int as a result. Nothing scarry there, functional programming is taking these guys ( f(int x) ) and saying not only are these guys not scarry, they're so ordinary that we're going to treat them just like we treat these ( int x: ) guys. These x: guys are data , these ( f ) guys are code , in functional programming we just say, well code is data too, we're not scarred of it. Now, let me introduce a little bit of notation, I'm gonna stop writing C-sharp, this is where it's gonna get a little intense, but I promise it will all be things you already know. Great! . If you know a function, you know everything you need to know to understand monads. Question: So, look, a function takes input and... but always returns something right? That's what function does, it does stuff, gives you functionality. It takes an input, and converts it into an output. That's your bread and butter, but the bread and butter is you just start tossing these ( f ) things around as easily you toss these ( x: ) things around. In ordinary programming, C-sharp, imperative programming, we think of data as being in one world, and then we think of these functions as stuff that operates on data, and they're special. In functional programming we say they're not special, functions are just like data. Let me prove to you that a function is like data. Every imperative programmer goes through this phase of learning that functions can be replaced with table lookups, often you do this for performance. You want to create the sine function, or the cosine function, make a big table and interpolate in the table, you don't actually sit there and do... everybody learns this trick. What's a table? Data . What is the function doing? Is looking stuff up. The act of looking up is universal. You could convert any function into a table lookup and then the functionality, the code, is the same for all functions, and the function is just data. Hmm... interesting, there's your proof. That concept alone should convince you that functions are of the same kind of thing as data. They're not special, you don't need to have special code for every function, since it's possible theoretically to convert every function into a table lookup and to have the evaluation part, the code be completely generic. Functions and data, data and functions, same thing. Think all nouny , no verby . :smiley: Question: So now, I mean, then I guess just quicky catch up, just so we get some of the confusion out of the way, when you think about the importance of say F-sharp, Haskell, definetly Erlang and the multicore world that everybody is talking right now, it has to do a lot with the notion of no shared state, no shared mutable state. That's right. Now, is that just a consequence, or a side effect of a side effect free language, like a functional language? Let's talk about shared mutable state and functional programming. Sure, so quickly. Pure functional programming, as in Haskell does not allow any state at all. Functions, once you create data, that's it, you created it, if you say int x equals 5 than x is 5 for ever and ever. It sounds like it's not possible to do much of anything useful with the language like that but in fact you can do almost everything you can do, you can't do shared mutable state, but you can simulate it with monads. Yes! Now we'll stop talking. No, you don't need to stop talking, because I want you to stop me any place where it's, where I'm starting to get confusing. I promise, and I will explain monads in things, in terms of things you already know and I'm going to go through four steps. One is Functions The second is Monoids. You don't know the word but you know what they are. Functions again, I'll come back to them and then Monads And this stuff is easy . It's conceptually very very simple, very very easy to explain, but you have to be willing to go with a little bit of a terse notation. Instead of saying int x: I wanna say x : int , and there is a very good reason I'm gonna do this. This phrase up here ( int x: ) says the variable x has type int . In mathematics you would say x is a member of the set of ints (x elem ints). Types and sets are almost exactly the same thing. I wanna write it with a colon here ( x : int ) because I want to put the types on the right hand side. This is an insertion x has type int . It means exactly the same as this. We are now leaving the realm of C-sharp and entering the realm of F-sharp, or the realm of functional programming when you see notation that looks like this. x : int Now let me write the type of a function. A function, remember how I wrote it before, this was ( int f (int x) ), now I have a very compact notation, I can say f : int -> int , a function f is a thing that takes an int and gives you an int . That's all it means, nothing more, nothing less. Next step. Instead of picking a particular type let me pick any type, and I'll use the word, the symbol a for any . x : a . x colon a means I am asserting that x , the variable x has type a . f : a -> a means I am asserting that the function f takes an a and gives you an a for a being any type. x : int f : int -> int ---------------- x : a f : a -> a Now, I'll go back to C-sharp for one last, one last little dive into C-sharp, this would be, you could say something like static a f <a> (a x)... . This ( f : a -> a ) is just short hand for generic types , stuff you already know, I promised you that wouldn't be anything you don't already know. We know types, we know generic types, we're not scared, we're almost to monoids. Let me add another function down here. This is a function g and I assert that my function g is a guy who takes an a and gives you an a . ( g : a -> a ) I now want to combine my two functions f and g . How can I combine them? Make them one function? Yeah! I can make them one function, there are couple of different ways of doing that, let me show you one. One way would be to say, take g , feed it a variable of type a , now I'm reusing the notation. The little a here ( g (a) ) means a variable of type little a . You forgive me for that, this is not at all confusing. This is, call g with a variable of type a and then call f on that result, f (g (a)) . That would be one way of combining these functions. Another way would be to call f first, f (a) , and then feed that result to g, g(f(a)) . Now, I'm going to take away the brackets, the reason is they are visually annoying. I'll leave this up here so that we are reminded that x has type a , f has type a to a , g has type a to a . In functional programming, we like to write function application, which means calling a function, function application, calling a function, sit in it. I promised you stuff you already know, nothing new. We like to write function application like this, g a . It looks like multiplication in ordinary mathematics. That's on purpose, because if g is a linear function and linear functions are special and happy and really cool, it is multiplication, so we make it look multiplication for any function, even if it's not a linear one. Not only does this get rid of brackets, but it reminds us that in the case of linear functions we have exactly this. Now, if I wished to, this is calling g with an argument of type a , now I write my combination, this time I need brackets, f (g a) and I won't go into this, the reason you need one pair of brackets here is because of currying . Currying has to do with functions of multiple arguments, I won't get into it today, but suffice it to say, if I chain a bunch of functions together, it associates to the left, f h g = (f h) g , automatically the brackets squeeze up on the left hand side in this kind of functional notation. This is standard in Haskell and F-sharp, nothing scarry here, just something you have to get used to. If you want to call g first and then f you have to write it this way. If you wanna call f first on a and then g write it this way, g (f a) . Let me now abstract, let's stare at this one for a second. f (g a) . Suppose, let me invent a notation, I'll call it little circle, (f . g) , f little circle g , this is a new function and the definition of this new function is that if I apply this new function f . g to the argument a , ( (f . g) a ), this is exactly the same as calling f with the result of calling g on a . (f . g) a = f (g a) This is a new function. I can now get rid of the a . I can't get rid of it on the right hand side, but I can get rid of it over here and I can give this new function a name, (f . g) = h . I can just say h is a new function, what's it's type? We know the type of g is a -> a , we know the type of f is a -> a . I took an a , I gave it to a g I get an a , I take an a I gave it to f I get an a . The type of this h thing has to be something that takes an a and gets an a . Look what we've done! (f . g) = h : a -> a . We've taken two things of the same type, a function ( f ) and a function ( g ), and we have created a new thing of the same type, h : a -> a . That is the essence of a monoid . The essence of a monoid, you have a way of taking two things of the same type, and creating another thing of the same type. This is an immenseley powerful concept, it enables you to create complexity starting with simplicity. I have a bunch of simple building blocks, I have a rule, little circle, for combining these building blocks in arbitrary ways, all it has to happen is the types line up and I get new things of the same type. Question: So let's step back real quickly and talk about perhaps the usefuleness of what you just did. So you took two function that take the same type, return the same type, you combine them into a new function h in this case. Let's talk about what that means in the context of usefuleness. This is the way to build complexity. Everyone knows that one of the biggest problems in software today is controlling complexity, we add features, we interface to other people's features, we try to extend our software to other environments, or other machines, we've got XML, we've got Ruby, we've got the web, we've got everything... And no matter, we find ourselves in this trap of extending our software constantly and it's not doing anything new it's just fitting to other stuff. We're out of control in the complexity space. The way to control complexity is compositionality . You step back, you create, it's a creative act, you step back, you create a design in which a number of smaller, simpler things... now, let me digress on the word simple . I use the word simple to mean small . Lot's of people use the word simple to mean something else, so I try to avoid the word simple and I just use the word small, because everybody agrees on what's smaller , but not everybody agrees on what's simpler. Some people use the word simple to mean more explicit , broader, some people use the word simple to mean more conscise , more abstract, that's why I avoid the word, but everybody agrees on what's smaller. The creative act of designing something, designing a software artifact in which it is easy to control complexity is the creative act of creating functions that can be combined this way, this is called function composition , little circle is called composition, and compositionality or composability, this is the way that you build up new things from things you already have. And remember I began this whole talk by telling you I'm gonna explain monads in terms of the things you already know. How am I gonna do that? By combining or composing them :smiley:. Question: *Now you've used the composability word, and we've talked about that on channel 9 a lot. And it's imporant, it's going to increase, as software get's more and more complicated, we need to have more and more control over the complexity. We need to be able build up big spaces, of rich software, but starting from simple building blocks. And the way to do it, the way to do it, is to ensure that the types line up, and to create yourself a generic composability operator that would allow you to take any things that are in your set of types, and combine them and create other things, new things, then are guaranteed to be in your set of types. Remember, we just calculated the type of h , we didn't have to assert it, it was implied by the fact that the type of f was a -> a , and the type of g was a -> a , and the little definition of composability operator which remember was, take g give it an a you get an a , feed that to f you get an a . Therefore the type of f . g , f composed g must be a -> a . There's no choice. Here's the beauty of this. If you discipline yourself and force yourself to put all your functions in this little type universe, then you can create new ones by composing them and be guaranteed that they will fit in the same type universe. You can't make a mistake, if you buy into the discipline. Question: What that means is you can change components anywhere in the system without destroying other parts of the system. Glue them together, mix and match, make anything you want, you can start with a small number of things, and end up with a large number of things, and they're all just correct by construction, at least in the realm of types. Now we're in the world of monoids. I'm going to give you a quick example to prove to you that you already know what a monoid is, and the example is a clock. 12 11 1 10 2 9 3 8 4 7 5 6 You already know what a clock looks like. A monoid is a set of things, plus a rule for combining the things, and that rule obeys some rules. That's it. You already know. Can you say that once more? . A monoid is a set of things, or a collection of things to be more precise, plus a rule for combining the things, and that rule obeys some rules. Just two levels of abstraction. And clock, the numbers on a clock form a monoid, the rule for combining them is: take one number x , add another number from the clocl y and then cast out 12 s, mod 12. (x + y) mod 12 . Everybody doing C-sharp or vb is familiar with mod operation. Now you could take any two numbers, I can take 7 and 10 , add them up, cast out 12 and I get 5 . In the monoid of clock arithmetic, 7 plus 10 is 5 , because it's 17 minus 12 is 5 . The rule, this is the rule ( x + y) mod 12 ), composed of two things, and the rules that the rule has to satisfy are the following: (+) (circled plus symbol) This little symbol stands for add the two and mod by 12 the constant.( combined ) The rules are: x (+) (y (+) z) is always the same as `(x (+) y) (+) z. In any monoid the rule for combinind things must satisfy this meta rule . This meta rule is called associativity , it means doesn't matter how you group the applications. The second rule is: The monoid must contain a special member such that x combined with the special member 12 is always x and the special member 12 combined with x is always x x (+) 12 = x 12 (+) x = x In the case of our clock the special member is 12 , if you add 12 to any number on the clock, you get the same number. 3 o'clock, add 12 hours, it's 3 o'clock again, am, pm we're not taking about that, we're talking about a 12 hour clock. Got it, absolute number, right . Notice, I carefully wrote the special member on the right, and the special member on the left because one of the rules that a monoid operation does not have to satisfy is commutativity . It is perfectly allowed for x combined y to be not equal to y combined x , ( x + y =/ y + x ). In the case of a clock, it so happens, if x combined y is y combined x , but we already found out in our function world, f on g of a is not g on f of a , sine , cosine ? cosine , sine , not the same thing. Anyway the point of the clock is to show you that a monoid is a simple concept, it has, just to review, a set of things, numbers, in this case numbers, in this case functions, a rule for combining the things in the case of a clock, add them and cast out 12 s and the rule has to satisfy meta-rules, and the two meta-rules are associativity and existance of a unit or a zero, generically or abstractly. OK, I'm gonna erase the clock example, because we've locked this monoid into our memory, because it's so simple, and now I'm going to go back to functions , se we're here: Functions [x] Monoid [x] Functions Monad And show you, I don't even really have to show you anything at all, do I? Because you already know the punch line. Functions under composition form a monoid, at least if the types are the same over here. Now, I'm going to just talk for a second, and I'm not actually gonna write this down, I'll leave this as exercise to the reader, because I want to get to monads without this extra complexity. In full generallity we allow ourselves to have different types, so we allow f : a -> b , and g : b -> c and then we can only combine them in one way. In that case they don't form a monoid but they form a monoidal category and that's where we go into category theory, but you don't need to know category theory to be fully conversant, to be fully fluent in this language of function composition. All you have to remember is the types need to line up. Everybody already knows this, from programming in staticly typed languages like C, C-sharp, and VB, the types of fun..., if you gonna nest function calls the types have to line up. There's nothing complicated about this, you don't need to know category theory to..., oh I mean if you want to learn category theory to understand the full , flowering glory of the, consequences of this astonishing... you can and by all means it will only increase your richness, but you can now speak french in the world of monoidal categories because you understand, as long as the types line up then compositionality makes sense. I'm gonna stick with the simpler monoid of functions here, all the types are the same because they form a monoid and it's absolutely crystal clear that just like a clock, once you go into the clock you can't fall out. You can add 3, and 17, and 49, and 600000... and you'll still get a number between 1 and 12. You can't make a mistake. Compositionality and clock arithmetic is exactly no more complicated than compositionality in function composition. This is the power of it, this is why compositionality is the thing that we need to know to control complexity. OK, let's prove that function composition is associative, that means f composed g , composed h , is always equal to f composed g composed h . (f . g) . h == f . (g . h) Remember I'm asserting that functions are in a monoid just like clock numbers are in a monoid. In order to prove that I have to demonstrate that my little circle operator is associative.","title":"Talks"},{"location":"talks/#talks","text":"","title":"Talks"},{"location":"talks/#adventures-with-types-spj","text":"","title":"Adventures with Types - SPJ"},{"location":"talks/#conal-elliott-denotational-design-from-meanings-to-programs","text":"youtube link: Conal Elliott - Denotational Design: From Meanings to Programs Main job of software designer, which is to build abstractions. What do we build? We build programs. But I don't think we build programs mainly, the program in the sense something you can run. For every program that I make, what's really more powerful in my output, what's more reusable, what's more value, is the libraries I create, so the components out of which I build programs. So every time I do programming I'm really looking for what abstractions are a great fit. If I can already find these abstractions supported of the shelf of some library I'll use them but most often I'm fairly dissatisfied with the building blocks, and so I wanna build my own abstractions, and then build a particular application on top. This quote I like very much, from the man I respect very much, Edsger Dijkstra: The purpose of abstraction is not to be vague, but to create a new semantic level in which one can be absolutely precise. So we can be precise and low level, and be precise without being abstract but then it's quite complicated and our cognitive abbilities will soon reach our limit before we realise our aspirations. There's a wonderful paper by Dijkstra called The Humble Programmer . Highly recommended talking about how we can use our abbilities, by walking into the project humbly realising that I'm gonna reach my cognitive limitations quite soon, then it informs the choices I want to make, I want to use abstractions that allow me to continue making progress even though I have a brain that has to fit in a skull. So there are three goals, I think of these as my three main goals when I'm doing a software project. Abstractions: precise, elegant, reusable. Implementations: correct, efficient, maintainable. Documentation: clear, simple, accurate. I'm gonna talk about precision a lot, and it's not that I think that precision is an end to itself, it's not. It's really these other things, like elegance and reusability. The reason I harp on precision is because it is so easy to fool ourselves and precision is what keeps us away from doing that. My implementation I'd like it to be fast, right?, but I don't want just fast, I want it to be correct, I care about what I implement. By writing no code I've written a very fast implementation but it's not gonna satisfy any interesting requirements. And of course, I want my program, my implementation to be maintainable, to sort of capture and hold my insights so that somebody coming back at me later can understand what I understood in putting them together. And then the documentation should also be simple, clear and accurate. So one way to make things simple is to make them inaccurate, I can tell you a simple story that isn't true and at some point I'm gonna harm you by doing that. So I want to tell the truth and to do so simply. Conventional programming is precise only about how, not what. So, I'm gonna say something, maybe a little jarring which is the commonly practiced state of the art of software is something that is, it's precise about how not about what. And I think maybe it's a historical accident, it's kind of the nature of our tool, our computer tool that we have to speak to it unambiguously, but it's only a tool for how, only a tool for sort of half of what we are talking about, or some fraction that we are talking about. And it imposes requirement on precision on us, on ambiguity, and the what , which to me is much more important than the how , it's a more fundamental part of the game, the tool doesn't impose it on us. And precision is hard and hard becomes unpleasant, so I think just for this kind of historical accident we've developed willingness and abbilities to be precise about hows. It is not only not right, it is not even wrong. Wolfgang Pauli came up with this description which I think applies to most software development. So it's not even wrong because there's no clear question, it's like an answer without a question. So that's what I would say: software without a specification is an answer without a question. Is it right is it wrong? We don't even know what it means. It's not even wrong I would say. And why does this matter? I love this quote from Bertrand Russell Everything is vague to a degree you do not realize till you have tried to make it precise. So Bertrand Russell in case you don't know, devoted years and years of his life, this was his life mission, to make things precise that people didn't really understand vague. So he found himself in the european mathematical community and was uncomfortable with the level of precision that was accepted in the day and he started poking at you know, professors and other practicing mathematicians, and they weren't really very receptive. They didn't like having it pointed out that the conclusions they were drawing were perhaps not sound, as they were based perhaps on unsound foundations. So what does it have to do with software? You might not want to write down a clear specification because you're comfortable, you think I basically know what I'm doing, I've got this kind of assignment, I've had a chat with my coworkers or my boss, or just internally in my head, I basically know what it's for, now let's get to the real work and do an implementation. All I suggest is what Bertrand Russell points out here which is that if you think you know what you're doing but you haven't made precise, you're probably wrong, or you're very likely wrong because the imprecisions don't come to the surface until you really try to make them precise. So don't be reasurred by a kind of informal like, ahaaa.., I pretty much know what I'm doing, it's time to go on. This applies particulary to computer programming, and there's this old,old quote by Demosthenes: What we wish, that we readily believe. And as a software delevoper, when I'm in the mode of trying to get something done, which is a lot of the time, sometimes I'm like trying to understand, or I'm trying to experince art or something, when I'm in the mode of trying to acomplish a goal, then, then what? The idea that my program might not be correct, or I might even understand what the question means, is it odds with my desire to move on to the next thing ? I wanna take a break, I wanna raise, I might just wanna do you other things that are more interesting, so I wish that my program were correct. That means I readily believe that it's correct. So I'm suggesting we have a built in bias to ignore the possibility that we don't even understand what the question is. So not even is it a possibility as Bertrand Russel pointed out that there's an important vaguery hidding but also I think there's, we have an internal psychological bias that makes it quite likely. So that's my pitch for precision, not because precision is important in itself, because it's a way to help us not fool ourselves and we have a tendency to fool ourselves. I prefer interactive talks to monologues even if I'm a monologist . So absolutely, questions whatever, please pipe up.","title":"Conal Elliott - Denotational Design: From Meanings to Programs"},{"location":"talks/#denotative-programming","text":"Peter Landin recommended \"denotative\" to replace ill-defined \"functional\" and \"declarative\" Preperties: Nested expression structure. Each expression denotes something, depending only on denotations of subexpressions. \"... gives us a test for whether the notation is genuinely functional or merely masqerading. (The Next 700 Programming Languages, 1966) So, we are this community that we call functional programming and that term has been around for quite a while. In 1966 Peter Landin has published a seminal paper, The Next 700 Programming Languages , and in this paper he points out that this term that was being used even at the time, functional programming, is really ill defined, it's this fuzzy term along with non-procedural I think is one, and declarative is another. And if you've ever used the internet :smiley:, particulary programming language communities,programmers blogs, that kind of thing, you might have seen people arguing over whose language is more functional, is Scala really a functional language, is Lisp or Scheme functional, is Haskell with IO functional? I've poked around that one. And what I see is that it's easy to start with these conversations and it's very hard to get anywhere helpful. And I think that Peter Landin explained in his 1966 paper why that was, and it's because these terms are not well defined. And he recommended the alternative, and this is what I recommend: I would say, deprecate the term functional and let's talk about something that has clear meaning and then revisit all these conversations if we still want to revisit about whose language is functional and whose not and how to be more in that direction. And Peter Landin suggested the term denotative , defined by having three properties. One is that, we have a nested expression structure, so it's an expression within a expression. And the second is that every expression denotes something. So now we are really getting to the heart of things, it's not just syntax, so structured expressions is syntactic property, but that expressions mean something, denotes something, and that something depends only on the denotations of the subexpressions. So these are his three properties of what he calls denotative language, or denotative programming style. And these three properties, and in particular the third one he says, it gives us the test for whether the notation is genuinely functional, or merely masquerading. So that's why I sometimes I kind of poke around and say maybe that's functional but it's not genuinely functional. Question from the audience : When Landin is saying denotation and like today when I'm saying denotation are we talking about the same thing? Answer: Yes, we are. So this paper was published in 1966, there was early work by Chris Strachey, at that time, what he called denotational semantics , and Peter Landin, if you haven't heard Peter Landin, Peter Landin is one the few, really kind of founding fathers, founding parents of our discipline, and not just of functional programming but of understanding programming languages. So Peter Landin is extremely important figure in our history, and made contributions. For instance, Peter Landin is the one who realized: \"Hey, wait a minute, lambda calculus and programming have something to do with each other.\" Peter Landin figured out something called SECD machine , that's something like, how can we actually evaluate lambda expressions with a machine, OK? So this is soo, we take this so deeply for granted. Peter Landin was on the ALGOL comittee, and that was where he noticed and he wrote a famous paper called A Correspondence Between ALGOL 60 and Church's Lambda-Notation , that's where he was realizing. This is the seminal paper on the idea of domain specific embedded languages. This paper gave rise, I believe, to Scheme, ML and Haskell among other languages. So very infuential. So that's the idea of denotative programming and it's a clearer question. If you ask is Scala functional, it's a non-question, right?, because functional doesn't mean anything, but is Scala denotative ? Well, that's a much more interesting question. The question is can we give a semantics to is, in this compositional style, the meaning of an expression depends on the meaning of its components and if so, what does it look like and are we happy with the results? And if we're happy with them celebrate, if we're not happy let's steer.","title":"Denotative programming"},{"location":"talks/#denotational-design","text":"Design methodology for \"genuinely functional\" programming: Precise, simple, and compelling specification. Informs use and implementation without entangling them. Standard algebraic abstractions. Free of abstraction leaks. Laws for free. Principled construction of correct implementation. So denotational design is a term I came up with a few years ago, and it's a subject of this workshop and here's a little pitch, a little commercial, so, denotational design, I think it's like a wonderful, natural fit for functional programming, particulary this programming in which we understand specifications in a clear way, and it has these properties, I'm claiming. It gives us precise, simple and compelling specifications. It addresses both the use of a library, the language so to speak, and it's implementation, but it doesn't entagle them. So, it's entirely clear about what the language means, what that distracting us with the implementation and concerns, and it entirely defines what it means for an implementation to be correct or not. It's correct if and onlf if it's outcome agrees exactly with the denotation. Doesn't mean they have to work at all similarly, in fact, denotations don't work, they just are. Implementations do , denotations are . It relates nicely to a set of abstractions that have come out of mathematics for last few hundred years, which we have also been using in some modern typed programming language like Haskell and Scala, so standard algebraic abstractions, things like functor, monoid and applicative, and things like that. Foldable, traversable, monad... And if you follow this principle, where ever you follow this principle, what I am suggesting to you, and if I don't convince during this talk, I want you to challange me on it. So, what I'm claiming is that, where you apply this principle you do not have abstraction leaks. And another way to say this, I mentioned earlier today, is that, if you try to apply this principle and you reach a point where it just doesn't work, you have an abstracion leak, and that's a very useful information. It tells me where my abstraction leaks are, I redesign, I eliminate the abstraction leak, and to do it by finding a way to use this principle. And these abstractions are not, so, a standard abstraction like functor or monoid, it's a set of operations, but those operations have properties so there are laws. If you use this methodology the laws will hold. You cannot use this methodology and define an instance in which the laws do not hold. And sometimes to give you examples today, sometimes this process will also lead you to a correct implementation, in addition to giving you a specification that you can use, you can derive a correct implementation. It may not be the most efficient implementation but definitely it will be correct, and if you have a more efficient implementation in mind then it offers a path, or it suggests there is a path to look for, how do I get to my efficient implementation, from this specification, by sort of, correctness (?) steps, how do I kind of calculate, even though if it takes some ingenuity and creativity.","title":"Denotational design"},{"location":"talks/#broad-outline","text":"Example, informally Pretty pictures Principles More examples Reflection So this is what I wanna talk about today. I am going to give you one example that I intend to give rather slowly and thoroughly, and it is a kind of, something I want to invite you to participate in. So this part is not so much a talk, as a seminar or something like that, for us to investigate together how we might do a library design, and the library design in particular is one for inventory, for image synthesis. Something based on some, example came from something I did around the year 2000. So we will go through this process and I hope you get a feel for what it is like for me to do, to apply the denotational design, and hopefully fun example. And then I'll show you some pretty pictures, which, you know, maybe I'll show you the pretty pictures first and then we will reflect and say what are the principles that we used, and then I have a few more smaller examples, hopefully to give some sense this isn't just a one-off thing. And absolutely, discussion, participation, questions, objections, moral indignation, what ever you have, I invite it. And I'm explicitly not asking you to believe anything I say today . OK? I don't want you to believe anything I say because I say it, I want you to try it on and see if it, how it feels to you, which is what I did. Example I want to talk about is image synthesis and manipulation. OK? That's the assignment. It's either your hobby or you just joined the group and that's the goal. We want to have some kind of groovy, a flexible fun to use library for doing image manipulation. How to start? What is success? We want it to be efficient to, but I don't think I'm gonna get into that, and in fact my library that I did this is extremely efficient, it runs insanely fast. So couple of questions is, where do we even start? Like, we're this team, a product development team, and this is our edict, image synthesis library, high-level, fun to use. So how do we start, how we define success? Any thought on how we start on this project? Assuming some of you are programmers :smiley: audience: \"What are the things we are manipulating with?\" OK, yeah, so what are the things we are going to be manipulating, so, imagery, well, let's get more specific than that, OK? So what kind of things, and then what is success. Well? That's probably very tightly related to this question, so what kind of functionality, so when we implement this functionality and provide it, conveniently, correctly, maybe that's success. We might understand better these question when we come back to them. So, what is we are trying to build? I'm gonna suggest, we can do this brainstorming, what kind of operations, (...) think this phase is terrific and illuminatig so I just kind of threw out a bunch here. So this is a kind of function that I would imagine would come out of either you know, kind of a clear requirements statement or a brainstorm.","title":"Broad outline:"},{"location":"talks/#functionality","text":"Import & export Spatial transformation: Affine: translate, scale, rotate Non-affine: swirls, lenses, inversions, ... Cropping Monochrome Overlay Blend Blur & sharpen Geometry, gradients, ... So some way of getting stuff into the system manipulation out the system, so it's purely synthetic imagery it's gonna be nothing interesting at all coming in from the outside. If I'm gonna do some kind of manipulation of given imagery well I'm gonna want to import something from maybe some standard file formats, and I'm gonna do all my manipulations, and then at the end I want to give something out, so maybe I have some kind of import/export kind of thing, and those are just the endpoint, now all the interesting stuff is in the middle. So one thing that comes up alot is spatial transformation so that can be things like translate, scale, rotate, OK? These are all linear, affine transformations they are called, but there are also interesting things like doing a twist in the middle or a bulge, or something like that. And I think I said I will show you some pictures so let's pretend that this is part of our information we get about what we are trying to do, let's see, conal.net/Pan/Gallery , so here are a bunch of pictures I made a long time ago with this language, but not really, these aren't the pictures that I describe, these are croppings and samplings of the pictures I described, so these are things I know how to put on the web, these are the kind of things that can be exported, and each of these is a link to a much higher res version. OK? So this is like a playful exploration of image synthesis. All of these are kind of collections of lots of examples. These maybe give you some sort of feel, as you can see, it's not toward kind of practical problem solving, advertisements or something like that, you know, more playful kind of stuff. All right, In some of these examples, there's kind of bulges that happen, like a magnifying glass effect, and some there are spiraly things happening which you might think of as, I have something linear and then I wrap it into a spiral, infinitely out, infinitely in. So there are all these interesting non-affine, and then there is turning things inside out, makes beutiful pictures. And there is cropping. I've got some image, and I just wanna like maybe crop a circular region out of it or some funny shape, there is this monochrome image, it may sound kind of silly, but it's actually quite useful, I may just want the images red, just red. Now that's likely to be my final output, maybe I'm a modern artist or something, but still likely even in that case, but combined with cropping and overlaying and things like that it's useful. So another operation is overlay , if I have two images and put one of them in front of the other one, so that shows up quite alot, reality in our image systems are doing that all the time, by looking at here there's all kinds of overlays happening. And then there is blending . I have two images and I want some sort of blend between the two, maybe some kind of linear interpolation, or something like that, some midway point between the two, and then there are area operations which are like blurring and sharpenning so I might have an image that I want all of it or some part of it to be blurred, or I might want to exaggarate trasitions sharpenning it, and there is all kinds of other toys, various geometric things, funny shapes, and then smooth gradients and things like that so, these are the kind of things that we'd like to be able to express.","title":"Functionality"},{"location":"talks/#api-first-pass","text":"So we're gonna define an API that's gonna be used in some language. We get to choose a functional language, maybe we are told to, maybe we get to. So, I wanna talk about designing functional APIs, so what kind of things show up in a functional API? I don't mean what kind of functionality, I mean... :smiley: Questions: So the observations that transform are a monoid. Answer: Yes, transforms are a monoid in an interesting sort of way, and I want to suggest we come back to that when we think about how can we elegantly structure. So in a functional language, in an imperative API the operations are gonna be about how to make up state, initialize it, and side effect it. OK? I've looked at alot of imperative imaging API's and they are like that. Build up some graphic state, set some brushes, you know, this kind of stuff, modify some output that's being accumulated, maybe it's on the screen or an offscreen buffer. So it's all about creating, initializing, modifying state, releasing state, that kind of thing. Functional API's are not like that. Functional API's, we want to describe things that are , and then building blocks for combining them. So we're gonna have instead of state and operations on it, we're gonna have types and values of those types and values of those types, just because that's the kind of programming that we do. So a good question to ask is what types are gonna be involved? OK, we don't yet have to know what those types mean, we can be a little fuzzy about it, because this is a process of clarifying our ideas. type Image over :: Image -> Image -> Image transform :: Transform -> Image -> Image crop :: Region -> Image -> Image monochrome :: Color -> Image -- shapes, gradients, etc. fromBitmap :: Bitmap -> Image toBitmap :: Image -> Bitmap This is an example of taking the previous slide and starting to give it some concrete shape, some concrete specific shape and again, so far it's just the API, it's not what anything means, and certainly not how to implement it. So we might have a type Image , so imperative API's don't have a type Image or if they do, they mean something off at of a file, or something like that, they don't mean it in compositional sense. But we instead of composing side effects, we're composing images. It's a functional language, so the idea of rendering or displaying or updating screen is not even part of the API, we're just talking about what the images are. So in each of those operations I mentioned in the previous slide, it's gonna show up somewhere as an operation with a type. So overlay for instance, is something that takes two images and gives an image, and in imperative API you dont' see overlay. You see a couple of pieces of code that do side effects, one of them being executed after the other, that's how overlay shows up in an imperative API, and our API overlay is an actual operation in it and it is about, or the expression of overlay of two images, it doesn't mean it goes anywhere, it doesn't mean it modifies anything yet. For instance, we might never use the result, or we might use it more than once, or we might transform it somewhere before it shows up. So that's overlay. And then there's transformation. If I have an image I might want a bigger version of that same image, or a version somewhere else, or a version that's been interestingly warped, so that's what transformation does, that's what transformations are for. Makes sense so far? Cropping also, we want to crop an image, what we need to describe some shape, I'll call that a region, so, what do I mean by image, what do I mean by region, what do I mean by transform, all this is gonna become clear, that's the creative process. And then, a monochrome image, I just give it a color, and I get an image of that color. And you might expect me to say, a color and how big it is or something like that, but we'll come back to that question if we have enough argument here. question from the audience: You're wondering why monochrome doesn't say give me a color and an image and I'll give you an image? Aha.. What would that operation intuitively, what would it mean informaly? Oh! I see, I have an image, and I want you to make that image red or something like that. Yeah, that's interesting. So here's a question which we kind of put up in the air, can we describe that operation, can we build it out of simpler parts? One of my goals, I'll make that explicit, is I want to define the simplest possible building blocks. If I have something that is somewhat complex operation I'd really like to make it not be permanent but definable out of simpler pieces. Why? So that I have a simpler set of primitives that I have to figure out what they mean and how to implement them. Like Robbies talk this morning, his keynote about this macro system, it's very nice to have a small set of language primitives, this is part of the ethos of Scheme, very small set of language primitives and then the abbility to define richer things in terms of simpler things and it means that my core is quite simple and implementation of those more complex can be quite simple by translation or if you want that specification you can more efficient implementation(?). question from the audience: So already some critique of this API, overlay , that sounds a bit ad-hoc, aren't there other way to combine images besides this one, and maybe this one can be built out of something simpler or more general. Yes, and I completely agree. So, what I am doing here, and absolutely welcome what you are doing, you're getting a little ahead which is fine, this is participatory, this is your talk, this is our session together, so we could, throw these all ideas up, and then critique them or we could critique them while we throw them up, it doesn't matter. There may be some personality issues, like I'll get really offended if you challange my idea or something but I actually won't so we don't need to do that. Yeah.. So already overlaying sounds suspiciously specialized. And in this last two we're just getting the information in and out of the system. But it's important, I'm not assuming that Bitmap and Image is the same thing, so what I am assuming is that there's this thing called bitmap which is like a jpeg, or who knows, it's just this kind of array of pixels which you know, have some kind of depth and that kind of thing, assuming there's going to be some stuff outside that we're going to want to bring some information in, do cool stuff, and then also after we're done with cool stuff, we're going to put out into. But I don't want to assume that that's what we mean by Image , and we will discuss there are very good reasons to not make that choice. How to implement? Now let's talk about how to implement. :smiley: wrong first question Let's not talk about how to implement, OK? So, I enjoy implementation, I am guessing you enjoy implementation or you wouldn't be here, but it's not a good first question, in part because it's not a well defined question. How is an answer, what is a question. How do we implement what, how do we implement this, No, we don't even know what this means, what's an image, these are just kind of intuitive and formal ideas, so let's be more clear about what these mean and then ask the question of how to implement. Does that makes sense, or let me put it differently.. Is that? Is there anybody who will like to go more into, more about the separation between why we would wanna make something more clear other than how to implement it at this point? I don't hear it talked about a lot anymore this distinction between specification and implementation. Question from audience: Identify symmetries? ... Ah, are you asking am I sort of content with this API? Is that what you are asking? I'm just making a guess, I don't think I'm right, I'm just trying... How they would be used? ... OK, by symmetries, do you mean, hm, maybe there's some structure that I realize I could refactor? Or... do you mean something like visual symmetries, I'm guessing you don't but maybe you do?... Oh, yeah, yeah, understanding maybe the set of images that are expressible in this form? :smiley:, I think I'm not converging, but I'm roughly, infinitely patient, (Smiles) about understanding what you are wanting to say. OK, ok, come back to it, and say oh! now I know how to get this through your thick skull Conal. So, how to implement ? NO (:smiley), different question, OK? What to implement? OK, and we did not just say what to implement in a precise way, so let's get more precise, so. What do these operations mean? What do I mean by mean? There's a more fundamental question which is what do the types mean. More centrally: What do the types mean? Once I know what the types mean, then I can at least understand the question of what do the operations mean, because the operations say, over takes two images and gives an image, we can't possibly talk about what over means without understanding what an image means, right? Because it's a function on images and gives an image. So this is the central question. Doing the API design we're going to define some types, types of values, because we are doing functional programming, and type functional programming in particular and we want to say what do the operations mean. Question from the audience: OK, let me see if I got you, and I want to encourage people to sit closer, for two reason, one is to be more included, and the other is for me to hear you more easily. If you don't want to do that I'll just try harder to hear. So, I think what you are asking is, when you say mean , do you mean how to implement them, and if you don't mean that, what do you mean? OK, thanks. Yeah, I absolutely do not intend to say how do we represent the types or how do we implement the operations, so that would be confusion. What I really mean is to clearly specify, I want to give some formal specification, somehow, that tells me as an implementor exactly what it means ... It's not that it tells me whether my implementation is correct or not, it tells me what it would mean for an implementation to be correct or not, that defines correctness. Then I know what to aim for. OK? It also tells me the breadth, so I want to be constrained enough to know what correctness is, I don't want to be over-constrained because I don't want to be told how to implement something, because I may have a lot of ideas, and here I'll maybe touch on some of the ways I've implemented this vastly different from each other, OK? So I don't want to be constrained in an implementation, or representation. I do want to get very clear, about, something about the specification, and there's a particular style of specification, and that's the one I'm talking about today, which is denotational . And I'll show you supersoon what that looks like. I am so sorry that's freezing in here, right? I think it's even worse for you than for me there. It's blowing here, yeah I don't know... So I appreciate you hanging in, if you have to leave, if your face turns blue, I won't be offended. So, central question for me is what types are there and what do they mean? . What is a mathematical model that I can use to think about those types in a precise way. So here, so far, the central type of interest is Image . There were some other types, transforms, colors, regions. So let's start with one, let's start with images. So what is an image? Specification goals: Adequate Simple Precise Before entering that question what do we want out of an answer and I am suggesting these three properties. I want my mathematical model to be adequate, if it's not adequate then I can't express the things I want to express. Either I want to or my users. It has to be simple. Why does it have to be simple? It's because I'm basically not very smart, I mean, I mean I'm smarter than a mouse, right? But I'm hopefully not as smart as my children will be and so on, and all of us collectively so I have very limited ability. So I need the specification to be simple enough that I can reason practicably and reliably. And then precision. And why? So that I really have simplicity and adequacy, not just think I do. So precision is all about helping me not fool myself, because I'm psychologically inclined to fool myself, so precision is a coping mechanism. OK? Now, let's go back to this question what an image is.:smiley:. This is a collaboration. What is an image? OK? And I want a lot of bad suggestions, I want lots of stupid ideas, what I really mean is, you know, please don't be shy about making suggestion, because every suggestion is illuminating either for it's strengths or it's weaknesses or both, and I apology for flashing my answer which I didn't mean to bias you with. When I've done this exercise, people have quite a variety of intuitions. It's what? A matrix of pixels? Sure, totally. So by matrix of pixels I assume you mean some kind of rectangual, regulary spaced collection of pixels and a pixel is... yeah, colors or maybe something that represents like 24-bit values to be interpreted in certain way. Yeah, so that's a very common answer, so it could be an array or matrix of pixels. Other? Oh, OK, so already we are getting criticism from the very start, programmers.. yeah, so maybe there's a little bit of a discrete bias here that we might wanna question. (Listening to a question)... OK, so it's sort of functioney view, yeah, and last year I talked about this before I talked about FRP, and so nobody made that suggestion but that's certainly one possibility as absurd as it sounds. :smiley: Anybody else? You'll see I'm joking, hopefully at least... A way of collecting and saving light? Sure! Yeah, absolutely. And so we might think that it's a way, hmm, that might being an algorithm, a function, a recipe, something like that, yeah, so, not to hurl insults, but an object oriented sort of perspective might be about it's an interface that has some kind of operations.. yeah, other thought? Yeah, sure, so an image might be represented by a collection of shapes, so for instance, in something like a pdf or postscript, yes, so pdf and postscript are all about image synthesis and manipulation, and they are all about geometry, so it could be some shapes, in other words some descriptions and then... totally, yeah (seeing?) graph, or it's called linear that's all that was. No, I worked in computer graphics, (shiver of disgust?)... so yeah absolutely, It could be some kind of data structury thing which contains in it some kind of descriptions of objects, 2 or 3d dimensional, some description of lights and cameras, kind of some information out of which one can synthesise an image, and a lot of particulary 3D API's are that way... OH! That sounds like a poets answer to me. So, Dicks contribution is, yeah an image is a short line or a short body of text that conjures a what in someones mind? Oh, conjures a picture in the readers mind, I see. I might recursively ask you what a picture is and wonder if you're gonna tell me it conjures an image in readers mind but I won't do that. Thank you. Yeah, absolutely, so again it's something that's like evocative in a sort of more poetic end of the spectrum than like a 3D scene graph API kind of a thing... Cool, so here is an interesting perspective is that maybe we were a little too focused on color, so an image is being sort of color over the discrete, continuous and finite, infinite whatever space, maybe there are other things as well that we maybe want to array over space. Maybe it's the overspaceness that matters more. Yeah, OK. So we can take all of these suggestion, if we are doing kind of classic brainstorming write them up on the board. We have our goals which I suggested here and we can evaluate them all and see where they lead. If we leave out precision we are not gonna get a very accurate estimate of where they lead, because for instance, simplicity, many ideas are simple only because they are not precise and when made precise they become quite complex, or their complexity becomes apparent. It's cold, sorry, god.. So my personal answer which especially if you've been listening me talk about FRP is perhaps somewhat perdictable, so my answer is it's an assignment of colors to 2D locations and I'll be more clear what I mean by that. My answer: assignment of colors to 2D locations. This is sort of the same answer in a less specific form than an array or a matrix of pixels. So that's, you know, an array it's an assignment for every slot in my array I assign one color to it, but it leaves kinda little more open ended what the shape of the container is. How to make precise? type Image If we start with this intuition, and then we want to make it precise so that we can understand the implications of it, we can do this to every idea that we've talked about, and some of them we might learn very early what some of it's limitations are, and that's an extremely fruitful part of this exercise. So let's make this precise, and here's the form in which I do it and this is the denotational form. mu :: Image -> (Loc -> Color) So we have a function, I'm calling it mu (greek letter), mu is supposed to suggest meaning , m right? So mu is a meaning function and the idea is that I want to say what an image means, or give you a model for relating to images, by, it's a mapping so mu is a mapping from images to something, and that something I'm saying is assignment of colors to locations and there's a simple precise to say that which is a function from location to colors. OK? Now over here on the right, so on the left of this main arrow, is the type we are interested in ( Image ), on the right is another type ( Loc -> Color ), for the most part you can think of that type as being, what we mean by functions in Haskell, what I really mean is mathematical functions, which is what we mean by functions in Haskell, but it's not what we mean by functions in most other languages, like an ML function, a LISP function, a Scala function, is it a function because it has side effects? I mean really a mathematical function. So you can either think of it as a math function, or you can think of it as a kind of a function in a purely functional language. But it's kind of important to make that distinction because it doesn't have to be implementable, OK?, because we're never gonna implement mu , we're just gonna use it to understand our library design and evaluate it as it emerges. What about regions? OK, so but Image was just one type, and I'm not saying that you should somehow know why this is a good choice, we're gonna look at it, I think. And then we can examine alternatives. So what about regions? Well, anybody have an idea what a region might mean? Locations to what? Ah... to opacity? Interesting, OK. So we might also talk about opacity over space, any other thoughts? Oh, a list of locations, yeah, that's actually quite common, used, in awful image, I don't mean this is an awful suggestion, what I mean is it so happens that a lot of the image, strike the word afwul, that choice shows up in a lot of them, often in a kind of compressed form, it maybe like a set of rectangles, that's quite common?... A function from location to boolean, quite similar suggestion as well, yeah, OK, same suggestion. And then... yeah sure, (?) polygon, so I might have some primitives for describing my regions, like polygons, and then I might mean a bunch of them, a union of them, perhaps countable, interesting choice, countable... mu :: Region -> (Loc -> Bool) So this is my choice, this is one of them, which is it's a function from locations to booleans, if you known this. Function to boolean is isomorphic to another mathematical notion that we use a lot, I hope you know this, which is sets. So function from location to boolean is the same as set of locations, something called characteristic function if you took math a while ago like I did. So another model is to say a region, the meaning of a region is a set of locations, that's very similar to the suggestion of a list of locations. So we could also say it's a set of polygons but a set of polygons is a more complex, it's maybe more efficient, but uh wait a minute, we're not talking about efficiency, that's a non issue yet, we may want to represent out set by a collection of polygons for instance... OK, let try to get what you're saying, so one was an idea and the other was a kind of motivation for it I think. OK, so I might say that a region is a function from, it's something that takes an image and a point and, an image and a location and says, it either says there's nothing here, or there is something here and this is what color there is. OK, so that would be some kind of relationship between like a region and an image right? I would hope that something like that because it involves both could be describable in terms of simpler pieces, let's see if that's the case... Oh, I see.. So let me try and tie this idea. Suppose we had a notion of a region which is a set of locations, in other words, a function from locations to bool, I mean, maybe that's not obvious to everybody. So the equivelancy or those two notions is, if I have a function from locations to bools I can think of it as a set, a set is just a question of are you in or are you out, and that's what the question that location and bool answers. So if I have a set I can form a function, just by saying are in the set or not. If I have the set I can form the function, but by asking the question given any location, are you in it, and if so yes, if not no. So given this simpler notion of regions, yeah, we can I think take that region, aha.. interesting, OK. So we can ask questions like, what is the set of all points in space that have some color, or what's the set of all points that have a particular color, I'm not sure we get to the expressiveness of what you're suggesting, that's interesting, we might come back to it. Ok, so here are a couple of models and this is the style that I mean of giving a meaning to image. Once we have, once we choose some, now we can go back to the operations and say what do they mean on the meanings . How do the operations work as if they were on meanings of images rather than on images themselves. So an image is going to be some datatype, it's going to have some representation, some runtime existance, the meaning is going to be a mathematical model, and have no runtime representation, no runtime existance, but it'll tell us what it means, what these operations mean, it'll give us a specifications I hope we'll see.","title":"API first pass"},{"location":"talks/#specifying-image-operations","text":"mu (over top bot) == ... mu (crop reg im) == ... mu (monochorme c) == ... mu (transform tr im) == ... Aaa, thank you, why didn't I just not use set? Hmm.. I haven't thought about that question in a while, and absolutely I find myself explaining this idea of region as a characteristic function in terms of the set, so I think that tells you and me, now that I think about it, that a set definitions, it feels a little more natural, a region is a set of points, that's it. So, let me change my mind, and say a region is going to be a set of points. Then I'll change my mind and get back to this one. OK? That does seem more natural, let's do that instead. So I don't mean something of type set in Haskell, for instance, which is finite, I don't even mean countable sets, because I want to say like, a disc is a region and it has uncountable, well, I'm getting ahead of myself, it's got a lot in it, so yeah, I mean set in the mathematical sense. So now given this model... yeah absolutely... OK, so there is awful a lot of sets, :smiley:, an awful lot of sets, maybe more sets than I really am interested in using, there's definitely more sets than I can possibly express in any language, right? That's one of the things, a corollary of Cantor's work in the 1800s. Yeah, sets seem maybe awfully ambitious, or awfully sloppy when you think about it. I really may want a much smaller, yeah, so am I tracking? Yeah, so one of the tradeoffs I make is I might make the denotation less precise, in another words have room for a lot more than I can express in order to make them simpler. And we might come back to that if there's interest and talk about that tradeoff... Interesting, so will we need some properties of sets which is going to restrict the kind of sets we can talk about, for instance, measurability, and measurability might be quite relevant to our ability to render something because we're going to render in pixels, and pixels cover entire areas, yeah, and if we want something like the integral of a function over the area, or something like that, then measurability is going to come into play, some form of measurability. So I don't know, we'll see, great question. (56.56)","title":"Specifying Image operations"},{"location":"talks/#brian-beckman-dont-fear-the-monad","text":"What I want to do is take the mystery out of monads. Monads without mystery or maybe monads without misery . Because if you've looked at link(?), or you've looked at F-sharp, or a much smaller number have looked at Haskell, you've encountered monads and they're scary the first time you encounter them. The main reason they're scary is you read the papers and they tell you oh, well, you need to know category theory before you can understand this, and while that's strictly speaking is true, it is not true that you need to know category theory to understand how monads work, what they're good for and even better yet, how to creatively use them yourself. Let me talk about foreign languages for a second, if you learn french or something, there are sort of three phases that you go through, the first is you hear french and you say, oh my goodness I'm scared to death of this, the second is, you learn a little bit of french, and you can recognize french words and you can read a little bit with a dictionary, and then the third and final phase is you can use it creatively in your own work, just speak freely. Those are the stages of learning any kind of new foreign concept. Monads are the same way. Everybody goes through this the first... The first encounter is horrifying, there is this strange concept and are you telling me I need to understand this strange bizzare concept in order to do anything with functional programming. Well, yeah the answer is yes, but what I want to take out of it is the strange and horrible part. It isn't strange and horrible, I'm going to explain it in terms of things you already know, I promise. Question: What the hell is functional programming? In three minutes or less, just quickly so that people can get to speed as to, they can then put the pieces together why monads would make sense in the context of functional programming? Not only do they make sense, they are a natural outgrowth of functional programming, you would invent them yourself if you've worked at it long enough, you would know the category theory, you would know the column monads, you would just call them better functions, that's what I am going to show you, but functional programming in a nutshell, is programming with functions. Yeah, right on... But normally, let me start scribbling on the board. Normally we are used to doing things like this in say, in C#: int x: static int f(nt x) { ... y = Math.Exp( (double) x) returns y; } Int x , and then we'll go some place and say, just for simplicity let's make everything static, we're inside some class here, right? And we say, static int f(int x) and it does something with the x , and it returns, oh maybe returns some y that's inside a function, let's make it do something, like y = Math.Exp ( (double) x) , it's already starting to look like a lot of stuff, but the important part is, we have a function f (int x) that takes an int as an argument and returns an int as a result. Nothing scarry there, functional programming is taking these guys ( f(int x) ) and saying not only are these guys not scarry, they're so ordinary that we're going to treat them just like we treat these ( int x: ) guys. These x: guys are data , these ( f ) guys are code , in functional programming we just say, well code is data too, we're not scarred of it. Now, let me introduce a little bit of notation, I'm gonna stop writing C-sharp, this is where it's gonna get a little intense, but I promise it will all be things you already know. Great! . If you know a function, you know everything you need to know to understand monads. Question: So, look, a function takes input and... but always returns something right? That's what function does, it does stuff, gives you functionality. It takes an input, and converts it into an output. That's your bread and butter, but the bread and butter is you just start tossing these ( f ) things around as easily you toss these ( x: ) things around. In ordinary programming, C-sharp, imperative programming, we think of data as being in one world, and then we think of these functions as stuff that operates on data, and they're special. In functional programming we say they're not special, functions are just like data. Let me prove to you that a function is like data. Every imperative programmer goes through this phase of learning that functions can be replaced with table lookups, often you do this for performance. You want to create the sine function, or the cosine function, make a big table and interpolate in the table, you don't actually sit there and do... everybody learns this trick. What's a table? Data . What is the function doing? Is looking stuff up. The act of looking up is universal. You could convert any function into a table lookup and then the functionality, the code, is the same for all functions, and the function is just data. Hmm... interesting, there's your proof. That concept alone should convince you that functions are of the same kind of thing as data. They're not special, you don't need to have special code for every function, since it's possible theoretically to convert every function into a table lookup and to have the evaluation part, the code be completely generic. Functions and data, data and functions, same thing. Think all nouny , no verby . :smiley: Question: So now, I mean, then I guess just quicky catch up, just so we get some of the confusion out of the way, when you think about the importance of say F-sharp, Haskell, definetly Erlang and the multicore world that everybody is talking right now, it has to do a lot with the notion of no shared state, no shared mutable state. That's right. Now, is that just a consequence, or a side effect of a side effect free language, like a functional language? Let's talk about shared mutable state and functional programming. Sure, so quickly. Pure functional programming, as in Haskell does not allow any state at all. Functions, once you create data, that's it, you created it, if you say int x equals 5 than x is 5 for ever and ever. It sounds like it's not possible to do much of anything useful with the language like that but in fact you can do almost everything you can do, you can't do shared mutable state, but you can simulate it with monads. Yes! Now we'll stop talking. No, you don't need to stop talking, because I want you to stop me any place where it's, where I'm starting to get confusing. I promise, and I will explain monads in things, in terms of things you already know and I'm going to go through four steps. One is Functions The second is Monoids. You don't know the word but you know what they are. Functions again, I'll come back to them and then Monads And this stuff is easy . It's conceptually very very simple, very very easy to explain, but you have to be willing to go with a little bit of a terse notation. Instead of saying int x: I wanna say x : int , and there is a very good reason I'm gonna do this. This phrase up here ( int x: ) says the variable x has type int . In mathematics you would say x is a member of the set of ints (x elem ints). Types and sets are almost exactly the same thing. I wanna write it with a colon here ( x : int ) because I want to put the types on the right hand side. This is an insertion x has type int . It means exactly the same as this. We are now leaving the realm of C-sharp and entering the realm of F-sharp, or the realm of functional programming when you see notation that looks like this. x : int Now let me write the type of a function. A function, remember how I wrote it before, this was ( int f (int x) ), now I have a very compact notation, I can say f : int -> int , a function f is a thing that takes an int and gives you an int . That's all it means, nothing more, nothing less. Next step. Instead of picking a particular type let me pick any type, and I'll use the word, the symbol a for any . x : a . x colon a means I am asserting that x , the variable x has type a . f : a -> a means I am asserting that the function f takes an a and gives you an a for a being any type. x : int f : int -> int ---------------- x : a f : a -> a Now, I'll go back to C-sharp for one last, one last little dive into C-sharp, this would be, you could say something like static a f <a> (a x)... . This ( f : a -> a ) is just short hand for generic types , stuff you already know, I promised you that wouldn't be anything you don't already know. We know types, we know generic types, we're not scared, we're almost to monoids. Let me add another function down here. This is a function g and I assert that my function g is a guy who takes an a and gives you an a . ( g : a -> a ) I now want to combine my two functions f and g . How can I combine them? Make them one function? Yeah! I can make them one function, there are couple of different ways of doing that, let me show you one. One way would be to say, take g , feed it a variable of type a , now I'm reusing the notation. The little a here ( g (a) ) means a variable of type little a . You forgive me for that, this is not at all confusing. This is, call g with a variable of type a and then call f on that result, f (g (a)) . That would be one way of combining these functions. Another way would be to call f first, f (a) , and then feed that result to g, g(f(a)) . Now, I'm going to take away the brackets, the reason is they are visually annoying. I'll leave this up here so that we are reminded that x has type a , f has type a to a , g has type a to a . In functional programming, we like to write function application, which means calling a function, function application, calling a function, sit in it. I promised you stuff you already know, nothing new. We like to write function application like this, g a . It looks like multiplication in ordinary mathematics. That's on purpose, because if g is a linear function and linear functions are special and happy and really cool, it is multiplication, so we make it look multiplication for any function, even if it's not a linear one. Not only does this get rid of brackets, but it reminds us that in the case of linear functions we have exactly this. Now, if I wished to, this is calling g with an argument of type a , now I write my combination, this time I need brackets, f (g a) and I won't go into this, the reason you need one pair of brackets here is because of currying . Currying has to do with functions of multiple arguments, I won't get into it today, but suffice it to say, if I chain a bunch of functions together, it associates to the left, f h g = (f h) g , automatically the brackets squeeze up on the left hand side in this kind of functional notation. This is standard in Haskell and F-sharp, nothing scarry here, just something you have to get used to. If you want to call g first and then f you have to write it this way. If you wanna call f first on a and then g write it this way, g (f a) . Let me now abstract, let's stare at this one for a second. f (g a) . Suppose, let me invent a notation, I'll call it little circle, (f . g) , f little circle g , this is a new function and the definition of this new function is that if I apply this new function f . g to the argument a , ( (f . g) a ), this is exactly the same as calling f with the result of calling g on a . (f . g) a = f (g a) This is a new function. I can now get rid of the a . I can't get rid of it on the right hand side, but I can get rid of it over here and I can give this new function a name, (f . g) = h . I can just say h is a new function, what's it's type? We know the type of g is a -> a , we know the type of f is a -> a . I took an a , I gave it to a g I get an a , I take an a I gave it to f I get an a . The type of this h thing has to be something that takes an a and gets an a . Look what we've done! (f . g) = h : a -> a . We've taken two things of the same type, a function ( f ) and a function ( g ), and we have created a new thing of the same type, h : a -> a . That is the essence of a monoid . The essence of a monoid, you have a way of taking two things of the same type, and creating another thing of the same type. This is an immenseley powerful concept, it enables you to create complexity starting with simplicity. I have a bunch of simple building blocks, I have a rule, little circle, for combining these building blocks in arbitrary ways, all it has to happen is the types line up and I get new things of the same type. Question: So let's step back real quickly and talk about perhaps the usefuleness of what you just did. So you took two function that take the same type, return the same type, you combine them into a new function h in this case. Let's talk about what that means in the context of usefuleness. This is the way to build complexity. Everyone knows that one of the biggest problems in software today is controlling complexity, we add features, we interface to other people's features, we try to extend our software to other environments, or other machines, we've got XML, we've got Ruby, we've got the web, we've got everything... And no matter, we find ourselves in this trap of extending our software constantly and it's not doing anything new it's just fitting to other stuff. We're out of control in the complexity space. The way to control complexity is compositionality . You step back, you create, it's a creative act, you step back, you create a design in which a number of smaller, simpler things... now, let me digress on the word simple . I use the word simple to mean small . Lot's of people use the word simple to mean something else, so I try to avoid the word simple and I just use the word small, because everybody agrees on what's smaller , but not everybody agrees on what's simpler. Some people use the word simple to mean more explicit , broader, some people use the word simple to mean more conscise , more abstract, that's why I avoid the word, but everybody agrees on what's smaller. The creative act of designing something, designing a software artifact in which it is easy to control complexity is the creative act of creating functions that can be combined this way, this is called function composition , little circle is called composition, and compositionality or composability, this is the way that you build up new things from things you already have. And remember I began this whole talk by telling you I'm gonna explain monads in terms of the things you already know. How am I gonna do that? By combining or composing them :smiley:. Question: *Now you've used the composability word, and we've talked about that on channel 9 a lot. And it's imporant, it's going to increase, as software get's more and more complicated, we need to have more and more control over the complexity. We need to be able build up big spaces, of rich software, but starting from simple building blocks. And the way to do it, the way to do it, is to ensure that the types line up, and to create yourself a generic composability operator that would allow you to take any things that are in your set of types, and combine them and create other things, new things, then are guaranteed to be in your set of types. Remember, we just calculated the type of h , we didn't have to assert it, it was implied by the fact that the type of f was a -> a , and the type of g was a -> a , and the little definition of composability operator which remember was, take g give it an a you get an a , feed that to f you get an a . Therefore the type of f . g , f composed g must be a -> a . There's no choice. Here's the beauty of this. If you discipline yourself and force yourself to put all your functions in this little type universe, then you can create new ones by composing them and be guaranteed that they will fit in the same type universe. You can't make a mistake, if you buy into the discipline. Question: What that means is you can change components anywhere in the system without destroying other parts of the system. Glue them together, mix and match, make anything you want, you can start with a small number of things, and end up with a large number of things, and they're all just correct by construction, at least in the realm of types. Now we're in the world of monoids. I'm going to give you a quick example to prove to you that you already know what a monoid is, and the example is a clock. 12 11 1 10 2 9 3 8 4 7 5 6 You already know what a clock looks like. A monoid is a set of things, plus a rule for combining the things, and that rule obeys some rules. That's it. You already know. Can you say that once more? . A monoid is a set of things, or a collection of things to be more precise, plus a rule for combining the things, and that rule obeys some rules. Just two levels of abstraction. And clock, the numbers on a clock form a monoid, the rule for combining them is: take one number x , add another number from the clocl y and then cast out 12 s, mod 12. (x + y) mod 12 . Everybody doing C-sharp or vb is familiar with mod operation. Now you could take any two numbers, I can take 7 and 10 , add them up, cast out 12 and I get 5 . In the monoid of clock arithmetic, 7 plus 10 is 5 , because it's 17 minus 12 is 5 . The rule, this is the rule ( x + y) mod 12 ), composed of two things, and the rules that the rule has to satisfy are the following: (+) (circled plus symbol) This little symbol stands for add the two and mod by 12 the constant.( combined ) The rules are: x (+) (y (+) z) is always the same as `(x (+) y) (+) z. In any monoid the rule for combinind things must satisfy this meta rule . This meta rule is called associativity , it means doesn't matter how you group the applications. The second rule is: The monoid must contain a special member such that x combined with the special member 12 is always x and the special member 12 combined with x is always x x (+) 12 = x 12 (+) x = x In the case of our clock the special member is 12 , if you add 12 to any number on the clock, you get the same number. 3 o'clock, add 12 hours, it's 3 o'clock again, am, pm we're not taking about that, we're talking about a 12 hour clock. Got it, absolute number, right . Notice, I carefully wrote the special member on the right, and the special member on the left because one of the rules that a monoid operation does not have to satisfy is commutativity . It is perfectly allowed for x combined y to be not equal to y combined x , ( x + y =/ y + x ). In the case of a clock, it so happens, if x combined y is y combined x , but we already found out in our function world, f on g of a is not g on f of a , sine , cosine ? cosine , sine , not the same thing. Anyway the point of the clock is to show you that a monoid is a simple concept, it has, just to review, a set of things, numbers, in this case numbers, in this case functions, a rule for combining the things in the case of a clock, add them and cast out 12 s and the rule has to satisfy meta-rules, and the two meta-rules are associativity and existance of a unit or a zero, generically or abstractly. OK, I'm gonna erase the clock example, because we've locked this monoid into our memory, because it's so simple, and now I'm going to go back to functions , se we're here: Functions [x] Monoid [x] Functions Monad And show you, I don't even really have to show you anything at all, do I? Because you already know the punch line. Functions under composition form a monoid, at least if the types are the same over here. Now, I'm going to just talk for a second, and I'm not actually gonna write this down, I'll leave this as exercise to the reader, because I want to get to monads without this extra complexity. In full generallity we allow ourselves to have different types, so we allow f : a -> b , and g : b -> c and then we can only combine them in one way. In that case they don't form a monoid but they form a monoidal category and that's where we go into category theory, but you don't need to know category theory to be fully conversant, to be fully fluent in this language of function composition. All you have to remember is the types need to line up. Everybody already knows this, from programming in staticly typed languages like C, C-sharp, and VB, the types of fun..., if you gonna nest function calls the types have to line up. There's nothing complicated about this, you don't need to know category theory to..., oh I mean if you want to learn category theory to understand the full , flowering glory of the, consequences of this astonishing... you can and by all means it will only increase your richness, but you can now speak french in the world of monoidal categories because you understand, as long as the types line up then compositionality makes sense. I'm gonna stick with the simpler monoid of functions here, all the types are the same because they form a monoid and it's absolutely crystal clear that just like a clock, once you go into the clock you can't fall out. You can add 3, and 17, and 49, and 600000... and you'll still get a number between 1 and 12. You can't make a mistake. Compositionality and clock arithmetic is exactly no more complicated than compositionality in function composition. This is the power of it, this is why compositionality is the thing that we need to know to control complexity. OK, let's prove that function composition is associative, that means f composed g , composed h , is always equal to f composed g composed h . (f . g) . h == f . (g . h) Remember I'm asserting that functions are in a monoid just like clock numbers are in a monoid. In order to prove that I have to demonstrate that my little circle operator is associative.","title":"Brian Beckman: Don't fear the Monad"}]}